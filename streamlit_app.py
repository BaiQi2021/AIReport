import streamlit as st
import asyncio
import pandas as pd
import json
from datetime import datetime, timedelta
import os
import sys

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler.scheduler import run_all_crawlers, CrawlerScheduler
from crawler import get_global_registry, CrawlerType
from analysis.gemini_agent import GeminiAIReportAgent
from database.db_session import init_db, get_session
from database.models import QbitaiArticle, CompanyArticle, AibaseArticle
from sqlalchemy import select, func, desc

# Page Config
st.set_page_config(
    page_title="AIå°æŠ¥ - æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå¹³å°",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for "Natural and Beautiful" look
st.markdown("""
<style>
    .main {
        background-color: #f8f9fa;
    }
    .stButton>button {
        width: 100%;
        border-radius: 20px;
        height: 3em;
        background-color: #4CAF50;
        color: white;
    }
    .stButton>button:hover {
        background-color: #45a049;
    }
    .metric-card {
        background-color: white;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        text-align: center;
    }
    h1, h2, h3 {
        color: #2c3e50;
    }
</style>
""", unsafe_allow_html=True)

# Initialize Session State
if 'report_content' not in st.session_state:
    st.session_state.report_content = None
if 'logs' not in st.session_state:
    st.session_state.logs = []

# Helper Functions
async def run_crawler_task(crawler_options, days, max_concurrent):
    await init_db()
    
    registry = get_global_registry()
    
    # If "all" is selected or passed
    if "all" in crawler_options:
         await run_all_crawlers(days=days, max_concurrent=max_concurrent, use_incremental=True)
         return

    # Run selected crawlers
    progress_text = "Operation in progress. Please wait."
    my_bar = st.progress(0, text=progress_text)
    total = len(crawler_options)
    
    for i, crawler_key in enumerate(crawler_options):
        my_bar.progress((i / total), text=f"Running crawler: {crawler_key}")
        
        runner = registry.get_crawler_runner(crawler_key)
        if runner:
            try:
                await runner(days=days)
            except Exception as e:
                st.error(f"Error running {crawler_key}: {e}")
        else:
            st.error(f"Could not load crawler runner: {crawler_key}")
            
    my_bar.empty()

async def get_article_count_in_range(days):
    await init_db()
    cutoff_time = int((datetime.now() - timedelta(days=days)).timestamp())
    
    async with get_session() as session:
        q_count = await session.scalar(select(func.count(QbitaiArticle.id)).where(QbitaiArticle.publish_time >= cutoff_time))
        c_count = await session.scalar(select(func.count(CompanyArticle.id)).where(CompanyArticle.publish_time >= cutoff_time))
        a_count = await session.scalar(select(func.count(AibaseArticle.id)).where(AibaseArticle.publish_time >= cutoff_time))
        
        return q_count + c_count + a_count

async def get_db_stats():
    await init_db()
    async with get_session() as session:
        # Count articles
        qbitai_count = await session.scalar(select(func.count(QbitaiArticle.id)))
        company_count = await session.scalar(select(func.count(CompanyArticle.id)))
        aibase_count = await session.scalar(select(func.count(AibaseArticle.id)))
        
        return {
            "QbitAI": qbitai_count,
            "Company Blogs": company_count,
            "Aibase": aibase_count,
            "Total": qbitai_count + company_count + aibase_count
        }

async def generate_report_step_by_step(days, report_count, custom_instructions=""):
    await init_db()
    agent = GeminiAIReportAgent()
    
    status_container = st.status("æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...", expanded=True)
    
    with status_container:
        st.write("ğŸ“¥ æ­£åœ¨ä»æ•°æ®åº“è·å–æ•°æ®...")
        news_items = await agent.fetch_articles_from_db(days=days)
        if not news_items:
            st.error("æœªæ‰¾åˆ°æ•°æ®ï¼")
            return None
        st.info(f"âœ… è·å–åˆ° {len(news_items)} æ¡åŸå§‹æ•°æ®")
        
        # Visualization: Raw Data Distribution
        sources = [item.source for item in news_items]
        source_counts = pd.Series(sources).value_counts()
        st.bar_chart(source_counts)

        st.write("ğŸ” æ­£åœ¨è¿›è¡Œæ™ºèƒ½è¿‡æ»¤ (Filtering)...")
        filtered_items = await agent.step1_filter(news_items)
        st.info(f"âœ… è¿‡æ»¤åå‰©ä½™: {len(filtered_items)} æ¡ (å‰”é™¤ {len(news_items) - len(filtered_items)} æ¡)")
        
        st.write("ğŸ§© æ­£åœ¨è¿›è¡Œå½’ç±» (Clustering)...")
        clustered_items = await agent.step2_cluster(filtered_items)
        st.info(f"âœ… å½’ç±»å®Œæˆ")

        st.write("ğŸ§¹ æ­£åœ¨è¿›è¡Œå»é‡ (Deduplication)...")
        deduped_items = await agent.step3_deduplicate(clustered_items)
        st.info(f"âœ… å»é‡åå‰©ä½™: {len(deduped_items)} æ¡")

        st.write("ğŸ† æ­£åœ¨è¿›è¡Œè¯„åˆ†æ’åº (Ranking)...")
        ranked_items = await agent.step4_rank(deduped_items)
        st.info(f"âœ… æ’åºå®Œæˆ")
        
        # Visualization: Funnel
        funnel_data = {
            "Stage": ["Raw", "Filtered", "Deduplicated"],
            "Count": [len(news_items), len(filtered_items), len(deduped_items)]
        }
        st.dataframe(pd.DataFrame(funnel_data))

        st.write("ğŸ“„ æ­£åœ¨è·å– arXiv è®ºæ–‡...")
        arxiv_papers = await agent.step5_fetch_arxiv_papers(ranked_items)
        st.info(f"âœ… è·å–åˆ° {len(arxiv_papers)} ç¯‡ç›¸å…³è®ºæ–‡")

        st.write("âœï¸ æ­£åœ¨æ’°å†™æœ€ç»ˆæŠ¥å‘Š...")
        report = await agent.generate_final_report(ranked_items, arxiv_papers=arxiv_papers, days=days, target_count=report_count, custom_instructions=custom_instructions)
        
        status_container.update(label="æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼", state="complete", expanded=False)
        return report

# Sidebar
st.sidebar.title("âš™ï¸ æ§åˆ¶é¢æ¿")

st.sidebar.subheader("1. æ•°æ®é‡‡é›†è®¾ç½®")
days_lookback = st.sidebar.slider("å›æº¯å¤©æ•° (Days)", 1, 30, 3)

# Specific list of crawlers as requested
target_crawlers = {
    "Anthropic": "anthropic",
    "OpenAI": "openai",
    "Meta AI": "meta",
    "NVIDIA": "nvidia",
    "Google DeepMind": "google_deepmind",
    "HubToday": "hubtoday",
    "é‡å­ä½": "qbitai",
    "AIbase": "aibase"
}

selected_crawlers_labels = st.sidebar.multiselect(
    "é€‰æ‹©çˆ¬è™« (Select Crawlers)",
    options=list(target_crawlers.keys()),
    default=list(target_crawlers.keys())
)

selected_crawler_keys = [target_crawlers[label] for label in selected_crawlers_labels]

if st.sidebar.button("ğŸš€ å¼€å§‹é‡‡é›† (Start Crawling)"):
    if not selected_crawler_keys:
        st.sidebar.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªçˆ¬è™«ï¼")
    else:
        with st.spinner(f"æ­£åœ¨è¿è¡Œçˆ¬è™«..."):
            asyncio.run(run_crawler_task(selected_crawler_keys, days_lookback, 3))
        st.sidebar.success("é‡‡é›†å®Œæˆï¼")

st.sidebar.markdown("---")

st.sidebar.subheader("2. æŠ¥å‘Šç”Ÿæˆè®¾ç½®")

# Get available article count
try:
    available_count = asyncio.run(get_article_count_in_range(days_lookback))
    st.sidebar.caption(f"ğŸ“… è¿‡å» {days_lookback} å¤©å†…å…±æœ‰ {available_count} ç¯‡æ–‡ç« ")
    max_report_count = min(50, max(5, available_count))
except Exception:
    available_count = 0
    max_report_count = 50

report_count = st.sidebar.number_input(
    "æŠ¥å‘Šæ¡ç›®æ•°é‡", 
    min_value=1, 
    max_value=max_report_count, 
    value=min(10, max_report_count),
    help=f"åŸºäºå½“å‰æ•°æ®é‡ï¼Œå»ºè®®ä¸è¶…è¿‡ {available_count} æ¡"
)

template_file = st.sidebar.file_uploader("ä¸Šä¼ æŠ¥å‘Šæ¨¡ç‰ˆ/æŒ‡ä»¤ (å¯é€‰)", type=["md", "txt"])
custom_instructions = ""
if template_file:
    custom_instructions = template_file.read().decode("utf-8")

if st.sidebar.button("âœ¨ ç”ŸæˆæŠ¥å‘Š (Generate Report)"):
    report = asyncio.run(generate_report_step_by_step(days_lookback, report_count, custom_instructions))
    if report:
        st.session_state.report_content = report

st.sidebar.markdown("---")
st.sidebar.info("Designed for AIReport Project")

# Main Content
col1, col2, col3, col4 = st.columns(4)

# Load stats
try:
    stats = asyncio.run(get_db_stats())
    col1.metric("æ€»æ–‡ç« æ•°", stats["Total"])
    col2.metric("QbitAI", stats["QbitAI"])
    col3.metric("å…¬å¸åšå®¢", stats["Company Blogs"])
    col4.metric("Aibase", stats["Aibase"])
except Exception as e:
    st.error(f"æ— æ³•è¿æ¥æ•°æ®åº“: {e}")

st.markdown("---")

# Report Display
if st.session_state.report_content:
    st.subheader("ğŸ“ ç”Ÿæˆçš„æŠ¥å‘Š (Generated Report)")
    
    tab1, tab2 = st.tabs(["é¢„è§ˆ (Preview)", "æºç  (Source)"])
    
    with tab1:
        st.markdown(st.session_state.report_content)
    
    with tab2:
        st.code(st.session_state.report_content, language="markdown")
    
    # Download Button
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½æŠ¥å‘Š (Download Markdown)",
        data=st.session_state.report_content,
        file_name=f"AI_Report_{datetime.now().strftime('%Y-%m-%d')}.md",
        mime="text/markdown"
    )
else:
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ç‚¹å‡» 'ç”ŸæˆæŠ¥å‘Š' æŒ‰é’®å¼€å§‹ã€‚")
    
    # Show recent data preview if no report
    st.subheader("ğŸ“Š æœ€è¿‘é‡‡é›†çš„æ•°æ®é¢„è§ˆ")
    
    async def get_recent_articles():
        async with get_session() as session:
            # Fetch a few from each table
            q_stmt = select(QbitaiArticle.title, QbitaiArticle.publish_date, QbitaiArticle.article_url).order_by(desc(QbitaiArticle.publish_time)).limit(5)
            c_stmt = select(CompanyArticle.title, CompanyArticle.publish_date, CompanyArticle.article_url).order_by(desc(CompanyArticle.publish_time)).limit(5)
            
            q_res = await session.execute(q_stmt)
            c_res = await session.execute(c_stmt)
            
            data = []
            for row in q_res:
                data.append({"Title": row.title, "Date": row.publish_date, "Source": "QbitAI", "URL": row.article_url})
            for row in c_res:
                data.append({"Title": row.title, "Date": row.publish_date, "Source": "Company Blog", "URL": row.article_url})
                
            return pd.DataFrame(data)

    try:
        df = asyncio.run(get_recent_articles())
        if not df.empty:
            st.dataframe(
                df,
                column_config={
                    "URL": st.column_config.LinkColumn("Link")
                },
                use_container_width=True
            )
        else:
            st.write("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œé‡‡é›†ã€‚")
    except Exception as e:
        st.error(f"åŠ è½½é¢„è§ˆæ•°æ®å¤±è´¥: {e}")
