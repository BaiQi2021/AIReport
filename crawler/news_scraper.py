#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI News Sites Scraper (æœºå™¨ä¹‹å¿ƒ)
çˆ¬å–AIæ–°é—»ç½‘ç«™çš„èµ„è®¯
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional

from bs4 import BeautifulSoup
from sqlalchemy import select

from crawler.base_scraper import BaseWebScraper
from database.models import JiqizhixinArticle
from database.db_session import get_session
from crawler import utils

logger = utils.setup_logger()


class JiqizhixinScraper(BaseWebScraper):
    """æœºå™¨ä¹‹å¿ƒçˆ¬è™«"""
    
    def __init__(self):
        super().__init__(
            base_url="https://www.jiqizhixin.com",
            company_name="jiqizhixin"
        )
    
    async def get_article_list(self, target_date: Optional[str] = None) -> List[Dict]:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„æ–‡ç« åˆ—è¡¨
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»Šå¤©
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            if target_date is None:
                target_date = datetime.now().strftime('%Y-%m-%d')
            
            logger.info(f"Fetching Jiqizhixin articles for date: {target_date}")
            
            articles = []
            max_articles = 20  # æœ€å¤š20ç¯‡æ–‡ç« 
            
            # ç¬¬ä¸€ç¯‡ï¼šç›´æ¥ä½¿ç”¨æ—¥æœŸï¼Œä¸å¸¦åºå·
            base_url_pattern = f"{self.base_url}/articles/{target_date}"
            
            # å°è¯•ç¬¬ä¸€ç¯‡ï¼ˆä¸å¸¦åºå·ï¼‰
            url = base_url_pattern
            article_id = f"jiqizhixin_{target_date}"
            
            # æµ‹è¯•URLæ˜¯å¦å¯è®¿é—®
            html = await self.fetch_page(url)
            if html:
                articles.append({
                    'article_id': article_id,
                    'url': url,
                    'publish_date': target_date
                })
                logger.info(f"Found article: {url}")
            else:
                logger.warning(f"First article not found for {target_date}")
                return []  # å¦‚æœç¬¬ä¸€ç¯‡éƒ½è®¿é—®å¤±è´¥ï¼Œè¯´æ˜è¯¥æ—¥æœŸæ²¡æœ‰æ–‡ç« 
            
            # ä»ç¬¬äºŒç¯‡å¼€å§‹ï¼Œä½¿ç”¨åºå·ï¼ˆ2-20ï¼‰
            for article_num in range(2, max_articles + 1):
                url = f"{base_url_pattern}-{article_num}"
                article_id = f"jiqizhixin_{target_date}-{article_num}"
                
                # æµ‹è¯•URLæ˜¯å¦å¯è®¿é—®
                html = await self.fetch_page(url)
                if html:
                    articles.append({
                        'article_id': article_id,
                        'url': url,
                        'publish_date': target_date
                    })
                    logger.info(f"Found article {article_num}: {url}")
                else:
                    # è®¿é—®å¤±è´¥ï¼Œè¯´æ˜è¯¥æ—¥æœŸçš„æ–‡ç« å·²ç»çˆ¬å–å®Œæ¯•
                    logger.info(f"Article {article_num} not found, stopping crawl for {target_date}")
                    break
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)
            
            logger.info(f"Found {len(articles)} articles for date {target_date}")
            return articles
        
        except Exception as e:
            logger.error(f"Failed to get Jiqizhixin article list for {target_date}: {e}")
            return []
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        """è·å–æ–‡ç« è¯¦æƒ…"""
        try:
            logger.info(f"Fetching Jiqizhixin article details: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            article = {
                'article_id': article_id,
                'article_url': url,
            }
            
            # æ ‡é¢˜ - å°è¯•å¤šç§æ–¹å¼æå–
            article['title'] = ''
            
            # 1. ä¼˜å…ˆæŸ¥æ‰¾ h1 æ ‡ç­¾
            title_elem = soup.find('h1')
            if title_elem:
                article['title'] = self.clean_text(title_elem.get_text())
            
            # 2. å¦‚æœh1æ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾åŒ…å« title ç±»çš„å…ƒç´ 
            if not article['title']:
                title_elem = soup.find(['h1', 'h2', 'div'], class_=lambda x: x and 'title' in str(x).lower())
                if title_elem:
                    article['title'] = self.clean_text(title_elem.get_text())
            
            # 3. æŸ¥æ‰¾ og:title meta æ ‡ç­¾
            if not article['title']:
                og_title = soup.find('meta', attrs={'property': 'og:title'})
                if og_title and og_title.get('content'):
                    article['title'] = og_title.get('content').strip()
            
            # 4. ä» title æ ‡ç­¾æå–
            if not article['title']:
                title_tag = soup.find('title')
                if title_tag:
                    title_text = title_tag.get_text(strip=True)
                    # ç§»é™¤ç½‘ç«™åç§°ï¼ˆå¦‚" | æœºå™¨ä¹‹å¿ƒ"ï¼‰
                    article['title'] = title_text.split('|')[0].split('-')[0].strip()
            
            # 5. ç¡®ä¿æ ‡é¢˜ä¸ä¸ºç©º
            if not article['title']:
                article['title'] = f"æœºå™¨ä¹‹å¿ƒæ–‡ç«  {article_id}"
                logger.warning(f"æ— æ³•æå–æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜: {article['title']}")
            else:
                logger.info(f"æå–åˆ°æ ‡é¢˜: {article['title'][:50]}...")
            
            # å†…å®¹ - å°è¯•å¤šç§é€‰æ‹©å™¨
            content_elem = None
            
            # 1. å°è¯•æŸ¥æ‰¾ article æ ‡ç­¾
            content_elem = soup.find('article')
            
            # 2. å°è¯•æŸ¥æ‰¾åŒ…å« content/article/post/detail/body ç±»çš„ div
            if not content_elem:
                for class_keyword in ['content', 'article', 'post', 'detail', 'body', 'text', 'main-content']:
                    content_elem = soup.find('div', class_=lambda x: x and class_keyword in str(x).lower())
                    if content_elem:
                        # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œç¡®ä¿ä¸æ˜¯å¯¼èˆªæ ç­‰
                        text = content_elem.get_text(strip=True)
                        if len(text) > 100:  # è‡³å°‘100å­—ç¬¦æ‰è®¤ä¸ºæ˜¯æ­£æ–‡
                            break
                        else:
                            content_elem = None
            
            # 3. å°è¯•æŸ¥æ‰¾ main æ ‡ç­¾
            if not content_elem:
                main_elem = soup.find('main')
                if main_elem:
                    text = main_elem.get_text(strip=True)
                    if len(text) > 100:
                        content_elem = main_elem
            
            # 4. å°è¯•æŸ¥æ‰¾åŒ…å«æ–‡ç« å†…å®¹çš„ç‰¹å®šIDæˆ–class
            if not content_elem:
                # å°è¯•æŸ¥æ‰¾å¸¸è§çš„æ–‡ç« å®¹å™¨ID
                for id_name in ['article-content', 'post-content', 'content', 'article-body', 'main-content']:
                    content_elem = soup.find(id=id_name)
                    if content_elem:
                        break
            
            # 5. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰divï¼Œé€‰æ‹©æ–‡æœ¬æœ€é•¿çš„
            if not content_elem:
                all_divs = soup.find_all('div', class_=True)
                max_length = 0
                best_div = None
                for div in all_divs:
                    text = div.get_text(strip=True)
                    # æ’é™¤å¯¼èˆªã€ä¾§è¾¹æ ç­‰
                    classes = ' '.join(div.get('class', [])).lower()
                    if any(exclude in classes for exclude in ['nav', 'menu', 'sidebar', 'footer', 'header', 'ad', 'comment']):
                        continue
                    if len(text) > max_length and len(text) > 200:  # è‡³å°‘200å­—ç¬¦
                        max_length = len(text)
                        best_div = div
                if best_div:
                    content_elem = best_div
            
            # æå–å†…å®¹
            if content_elem:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼ˆå¯¼èˆªã€å¹¿å‘Šã€è¯„è®ºç­‰ï¼‰
                for unwanted in content_elem.find_all(['nav', 'aside', 'header', 'footer', 'script', 'style']):
                    unwanted.decompose()
                
                # ç§»é™¤åŒ…å«ç‰¹å®šç±»çš„å…ƒç´ 
                for unwanted_class in ['nav', 'menu', 'sidebar', 'ad', 'advertisement', 'comment', 'related', 'share']:
                    for elem in content_elem.find_all(class_=lambda x: x and unwanted_class in str(x).lower()):
                        elem.decompose()
                
                article['content'] = self.clean_text(content_elem.get_text())
            else:
                article['content'] = ''
                logger.warning(f"æ— æ³•æå–å†…å®¹ï¼Œarticle_id: {article_id}")
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œå°è¯•ä»meta descriptionè·å–
            if not article['content'] or len(article['content']) < 50:
                desc_elem = soup.find('meta', attrs={'property': 'og:description'})
                if desc_elem and desc_elem.get('content'):
                    article['content'] = desc_elem.get('content')
                    logger.info(f"ä½¿ç”¨ og:description ä½œä¸ºå†…å®¹ï¼Œarticle_id: {article_id}")
            
            # æå–å‚è€ƒé“¾æ¥
            reference_links = self.extract_reference_links(soup, content_elem)
            article['reference_links'] = json.dumps(reference_links, ensure_ascii=False) if reference_links else ''
            
            # æè¿°
            desc_elem = soup.find('meta', attrs={'name': 'description'})
            if not desc_elem:
                desc_elem = soup.find('meta', attrs={'property': 'og:description'})
            if desc_elem:
                article['description'] = desc_elem.get('content', '')
            else:
                article['description'] = article['content'][:200]
            
            # ä½œè€…
            author_elem = soup.find(class_=lambda x: x and 'author' in str(x).lower())
            article['author'] = self.clean_text(author_elem.get_text()) if author_elem else 'æœºå™¨ä¹‹å¿ƒ'
            
            # å‘å¸ƒæ—¶é—´æå–é€»è¾‘å¢å¼ºï¼ˆå‚è€ƒGoogleAIScraperï¼‰
            time_str = None
            
            # 1. å°è¯•ä»JSON-LDæå–
            ld_scripts = soup.find_all('script', type='application/ld+json')
            for script in ld_scripts:
                try:
                    if not script.string:
                        continue
                    data = json.loads(script.string)
                    if isinstance(data, list):
                        data = data[0]
                    
                    # é€’å½’æŸ¥æ‰¾ datePublished
                    def find_date(obj):
                        if isinstance(obj, dict):
                            for k, v in obj.items():
                                if k in ['datePublished', 'dateCreated', 'dateModified']:
                                    return v
                                if isinstance(v, (dict, list)):
                                    res = find_date(v)
                                    if res: return res
                        elif isinstance(obj, list):
                            for item in obj:
                                res = find_date(item)
                                if res: return res
                        return None
                    
                    time_str = find_date(data)
                    if time_str:
                        logger.debug(f"Found date in JSON-LD: {time_str}")
                        break
                except Exception:
                    continue
            
            # 2. å°è¯•ä»metaæ ‡ç­¾æå–
            if not time_str:
                time_elem = soup.find('meta', attrs={'property': 'article:published_time'})
                if time_elem:
                    time_str = time_elem.get('content', '')
                if not time_str:
                    time_elem = soup.find('meta', attrs={'name': 'publishdate'})
                    if time_elem:
                        time_str = time_elem.get('content', '')
                if not time_str:
                    time_elem = soup.find('meta', attrs={'name': 'date'})
                    if time_elem:
                        time_str = time_elem.get('content', '')
            
            # 3. å°è¯•ä»timeæ ‡ç­¾æå–
            if not time_str:
                time_elem = soup.find('time')
                if time_elem:
                    time_str = time_elem.get('datetime') or time_elem.get_text(strip=True)
            
            # 4. å°è¯•ä»classåŒ…å«time/dateçš„span/divæå–
            if not time_str:
                time_elem = soup.find(['span', 'div'], class_=lambda x: x and ('time' in str(x).lower() or 'date' in str(x).lower() or 'publish' in str(x).lower()))
                if time_elem:
                    time_str = time_elem.get_text(strip=True)
            
            # 5. å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»URLæˆ–æ–‡ç« IDä¸­æå–æ—¥æœŸï¼ˆä½œä¸ºfallbackï¼‰
            if not time_str:
                # å°è¯•ä»URLä¸­æå–æ—¥æœŸæ¨¡å¼ YYYY-MM-DD
                # URLæ ¼å¼: https://www.jiqizhixin.com/articles/2025-12-21 æˆ– /articles/2025-12-21-3
                url_match = re.search(r'/articles/(\d{4}-\d{2}-\d{2})(?:-\d+)?', url)
                if url_match:
                    date_str = url_match.group(1)
                    time_str = f"{date_str} 12:00:00"  # é»˜è®¤è®¾ç½®ä¸ºä¸­åˆ12ç‚¹
                    logger.debug(f"Extracted date from URL: {date_str}")
            
            # 6. å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»article_idä¸­æå–æ—¥æœŸ
            if not time_str:
                # article_idæ ¼å¼: jiqizhixin_2025-12-21 æˆ– jiqizhixin_2025-12-21-3
                id_match = re.search(r'jiqizhixin_(\d{4}-\d{2}-\d{2})', article_id)
                if id_match:
                    date_str = id_match.group(1)
                    time_str = f"{date_str} 12:00:00"
                    logger.debug(f"Extracted date from article_id: {date_str}")
            
            if not time_str:
                logger.warning(f"Skip article {article_id}: missing publish time.")
                return None
            
            publish_ts = self.parse_timestamp(time_str)
            if publish_ts is None:
                logger.warning(f"Skip article {article_id}: cannot parse publish time: {time_str}")
                return None
            article['publish_time'] = publish_ts
            article['publish_date'] = datetime.fromtimestamp(publish_ts).strftime('%Y-%m-%d')
            
            # åˆ†ç±»
            cat_elem = soup.find(class_=lambda x: x and 'category' in str(x).lower())
            article['category'] = self.clean_text(cat_elem.get_text()) if cat_elem else 'AIèµ„è®¯'
            
            # æ ‡ç­¾
            tags = []
            for tag_elem in soup.find_all(class_=lambda x: x and 'tag' in str(x).lower()):
                tag_text = self.clean_text(tag_elem.get_text())
                if tag_text and len(tag_text) < 50:
                    tags.append(tag_text)
            article['tags'] = json.dumps(tags, ensure_ascii=False) if tags else ''
            
            # å°é¢å›¾ç‰‡
            img_elem = soup.find('meta', attrs={'property': 'og:image'})
            if img_elem:
                article['cover_image'] = img_elem.get('content', '')
            else:
                img_elem = soup.find('img')
                article['cover_image'] = img_elem.get('src', '') if img_elem else ''
            
            # å…¶ä»–å­—æ®µ
            article['read_count'] = 0
            article['like_count'] = 0
            article['comment_count'] = 0
            article['share_count'] = 0
            article['collect_count'] = 0
            article['is_original'] = 0
            
            return article
        
        except Exception as e:
            logger.error(f"Failed to get Jiqizhixin article details {article_id}: {e}")
            return None


async def save_news_article_to_db(article: Dict):
    """ä¿å­˜æ–°é—»æ–‡ç« åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨JiqizhixinArticleè¡¨ï¼‰"""
    async with get_session() as session:
        article_id = article.get('article_id')
        
        stmt = select(JiqizhixinArticle).where(JiqizhixinArticle.article_id == article_id)
        result = await session.execute(stmt)
        existing = result.scalar_one_or_none()
        
        if existing:
            existing.last_modify_ts = utils.get_current_timestamp()
            for key, value in article.items():
                if hasattr(existing, key) and key not in ['id', 'add_ts']:
                    setattr(existing, key, value)
            logger.info(f"Updated Jiqizhixin article: {article_id}")
        else:
            article['add_ts'] = utils.get_current_timestamp()
            article['last_modify_ts'] = utils.get_current_timestamp()
            article['source_keyword'] = 'jiqizhixin'  # æ ‡è®°æ¥æº
            
            valid_keys = {c.name for c in JiqizhixinArticle.__table__.columns}
            filtered_article = {k: v for k, v in article.items() if k in valid_keys}
            
            db_article = JiqizhixinArticle(**filtered_article)
            session.add(db_article)
            logger.info(f"Saved new Jiqizhixin article: {article_id}")


async def run_jiqizhixin_crawler(days: int = 7):
    """è¿è¡Œæœºå™¨ä¹‹å¿ƒçˆ¬è™«"""
    logger.info("=" * 60)
    logger.info("ğŸš€ Jiqizhixin Crawler Started")
    logger.info("=" * 60)
    
    start_date = (datetime.now() - timedelta(days=days)).replace(hour=0, minute=0, second=0, microsecond=0)
    end_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    logger.info(f"Date range: {start_date.date()} to {end_date.date()}")
    
    scraper = JiqizhixinScraper()
    await scraper.init()
    
    try:
        # éå†æ—¥æœŸèŒƒå›´å†…çš„æ¯ä¸€å¤©
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            logger.info(f"Processing date: {date_str}")
            
            # è·å–è¯¥æ—¥æœŸçš„æ‰€æœ‰æ–‡ç« 
            articles = await scraper.get_article_list(target_date=date_str)
            
            if not articles:
                logger.info(f"No articles found for {date_str}")
            else:
                for article_item in articles:
                    try:
                        article = await scraper.get_article_detail(
                            article_item['article_id'],
                            article_item['url']
                        )
                        
                        if not article:
                            continue
                        
                        await save_news_article_to_db(article)
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"Error processing Jiqizhixin article {article_item['article_id']}: {e}")
                        continue
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€å¤©
            current_date += timedelta(days=1)
            await asyncio.sleep(2)  # æ—¥æœŸä¹‹é—´çš„å»¶è¿Ÿ
        
    finally:
        await scraper.close()
        logger.info("Jiqizhixin Crawler finished.")


if __name__ == "__main__":
    asyncio.run(run_jiqizhixin_crawler())
