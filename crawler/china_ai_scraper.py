#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
China AI Companies Scraper
Áà¨ÂèñÂõΩÂÜÖ‰∏ªË¶ÅAIÂÖ¨Âè∏ÔºàÁôæÂ∫¶„ÄÅÈòøÈáå„ÄÅÊô∫Ë∞±AIÁ≠âÔºâÁöÑÂÆòÁΩëÊñ∞ÈóªÂíåÁ†îÁ©∂
"""

import json
from datetime import datetime
from typing import Dict, List, Optional

from bs4 import BeautifulSoup

from crawler.base_scraper import BaseWebScraper
from crawler.openai_scraper import save_company_article_to_db
from crawler import utils

logger = utils.setup_logger()


class ZhipuAIScraper(BaseWebScraper):
    """Êô∫Ë∞±AIÁà¨Ëô´"""
    
    def __init__(self):
        super().__init__(
            base_url="https://www.zhipuai.cn",
            company_name="zhipu"
        )
        self.news_url = "https://www.zhipuai.cn/news"
    
    async def get_article_list(self, page: int = 1, article_type: str = 'news') -> List[Dict]:
        """Ëé∑ÂèñÊñáÁ´†ÂàóË°®"""
        try:
            url = self.news_url
            logger.info(f"Fetching Zhipu AI {article_type} list from {url}...")
            
            html = await self.fetch_page(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            articles = []
            
            article_elements = soup.find_all(['article', 'div', 'li'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['news', 'item', 'card']))
            
            if not article_elements:
                article_elements = soup.select('a[href*="/news/"], a[href*="/article/"]')
            
            logger.info(f"Found {len(article_elements)} potential article elements")
            
            for elem in article_elements[:30]:
                try:
                    if elem.name == 'a':
                        link_elem = elem
                    else:
                        link_elem = elem.find('a', href=True)
                    
                    if not link_elem:
                        continue
                    
                    url = link_elem.get('href', '')
                    if not url:
                        continue
                    
                    if url.startswith('/'):
                        url = self.base_url + url
                    elif not url.startswith('http'):
                        continue
                    
                    article_id = self.extract_article_id(url)
                    if not article_id:
                        continue
                    
                    title_elem = elem.find(['h1', 'h2', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = link_elem
                    title = self.clean_text(title_elem.get_text())
                    
                    if not title or len(title) < 5:
                        continue
                    
                    articles.append({
                        'article_id': f"zhipu_{article_id}",
                        'title': title[:500],
                        'url': url,
                        'article_type': 'news',
                    })
                    
                except Exception as e:
                    logger.warning(f"Failed to parse article element: {e}")
                    continue
            
            logger.info(f"Extracted {len(articles)} Zhipu articles")
            return articles
        
        except Exception as e:
            logger.error(f"Failed to get Zhipu article list: {e}")
            return []
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        """Ëé∑ÂèñÊñáÁ´†ËØ¶ÊÉÖ"""
        try:
            logger.info(f"Fetching Zhipu article details: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            article = {
                'article_id': article_id,
                'article_url': url,
                'company': self.company_name,
            }
            
            # Ê†áÈ¢ò
            title_elem = soup.find('h1')
            if not title_elem:
                title_elem = soup.find('title')
            article['title'] = self.clean_text(title_elem.get_text()) if title_elem else ''
            
            # ÂÜÖÂÆπ
            content_elem = soup.find(['article', 'div'], class_=lambda x: x and ('content' in str(x).lower() or 'article' in str(x).lower()))
            if not content_elem:
                content_elem = soup.find('main')
            
            article['content'] = self.clean_text(content_elem.get_text()) if content_elem else ''
            
            # ÊèêÂèñÂèÇËÄÉÈìæÊé•
            reference_links = self.extract_reference_links(soup, content_elem)
            article['reference_links'] = json.dumps(reference_links, ensure_ascii=False) if reference_links else ''
            
            # ÊèèËø∞
            desc_elem = soup.find('meta', attrs={'name': 'description'})
            if desc_elem:
                article['description'] = desc_elem.get('content', '')
            else:
                article['description'] = article['content'][:300]
            
            # ‰ΩúËÄÖ
            article['author'] = 'Êô∫Ë∞±AI'
            
            # ÂèëÂ∏ÉÊó∂Èó¥
            time_elem = soup.find(['time', 'span'], class_=lambda x: x and 'time' in str(x).lower())
            time_str = time_elem.get_text() if time_elem else ''
            
            article['publish_time'] = self.parse_timestamp(time_str) if time_str else utils.get_current_timestamp()
            article['publish_date'] = datetime.fromtimestamp(article['publish_time']).strftime('%Y-%m-%d')
            
            # ÂàÜÁ±ªÂíåÊ†áÁ≠æ
            article['category'] = 'AI News'
            article['tags'] = ''
            article['cover_image'] = ''
            article['article_type'] = 'news'
            article['is_research'] = 0
            article['is_product'] = 1 if any(keyword in article['title'] for keyword in ['GLM', 'Êô∫Ë∞±', 'ÂèëÂ∏É', '‰∫ßÂìÅ']) else 0
            
            return article
        
        except Exception as e:
            logger.error(f"Failed to get Zhipu article details {article_id}: {e}")
            return None


class AlibabaQwenScraper(BaseWebScraper):
    """ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆÁà¨Ëô´"""
    
    def __init__(self):
        super().__init__(
            base_url="https://tongyi.aliyun.com",
            company_name="alibaba"
        )
        self.blog_url = "https://developer.aliyun.com/topic/tongyi"  # Êõ¥Êñ∞‰∏∫‰∏ìÈ¢òÈ°µ
    
    async def get_article_list(self, page: int = 1, article_type: str = 'blog') -> List[Dict]:
        """Ëé∑ÂèñÊñáÁ´†ÂàóË°®"""
        try:
            # ÈªòËÆ§Áà¨ÂèñÂçöÂÆ¢ÔºåÂõ†‰∏∫ÂÆòÁΩëÈ¶ñÈ°µÈÄöÂ∏∏ÊòØÂä®ÊÄÅÂä†ËΩΩÁöÑ
            url = self.blog_url if article_type == 'blog' else self.base_url
            logger.info(f"Fetching Alibaba Qwen {article_type} list from {url}...")
            
            html = await self.fetch_page(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            articles = []
            
            article_elements = soup.find_all(['article', 'div', 'li'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['news', 'item', 'list', 'card', 'article']))
            
            if not article_elements:
                article_elements = soup.select('a[href*="/article/"], a[href*="/news/"]')
            
            logger.info(f"Found {len(article_elements)} potential article elements")
            
            for elem in article_elements[:30]:
                try:
                    if elem.name == 'a':
                        link_elem = elem
                    else:
                        link_elem = elem.find('a', href=True)
                    
                    if not link_elem:
                        continue
                    
                    url = link_elem.get('href', '')
                    if not url:
                        continue
                    
                    if url.startswith('/'):
                        url = 'https://developer.aliyun.com' + url if 'developer' in self.blog_url else self.base_url + url
                    elif not url.startswith('http'):
                        continue
                    
                    article_id = self.extract_article_id(url)
                    if not article_id:
                        continue
                    
                    title_elem = elem.find(['h1', 'h2', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = link_elem
                    title = self.clean_text(title_elem.get_text())
                    
                    if not title or len(title) < 5:
                        continue
                    
                    articles.append({
                        'article_id': f"alibaba_{article_id}",
                        'title': title[:500],
                        'url': url,
                        'article_type': 'blog' if 'developer' in url else 'news',
                    })
                    
                except Exception as e:
                    logger.warning(f"Failed to parse article element: {e}")
                    continue
            
            logger.info(f"Extracted {len(articles)} Alibaba Qwen articles")
            return articles
        
        except Exception as e:
            logger.error(f"Failed to get Alibaba Qwen article list: {e}")
            return []
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        """Ëé∑ÂèñÊñáÁ´†ËØ¶ÊÉÖ"""
        try:
            logger.info(f"Fetching Alibaba Qwen article details: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            article = {
                'article_id': article_id,
                'article_url': url,
                'company': self.company_name,
            }
            
            # Ê†áÈ¢ò
            title_elem = soup.find('h1')
            if not title_elem:
                title_elem = soup.find('title')
            article['title'] = self.clean_text(title_elem.get_text()) if title_elem else ''
            
            # ÂÜÖÂÆπ
            content_elem = soup.find(['article', 'div'], class_=lambda x: x and ('content' in str(x).lower() or 'article' in str(x).lower()))
            if not content_elem:
                content_elem = soup.find('main')
            
            article['content'] = self.clean_text(content_elem.get_text()) if content_elem else ''
            
            # ÊèêÂèñÂèÇËÄÉÈìæÊé•
            reference_links = self.extract_reference_links(soup, content_elem)
            article['reference_links'] = json.dumps(reference_links, ensure_ascii=False) if reference_links else ''
            
            # ÊèèËø∞
            desc_elem = soup.find('meta', attrs={'name': 'description'})
            if desc_elem:
                article['description'] = desc_elem.get('content', '')
            else:
                article['description'] = article['content'][:300]
            
            # ‰ΩúËÄÖ
            article['author'] = 'ÈòøÈáå‰∫ëÈÄö‰πâ'
            
            # ÂèëÂ∏ÉÊó∂Èó¥
            time_elem = soup.find(['time', 'span'], class_=lambda x: x and 'time' in str(x).lower())
            time_str = time_elem.get_text() if time_elem else ''
            
            article['publish_time'] = self.parse_timestamp(time_str) if time_str else utils.get_current_timestamp()
            article['publish_date'] = datetime.fromtimestamp(article['publish_time']).strftime('%Y-%m-%d')
            
            # ÂàÜÁ±ªÂíåÊ†áÁ≠æ
            article['category'] = 'AI News'
            article['tags'] = ''
            article['cover_image'] = ''
            article['article_type'] = 'blog' if 'developer' in url else 'news'
            article['is_research'] = 0
            article['is_product'] = 1 if any(keyword in article['title'] for keyword in ['ÈÄö‰πâ', 'Qwen', 'ÂèëÂ∏É', '‰∫ßÂìÅ']) else 0
            
            return article
        
        except Exception as e:
            logger.error(f"Failed to get Alibaba Qwen article details {article_id}: {e}")
            return None


class MoonshotAIScraper(BaseWebScraper):
    """Moonshot AIÔºàÊúà‰πãÊöóÈù¢ÔºâÁà¨Ëô´"""
    
    def __init__(self):
        super().__init__(
            base_url="https://www.moonshot.cn",
            company_name="moonshot"
        )
    
    async def get_article_list(self, page: int = 1, article_type: str = 'news') -> List[Dict]:
        """Ëé∑ÂèñÊñáÁ´†ÂàóË°®"""
        try:
            url = self.base_url
            logger.info(f"Fetching Moonshot AI {article_type} list from {url}...")
            
            html = await self.fetch_page(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            articles = []
            
            article_elements = soup.find_all(['article', 'div', 'li'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['news', 'item', 'card']))
            
            if not article_elements:
                article_elements = soup.select('a[href*="/news/"], a[href*="/article/"]')
            
            logger.info(f"Found {len(article_elements)} potential article elements")
            
            for elem in article_elements[:30]:
                try:
                    if elem.name == 'a':
                        link_elem = elem
                    else:
                        link_elem = elem.find('a', href=True)
                    
                    if not link_elem:
                        continue
                    
                    url = link_elem.get('href', '')
                    if not url:
                        continue
                    
                    if url.startswith('/'):
                        url = self.base_url + url
                    elif not url.startswith('http'):
                        continue
                    
                    article_id = self.extract_article_id(url)
                    if not article_id:
                        continue
                    
                    title_elem = elem.find(['h1', 'h2', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = link_elem
                    title = self.clean_text(title_elem.get_text())
                    
                    if not title or len(title) < 5:
                        continue
                    
                    articles.append({
                        'article_id': f"moonshot_{article_id}",
                        'title': title[:500],
                        'url': url,
                        'article_type': 'news',
                    })
                    
                except Exception as e:
                    logger.warning(f"Failed to parse article element: {e}")
                    continue
            
            logger.info(f"Extracted {len(articles)} Moonshot AI articles")
            return articles
        
        except Exception as e:
            logger.error(f"Failed to get Moonshot AI article list: {e}")
            return []
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        """Ëé∑ÂèñÊñáÁ´†ËØ¶ÊÉÖ"""
        try:
            logger.info(f"Fetching Moonshot AI article details: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            article = {
                'article_id': article_id,
                'article_url': url,
                'company': self.company_name,
            }
            
            # Ê†áÈ¢ò
            title_elem = soup.find('h1')
            if not title_elem:
                title_elem = soup.find('title')
            article['title'] = self.clean_text(title_elem.get_text()) if title_elem else ''
            
            # ÂÜÖÂÆπ
            content_elem = soup.find(['article', 'div'], class_=lambda x: x and ('content' in str(x).lower() or 'article' in str(x).lower()))
            if not content_elem:
                content_elem = soup.find('main')
            
            article['content'] = self.clean_text(content_elem.get_text()) if content_elem else ''
            
            # ÊèêÂèñÂèÇËÄÉÈìæÊé•
            reference_links = self.extract_reference_links(soup, content_elem)
            article['reference_links'] = json.dumps(reference_links, ensure_ascii=False) if reference_links else ''
            
            # ÊèèËø∞
            desc_elem = soup.find('meta', attrs={'name': 'description'})
            if desc_elem:
                article['description'] = desc_elem.get('content', '')
            else:
                article['description'] = article['content'][:300]
            
            # ‰ΩúËÄÖ
            article['author'] = 'Moonshot AI'
            
            # ÂèëÂ∏ÉÊó∂Èó¥
            time_elem = soup.find(['time', 'span'], class_=lambda x: x and 'time' in str(x).lower())
            time_str = time_elem.get_text() if time_elem else ''
            
            article['publish_time'] = self.parse_timestamp(time_str) if time_str else utils.get_current_timestamp()
            article['publish_date'] = datetime.fromtimestamp(article['publish_time']).strftime('%Y-%m-%d')
            
            # ÂàÜÁ±ªÂíåÊ†áÁ≠æ
            article['category'] = 'AI News'
            article['tags'] = ''
            article['cover_image'] = ''
            article['article_type'] = 'news'
            article['is_research'] = 0
            article['is_product'] = 1 if any(keyword in article['title'] for keyword in ['Kimi', 'ÂèëÂ∏É', '‰∫ßÂìÅ']) else 0
            
            return article
        
        except Exception as e:
            logger.error(f"Failed to get Moonshot AI article details {article_id}: {e}")
            return None


async def run_china_ai_crawler(days: int = 7):
    """ËøêË°åÂõΩÂÜÖAIÂÖ¨Âè∏Áà¨Ëô´"""
    logger.info("=" * 60)
    logger.info("üöÄ China AI Companies Crawler Started")
    logger.info("=" * 60)
    
    # Êô∫Ë∞±AI
    zhipu_scraper = ZhipuAIScraper()
    await zhipu_scraper.init()
    
    try:
        logger.info("Fetching Zhipu AI articles...")
        articles = await zhipu_scraper.get_article_list()
        
        for article_item in articles[:15]:
            try:
                article = await zhipu_scraper.get_article_detail(
                    article_item['article_id'],
                    article_item['url']
                )
                
                if article:
                    await save_company_article_to_db(article)
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error processing Zhipu article: {e}")
                continue
    finally:
        await zhipu_scraper.close()
    
    # ÈòøÈáå‰∫ëÈÄö‰πâ
    alibaba_scraper = AlibabaQwenScraper()
    await alibaba_scraper.init()
    
    try:
        logger.info("Fetching Alibaba Qwen articles...")
        articles = await alibaba_scraper.get_article_list()
        
        for article_item in articles[:15]:
            try:
                article = await alibaba_scraper.get_article_detail(
                    article_item['article_id'],
                    article_item['url']
                )
                
                if article:
                    await save_company_article_to_db(article)
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error processing Alibaba article: {e}")
                continue
    finally:
        await alibaba_scraper.close()
    
    # Moonshot AI
    moonshot_scraper = MoonshotAIScraper()
    await moonshot_scraper.init()
    
    try:
        logger.info("Fetching Moonshot AI articles...")
        articles = await moonshot_scraper.get_article_list()
        
        for article_item in articles[:15]:
            try:
                article = await moonshot_scraper.get_article_detail(
                    article_item['article_id'],
                    article_item['url']
                )
                
                if article:
                    await save_company_article_to_db(article)
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error processing Moonshot article: {e}")
                continue
    finally:
        await moonshot_scraper.close()
        logger.info("China AI Companies Crawler finished.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(run_china_ai_crawler())

