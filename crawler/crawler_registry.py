#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«æ³¨å†Œä¸­å¿ƒ (Crawler Registry)
ç»Ÿä¸€ç®¡ç†å’Œæ³¨å†Œæ‰€æœ‰çˆ¬è™«ï¼Œæä¾›ç»Ÿä¸€çš„è®¿é—®æ¥å£
"""

import importlib
from typing import Dict, List, Type, Optional, Callable, Any
from enum import Enum

from crawler import utils
from crawler.constants import CRAWLER_CONFIGS

logger = utils.setup_logger()


class CrawlerType(Enum):
    """çˆ¬è™«ç±»å‹æšä¸¾"""
    COMPANY = "company"  # AIå…¬å¸å®˜ç½‘
    NEWS = "news"  # æ–°é—»åª’ä½“
    TOOLS = "tools"  # AIå·¥å…·åšå®¢


class CrawlerRegistry:
    """çˆ¬è™«æ³¨å†Œä¸­å¿ƒ"""
    
    def __init__(self):
        self._crawlers: Dict[str, Dict] = {}
    
    def register(
        self, 
        key: str, 
        name: str, 
        crawler_class: Type = None,
        crawler_type: CrawlerType = CrawlerType.COMPANY,
        enabled: bool = True,
        priority: int = 5,
        description: str = "",
        db_table: str = "company_article",
        module_path: str = None,
        class_name: str = None,
        runner_name: str = None,
    ):
        """
        æ³¨å†Œçˆ¬è™«
        
        Args:
            key: çˆ¬è™«å”¯ä¸€æ ‡è¯†
            name: çˆ¬è™«æ˜¾ç¤ºåç§°
            crawler_class: çˆ¬è™«ç±»ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›module_pathå’Œclass_nameåˆ™åŠ¨æ€å¯¼å…¥ï¼‰
            crawler_type: çˆ¬è™«ç±»å‹
            enabled: æ˜¯å¦å¯ç”¨
            priority: ä¼˜å…ˆçº§ï¼ˆ1-10ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
            description: æè¿°ä¿¡æ¯
            db_table: æ•°æ®åº“è¡¨å
            module_path: æ¨¡å—è·¯å¾„ï¼ˆç”¨äºåŠ¨æ€å¯¼å…¥ï¼‰
            class_name: ç±»åï¼ˆç”¨äºåŠ¨æ€å¯¼å…¥ï¼‰
            runner_name: runnerå‡½æ•°åï¼ˆç”¨äºåŠ¨æ€å¯¼å…¥ï¼‰
        """
        self._crawlers[key] = {
            'key': key,
            'name': name,
            'class': crawler_class,
            'type': crawler_type,
            'enabled': enabled,
            'priority': priority,
            'description': description,
            'db_table': db_table,
            'module_path': module_path,
            'class_name': class_name,
            'runner_name': runner_name,
        }
        logger.debug(f"Registered crawler: {name} ({key})")
    
    def get_crawler(self, key: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šçš„çˆ¬è™«é…ç½®"""
        return self._crawlers.get(key)
    
    def get_all_crawlers(self, enabled_only: bool = True) -> List[Dict]:
        """è·å–æ‰€æœ‰çˆ¬è™«"""
        crawlers = list(self._crawlers.values())
        if enabled_only:
            crawlers = [c for c in crawlers if c.get('enabled', True)]
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        crawlers.sort(key=lambda x: x.get('priority', 999))
        return crawlers
    
    def get_crawlers_by_type(self, crawler_type: CrawlerType, enabled_only: bool = True) -> List[Dict]:
        """æ ¹æ®ç±»å‹è·å–çˆ¬è™«"""
        crawlers = [c for c in self._crawlers.values() if c.get('type') == crawler_type]
        if enabled_only:
            crawlers = [c for c in crawlers if c.get('enabled', True)]
        crawlers.sort(key=lambda x: x.get('priority', 999))
        return crawlers
    
    def get_crawler_class(self, key: str) -> Optional[Type]:
        """è·å–çˆ¬è™«ç±»ï¼ˆåŠ¨æ€å¯¼å…¥ï¼‰"""
        crawler_info = self.get_crawler(key)
        if not crawler_info:
            logger.error(f"Crawler {key} not found")
            return None
        
        # å¦‚æœå·²ç»æœ‰ç±»å¯¹è±¡ï¼Œç›´æ¥è¿”å›
        if crawler_info.get('class'):
            return crawler_info['class']
        
        # å¦åˆ™å°è¯•åŠ¨æ€å¯¼å…¥
        module_path = crawler_info.get('module_path')
        class_name = crawler_info.get('class_name')
        
        if not module_path or not class_name:
            logger.error(f"Crawler {key} missing module_path or class_name")
            return None
        
        try:
            module = importlib.import_module(module_path)
            crawler_class = getattr(module, class_name)
            # ç¼“å­˜ç±»å¯¹è±¡
            crawler_info['class'] = crawler_class
            return crawler_class
        except Exception as e:
            logger.error(f"Failed to import crawler {key} from {module_path}.{class_name}: {e}")
            return None
    
    def get_crawler_runner(self, key: str) -> Optional[Callable]:
        """è·å–çˆ¬è™«runnerå‡½æ•°ï¼ˆåŠ¨æ€å¯¼å…¥ï¼‰"""
        crawler_info = self.get_crawler(key)
        if not crawler_info:
            logger.error(f"Crawler {key} not found")
            return None
        
        runner_name = crawler_info.get('runner_name')
        if not runner_name:
            logger.warning(f"Crawler {key} has no runner function")
            return None
        
        module_path = crawler_info.get('module_path')
        if not module_path:
            logger.error(f"Crawler {key} missing module_path")
            return None
        
        try:
            module = importlib.import_module(module_path)
            runner_func = getattr(module, runner_name)
            return runner_func
        except Exception as e:
            logger.error(f"Failed to import runner {runner_name} from {module_path}: {e}")
            return None
    
    def list_crawlers(self):
        """æ‰“å°æ‰€æœ‰å·²æ³¨å†Œçš„çˆ¬è™«"""
        logger.info("=" * 80)
        logger.info("ğŸ“‹ å·²æ³¨å†Œçš„çˆ¬è™«åˆ—è¡¨")
        logger.info("=" * 80)
        
        for crawler_type in CrawlerType:
            crawlers = self.get_crawlers_by_type(crawler_type, enabled_only=False)
            if crawlers:
                logger.info(f"\nğŸ”¹ {crawler_type.value.upper()} ç±»å‹:")
                for c in crawlers:
                    status = "âœ…" if c.get('enabled') else "âŒ"
                    logger.info(f"  {status} {c['name']:20} ({c['key']:15}) - Priority: {c['priority']}")
        
        logger.info("\n" + "=" * 80)
        logger.info(f"æ€»è®¡: {len(self._crawlers)} ä¸ªçˆ¬è™«")
        enabled_count = len([c for c in self._crawlers.values() if c.get('enabled')])
        logger.info(f"å¯ç”¨: {enabled_count} ä¸ª")
        logger.info("=" * 80)


# å…¨å±€çˆ¬è™«æ³¨å†Œä¸­å¿ƒå®ä¾‹
_global_registry = None


def get_global_registry() -> CrawlerRegistry:
    """è·å–å…¨å±€çˆ¬è™«æ³¨å†Œä¸­å¿ƒå®ä¾‹"""
    global _global_registry
    if _global_registry is None:
        _global_registry = CrawlerRegistry()
        _register_all_crawlers(_global_registry)
    return _global_registry


def _register_all_crawlers(registry: CrawlerRegistry):
    """ä»é…ç½®è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰çˆ¬è™«"""
    try:
        for config in CRAWLER_CONFIGS:
            # è½¬æ¢typeå­—ç¬¦ä¸²ä¸ºæšä¸¾
            crawler_type_str = config.get('type', 'company')
            crawler_type = CrawlerType(crawler_type_str)
            
            registry.register(
                key=config['key'],
                name=config['name'],
                crawler_type=crawler_type,
                enabled=config.get('enabled', True),
                priority=config.get('priority', 5),
                description=config.get('description', ''),
                db_table=config.get('db_table', 'company_article'),
                module_path=config.get('module'),
                class_name=config.get('class'),
                runner_name=config.get('runner'),
            )
        
        logger.info(f"âœ… æˆåŠŸæ³¨å†Œ {len(registry.get_all_crawlers(enabled_only=False))} ä¸ªçˆ¬è™«")
        
    except Exception as e:
        logger.error(f"æ³¨å†Œçˆ¬è™«å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # æµ‹è¯•æ³¨å†Œä¸­å¿ƒ
    registry = get_global_registry()
    registry.list_crawlers()
    
    # æµ‹è¯•åŠ¨æ€å¯¼å…¥
    print("\næµ‹è¯•åŠ¨æ€å¯¼å…¥:")
    qbitai_class = registry.get_crawler_class('qbitai')
    print(f"Qbitai Class: {qbitai_class}")
    
    qbitai_runner = registry.get_crawler_runner('qbitai')
    print(f"Qbitai Runner: {qbitai_runner}")
