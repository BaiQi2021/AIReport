#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google AI (DeepMind & Google Research) Scraper
çˆ¬å–Google AIã€DeepMindçš„ç ”ç©¶è®ºæ–‡å’Œåšå®¢
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional

from bs4 import BeautifulSoup

from crawler.base_scraper import BaseWebScraper
from crawler.openai_scraper import save_company_article_to_db
from crawler import utils

logger = utils.setup_logger()


class GoogleAIScraper(BaseWebScraper):
    """Google AIå®˜ç½‘çˆ¬è™«ï¼ˆåŒ…æ‹¬DeepMindï¼‰"""
    
    def __init__(self, source: str = 'google'):
        """
        Args:
            source: 'google' for Google AI Blog, 'deepmind' for DeepMind
        """
        if source == 'deepmind':
            base_url = "https://deepmind.google"
            company_name = "deepmind"
        else:
            base_url = "https://blog.google"
            company_name = "google"
        
        super().__init__(base_url=base_url, company_name=company_name)
        self.source = source
        
        if source == 'deepmind':
            self.blog_url = "https://deepmind.google/discover/blog/"
            self.research_url = "https://deepmind.google/research/"
        else:
            self.blog_url = "https://blog.google/technology/ai/"
    
    async def get_article_list(self, page: int = 1, article_type: str = 'blog') -> List[Dict]:
        """è·å–æ–‡ç« åˆ—è¡¨"""
        try:
            if self.source == 'deepmind':
                if article_type == 'research':
                    url = self.research_url
                else:
                    url = self.blog_url
            else:
                url = self.blog_url
            
            logger.info(f"Fetching {self.company_name} {article_type} list from {url}...")
            
            html = await self.fetch_page(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            articles = []
            
            # Googleå’ŒDeepMindéƒ½ä½¿ç”¨articleæ ‡ç­¾æˆ–ç‰¹å®šçš„å¡ç‰‡å®¹å™¨
            article_elements = soup.find_all(['article', 'div'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['post', 'card', 'item', 'article']))
            
            if not article_elements:
                article_elements = soup.select('a[href*="/blog/"], a[href*="/research/"], a[href*="/discover/"]')
            
            logger.info(f"Found {len(article_elements)} potential article elements")
            
            for elem in article_elements[:30]:
                try:
                    if elem.name == 'a':
                        link_elem = elem
                    else:
                        link_elem = elem.find('a', href=True)
                    
                    if not link_elem:
                        continue
                    
                    url = link_elem.get('href', '')
                    if not url:
                        continue
                    
                    if url.startswith('/'):
                        url = self.base_url + url
                    elif not url.startswith('http'):
                        continue
                    
                    article_id = self.extract_article_id(url)
                    if not article_id:
                        continue
                    
                    title_elem = elem.find(['h1', 'h2', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = link_elem
                    title = self.clean_text(title_elem.get_text())
                    
                    if not title or len(title) < 5:
                        continue
                    
                    if '/research/' in url:
                        determined_type = 'research'
                    else:
                        determined_type = 'blog'
                    
                    articles.append({
                        'article_id': f"{self.company_name}_{article_id}",
                        'title': title[:500],
                        'url': url,
                        'article_type': determined_type,
                    })
                    
                except Exception as e:
                    logger.warning(f"Failed to parse article element: {e}")
                    continue
            
            logger.info(f"Extracted {len(articles)} {self.company_name} articles")
            return articles
        
        except Exception as e:
            logger.error(f"Failed to get {self.company_name} article list: {e}")
            return []
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        """è·å–æ–‡ç« è¯¦æƒ…"""
        try:
            logger.info(f"Fetching {self.company_name} article details: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            article = {
                'article_id': article_id,
                'article_url': url,
                'company': self.company_name,
            }
            
            # æ ‡é¢˜
            title_elem = soup.find('h1')
            if not title_elem:
                title_elem = soup.find('title')
            article['title'] = self.clean_text(title_elem.get_text()) if title_elem else ''
            
            # å†…å®¹
            content_elem = soup.find('article')
            if not content_elem:
                content_elem = soup.find('main')
            if not content_elem:
                content_elem = soup.find(['div'], class_=lambda x: x and ('content' in str(x).lower() or 'article' in str(x).lower()))
            
            article['content'] = self.clean_text(content_elem.get_text()) if content_elem else ''
            
            # æå–å‚è€ƒé“¾æ¥
            reference_links = self.extract_reference_links(soup, content_elem)
            article['reference_links'] = json.dumps(reference_links, ensure_ascii=False) if reference_links else ''
            
            # æè¿°
            desc_elem = soup.find('meta', attrs={'name': 'description'})
            if not desc_elem:
                desc_elem = soup.find('meta', attrs={'property': 'og:description'})
            if desc_elem:
                article['description'] = desc_elem.get('content', '')
            else:
                article['description'] = article['content'][:300]
            
            # ä½œè€…
            author_elem = soup.find(['span', 'div', 'p'], class_=lambda x: x and 'author' in str(x).lower())
            if not author_elem:
                author_elem = soup.find('meta', attrs={'name': 'author'})
                article['author'] = author_elem.get('content', '') if author_elem else ('DeepMind' if self.source == 'deepmind' else 'Google AI')
            else:
                article['author'] = self.clean_text(author_elem.get_text())
            
            # å‘å¸ƒæ—¶é—´æå–é€»è¾‘å¢å¼º
            time_str = None
            
            # 1. å°è¯•ä»JSON-LDæå–
            ld_scripts = soup.find_all('script', type='application/ld+json')
            for script in ld_scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, list):
                        data = data[0]
                    if 'datePublished' in data:
                        time_str = data['datePublished']
                        logger.info(f"Found datePublished in JSON-LD: {time_str}")
                        break
                    if 'dateCreated' in data:
                        time_str = data['dateCreated']
                        logger.info(f"Found dateCreated in JSON-LD: {time_str}")
                        break
                except Exception:
                    continue
            
            # 2. å°è¯•ä»metaæ ‡ç­¾æå–
            if not time_str:
                time_elem = soup.find('meta', attrs={'property': 'article:published_time'})
                if time_elem:
                    time_str = time_elem.get('content', '')
            
            # 3. å°è¯•ä»timeæ ‡ç­¾æå–
            if not time_str:
                # ä¼˜å…ˆæŸ¥æ‰¾ä½äºheaderæˆ–articleå†…çš„timeæ ‡ç­¾
                time_elem = None
                if content_elem:
                    time_elem = content_elem.find('time')
                
                if not time_elem:
                    # æŸ¥æ‰¾classåŒ…å«dateçš„å…ƒç´ ä¸­çš„time
                    date_container = soup.find(['div', 'span', 'p'], class_=lambda x: x and 'date' in str(x).lower())
                    if date_container:
                        time_elem = date_container.find('time')
                
                if not time_elem:
                    # å…¨å±€æŸ¥æ‰¾
                    time_elem = soup.find('time')
                
                if time_elem:
                    # ä¼˜å…ˆä½¿ç”¨ datetime å±æ€§
                    dt_attr = time_elem.get('datetime', '')
                    text_content = time_elem.get_text().strip()
                    
                    # å¦‚æœdatetimeå±æ€§åªåŒ…å«å¹´æœˆï¼ˆå¦‚May 2025ï¼‰ï¼Œä¸”æ–‡æœ¬åŒ…å«æ›´è¯¦ç»†æ—¥æœŸï¼Œä¼˜å…ˆç”¨æ–‡æœ¬
                    if dt_attr and len(dt_attr) < 10 and len(text_content) > len(dt_attr):
                         time_str = text_content
                    else:
                        time_str = dt_attr or text_content
            
            # 4. å°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æå–æ—¥æœŸæ¨¡å¼
            if not time_str:
                # æŸ¥æ‰¾åŒ…å«æ—¥æœŸçš„æ–‡æœ¬å…ƒç´ 
                date_pattern = re.compile(r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}', re.IGNORECASE)
                
                # åœ¨æ ‡é¢˜é™„è¿‘æˆ–metadataåŒºåŸŸæŸ¥æ‰¾
                meta_area = soup.find(['header', 'div'], class_=lambda x: x and any(k in str(x).lower() for k in ['meta', 'info', 'date', 'author']))
                if meta_area:
                    match = date_pattern.search(meta_area.get_text())
                    if match:
                        time_str = match.group(0)
                
                if not time_str:
                    # åœ¨å…¨æ–‡å¼€å¤´æŸ¥æ‰¾ï¼ˆå‰2000å­—ç¬¦ï¼‰
                    match = date_pattern.search(soup.get_text()[:2000])
                    if match:
                        time_str = match.group(0)
            
            if not time_str:
                logger.warning(f"Skip article {article_id}: missing publish time.")
                return None
                
            publish_ts = self.parse_timestamp(time_str)
            if publish_ts is None:
                logger.warning(f"Skip article {article_id}: cannot parse publish time: {time_str}")
                return None
            
            article['publish_time'] = publish_ts
            article['publish_date'] = datetime.fromtimestamp(publish_ts).strftime('%Y-%m-%d')
            
            # åˆ†ç±»
            article['category'] = 'AI Research' if '/research/' in url else 'AI Blog'
            
            # æ ‡ç­¾
            tag_elements = soup.find_all(['a', 'span'], class_=lambda x: x and 'tag' in str(x).lower())
            tags = []
            for tag_elem in tag_elements:
                tag_text = self.clean_text(tag_elem.get_text())
                if tag_text and len(tag_text) < 50:
                    tags.append(tag_text)
            article['tags'] = json.dumps(tags, ensure_ascii=False) if tags else ''
            
            # å°é¢å›¾ç‰‡
            img_elem = soup.find('meta', attrs={'property': 'og:image'})
            if img_elem:
                article['cover_image'] = img_elem.get('content', '')
            else:
                img_elem = soup.find('img')
                article['cover_image'] = img_elem.get('src', '') if img_elem else ''
            
            # æ–‡ç« ç±»å‹åˆ¤æ–­
            article['article_type'] = 'research' if '/research/' in url else 'blog'
            article['is_research'] = 1 if article['article_type'] == 'research' else 0
            article['is_product'] = 1 if any(keyword in article['title'].lower() for keyword in ['gemini', 'bard', 'palm', 'product', 'launch', 'release', 'announce']) else 0
            
            return article
        
        except Exception as e:
            logger.error(f"Failed to get {self.company_name} article details {article_id}: {e}")
            return None


async def run_google_ai_crawler(days: int = 7):
    """è¿è¡ŒGoogle AIçˆ¬è™«"""
    logger.info("=" * 60)
    logger.info(f"ğŸš€ Google AI Crawler Started (Filter: last {days} days)")
    logger.info("=" * 60)
    
    # Google AI Blog
    google_scraper = GoogleAIScraper(source='google')
    await google_scraper.init()
    
    try:
        logger.info("Fetching Google AI blog articles...")
        articles = await google_scraper.get_article_list(article_type='blog')
        
        count = 0
        for article_item in articles[:15]:
            try:
                article = await google_scraper.get_article_detail(
                    article_item['article_id'],
                    article_item['url']
                )
                
                if article:
                    # æ£€æŸ¥æ—¥æœŸ
                    if days > 0:
                        article_ts = article['publish_time']
                        now_ts = datetime.now().timestamp()
                        # å¦‚æœæ–‡ç« æ—¶é—´åœ¨æœªæ¥ï¼ˆå…è®¸1å¤©è¯¯å·®ï¼‰ï¼Œæˆ–è€…æ˜¯æœ€è¿‘dayså¤©å†…çš„
                        if article_ts > now_ts + 86400:
                             logger.warning(f"Skip article {article['title']}: future date ({article['publish_date']})")
                             continue
                        if now_ts - article_ts > days * 86400:
                             logger.info(f"Skip article {article['title']}: too old ({article['publish_date']})")
                             continue
                    
                    await save_company_article_to_db(article)
                    count += 1
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error processing Google AI article: {e}")
                continue
        logger.info(f"Saved {count} Google AI articles")
                
    finally:
        await google_scraper.close()
    
    # DeepMind
    deepmind_scraper = GoogleAIScraper(source='deepmind')
    await deepmind_scraper.init()
    
    try:
        # DeepMind Blog
        logger.info("Fetching DeepMind blog articles...")
        blog_articles = await deepmind_scraper.get_article_list(article_type='blog')
        
        count = 0
        for article_item in blog_articles[:15]:
            try:
                article = await deepmind_scraper.get_article_detail(
                    article_item['article_id'],
                    article_item['url']
                )
                
                if article:
                    # æ£€æŸ¥æ—¥æœŸ
                    if days > 0:
                        article_ts = article['publish_time']
                        now_ts = datetime.now().timestamp()
                        if article_ts > now_ts + 86400:
                             logger.warning(f"Skip article {article['title']}: future date ({article['publish_date']})")
                             continue
                        if now_ts - article_ts > days * 86400:
                             logger.info(f"Skip article {article['title']}: too old ({article['publish_date']})")
                             continue
                    
                    await save_company_article_to_db(article)
                    count += 1
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error processing DeepMind blog article: {e}")
                continue
        logger.info(f"Saved {count} DeepMind blog articles")
        
        # DeepMind Research
        logger.info("Fetching DeepMind research articles...")
        research_articles = await deepmind_scraper.get_article_list(article_type='research')
        
        count = 0
        for article_item in research_articles[:15]:
            try:
                article = await deepmind_scraper.get_article_detail(
                    article_item['article_id'],
                    article_item['url']
                )
                
                if article:
                    # æ£€æŸ¥æ—¥æœŸ
                    if days > 0:
                        article_ts = article['publish_time']
                        now_ts = datetime.now().timestamp()
                        if article_ts > now_ts + 86400:
                             logger.warning(f"Skip article {article['title']}: future date ({article['publish_date']})")
                             continue
                        if now_ts - article_ts > days * 86400:
                             logger.info(f"Skip article {article['title']}: too old ({article['publish_date']})")
                             continue
                    
                    await save_company_article_to_db(article)
                    count += 1
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Error processing DeepMind research article: {e}")
                continue
        logger.info(f"Saved {count} DeepMind research articles")
        
    finally:
        await deepmind_scraper.close()
        logger.info("Google AI & DeepMind Crawler finished.")


if __name__ == "__main__":
    import asyncio
    asyncio.run(run_google_ai_crawler())
