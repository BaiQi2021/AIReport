#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çš„çˆ¬è™«è°ƒåº¦å™¨ (Unified Crawler Scheduler)
æ”¯æŒå¹¶å‘æ‰§è¡Œã€å¢é‡æ›´æ–°å’ŒåŠ¨æ€çˆ¬è™«åŠ è½½
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Callable, Optional
from sqlalchemy import select, func

from crawler import utils
from crawler.crawler_registry import get_global_registry, CrawlerType
from crawler.constants import SCHEDULER_CONFIG
from database.models import CompanyArticle, QbitaiArticle
from database.db_session import get_session

logger = utils.setup_logger()


class IncrementalUpdateManager:
    """å¢é‡æ›´æ–°ç®¡ç†å™¨"""
    
    @staticmethod
    async def get_latest_article_time(company: str = None) -> datetime:
        """è·å–æŸå…¬å¸æœ€æ–°æ–‡ç« çš„æ—¶é—´"""
        try:
            async with get_session() as session:
                if company:
                    stmt = select(func.max(CompanyArticle.publish_time)).where(
                        CompanyArticle.company == company
                    )
                else:
                    stmt = select(func.max(CompanyArticle.publish_time))
                
                result = await session.execute(stmt)
                max_timestamp = result.scalar()
                
                if max_timestamp:
                    return datetime.fromtimestamp(max_timestamp)
                else:
                    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›7å¤©å‰
                    return datetime.now() - timedelta(days=7)
        except Exception as e:
            logger.error(f"Failed to get latest article time: {e}")
            return datetime.now() - timedelta(days=7)
    
    @staticmethod
    async def get_latest_news_time(source: str = None) -> datetime:
        """è·å–æŸæ–°é—»æºæœ€æ–°æ–‡ç« çš„æ—¶é—´"""
        try:
            async with get_session() as session:
                if source:
                    # æ ¹æ®article_idå‰ç¼€åˆ¤æ–­æ¥æº
                    stmt = select(func.max(QbitaiArticle.publish_time)).where(
                        QbitaiArticle.article_id.like(f"{source}_%")
                    )
                else:
                    stmt = select(func.max(QbitaiArticle.publish_time))
                
                result = await session.execute(stmt)
                max_timestamp = result.scalar()
                
                if max_timestamp:
                    return datetime.fromtimestamp(max_timestamp)
                else:
                    return datetime.now() - timedelta(days=7)
        except Exception as e:
            logger.error(f"Failed to get latest news time: {e}")
            return datetime.now() - timedelta(days=7)
    
    @staticmethod
    async def should_crawl(source: str, source_type: str = 'company', threshold_hours: int = 1) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦çˆ¬å–ï¼ˆåŸºäºæœ€åæ›´æ–°æ—¶é—´ï¼‰
        
        Args:
            source: æ•°æ®æºæ ‡è¯†
            source_type: æ•°æ®æºç±»å‹ (company/news/tools)
            threshold_hours: é˜ˆå€¼å°æ—¶æ•°
        """
        try:
            if source_type == 'company':
                latest_time = await IncrementalUpdateManager.get_latest_article_time(source)
            else:
                latest_time = await IncrementalUpdateManager.get_latest_news_time(source)
            
            # å¦‚æœæœ€æ–°æ–‡ç« è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™éœ€è¦çˆ¬å–
            time_diff = datetime.now() - latest_time
            should_crawl = time_diff > timedelta(hours=threshold_hours)
            
            if should_crawl:
                logger.info(f"Source {source} needs crawling (last update: {time_diff.total_seconds()/3600:.1f}h ago)")
            else:
                logger.info(f"Source {source} is up to date (last update: {time_diff.total_seconds()/60:.1f}m ago)")
            
            return should_crawl
        except Exception as e:
            logger.error(f"Error checking if should crawl {source}: {e}")
            return True  # å‡ºé”™æ—¶é»˜è®¤çˆ¬å–


class CrawlerScheduler:
    """çˆ¬è™«è°ƒåº¦å™¨"""
    
    def __init__(
        self,
        days: int = 7,
        max_concurrent: int = None,
        use_incremental: bool = None,
        crawler_delay: int = None,
    ):
        """
        Args:
            days: çˆ¬å–å¤©æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            use_incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ›´æ–°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            crawler_delay: çˆ¬è™«ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        self.days = days
        self.max_concurrent = max_concurrent or SCHEDULER_CONFIG['max_concurrent']
        self.use_incremental = use_incremental if use_incremental is not None else SCHEDULER_CONFIG['use_incremental']
        self.crawler_delay = crawler_delay or SCHEDULER_CONFIG['crawler_delay']
        
        self.results = {
            'total_crawlers': 0,
            'success_crawlers': 0,
            'failed_crawlers': 0,
            'skipped_crawlers': 0,
            'crawlers': []
        }
        
        self.incremental_manager = IncrementalUpdateManager()
        self.registry = get_global_registry()
    
    async def run_crawler_with_tracking(
        self,
        crawler_key: str,
        crawler_name: str,
        crawler_runner: Callable,
        crawler_type: str = 'company'
    ):
        """è¿è¡Œå•ä¸ªçˆ¬è™«å¹¶è·Ÿè¸ªç»“æœ"""
        self.results['total_crawlers'] += 1
        
        try:
            # å¢é‡æ›´æ–°æ£€æŸ¥
            if self.use_incremental:
                threshold_hours = SCHEDULER_CONFIG['incremental_threshold'] / 3600
                should_crawl = await self.incremental_manager.should_crawl(
                    crawler_key, 
                    crawler_type,
                    threshold_hours=threshold_hours
                )
                if not should_crawl:
                    logger.info(f"â­ï¸  Skipping {crawler_name} (up to date)")
                    self.results['skipped_crawlers'] += 1
                    self.results['crawlers'].append({
                        'name': crawler_name,
                        'key': crawler_key,
                        'status': 'skipped',
                        'reason': 'up_to_date'
                    })
                    return
            
            logger.info(f"ğŸ¯ Starting {crawler_name}...")
            start_time = datetime.now()
            
            # è¿è¡Œçˆ¬è™«
            await crawler_runner(days=self.days)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.results['success_crawlers'] += 1
            self.results['crawlers'].append({
                'name': crawler_name,
                'key': crawler_key,
                'status': 'success',
                'duration': duration
            })
            
            logger.info(f"âœ… {crawler_name} completed in {duration:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ {crawler_name} failed: {e}")
            self.results['failed_crawlers'] += 1
            self.results['crawlers'].append({
                'name': crawler_name,
                'key': crawler_key,
                'status': 'failed',
                'error': str(e)
            })
    
    async def run_crawlers_by_type(self, crawler_type: CrawlerType):
        """å¹¶å‘è¿è¡ŒæŒ‡å®šç±»å‹çš„çˆ¬è™«"""
        crawlers = self.registry.get_crawlers_by_type(crawler_type, enabled_only=True)
        
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š Running {len(crawlers)} {crawler_type.value} crawlers (max concurrent: {self.max_concurrent})")
        logger.info("=" * 80)
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for crawler_info in crawlers:
            crawler_key = crawler_info['key']
            crawler_name = crawler_info['name']
            
            # è·å–runnerå‡½æ•°
            runner_func = self.registry.get_crawler_runner(crawler_key)
            if not runner_func:
                logger.warning(f"No runner function found for {crawler_key}, skipping")
                continue
            
            task = self.run_crawler_with_tracking(
                crawler_key,
                crawler_name,
                runner_func,
                crawler_type.value
            )
            tasks.append(task)
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def run_with_semaphore(task):
            async with semaphore:
                await task
                await asyncio.sleep(self.crawler_delay)  # çˆ¬è™«é—´å»¶è¿Ÿ
        
        # å¹¶å‘æ‰§è¡Œ
        if tasks:
            await asyncio.gather(*[run_with_semaphore(task) for task in tasks])
        else:
            logger.warning(f"No enabled crawlers found for type: {crawler_type.value}")
    
    async def run_all(self):
        """è¿è¡Œæ‰€æœ‰çˆ¬è™«"""
        logger.info("ğŸš€" * 40)
        logger.info("   AI REPORT - UNIFIED CRAWLER SCHEDULER")
        logger.info("ğŸš€" * 40)
        logger.info(f"ğŸ“… Date Range: Last {self.days} days")
        logger.info(f"âš¡ Max Concurrent: {self.max_concurrent}")
        logger.info(f"ğŸ”„ Incremental Update: {'Enabled' if self.use_incremental else 'Disabled'}")
        logger.info(f"â° Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("")
        
        overall_start = datetime.now()
        
        # è¿è¡Œæ‰€æœ‰ç±»å‹çš„çˆ¬è™«
        for crawler_type in CrawlerType:
            await self.run_crawlers_by_type(crawler_type)
        
        overall_end = datetime.now()
        overall_duration = (overall_end - overall_start).total_seconds()
        
        # æ‰“å°æ€»ç»“
        self.print_summary(overall_duration)
        
        return self.results
    
    def print_summary(self, total_duration: float):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        logger.info("")
        logger.info("=" * 80)
        logger.info("ğŸ“Š CRAWLER EXECUTION SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Total Crawlers: {self.results['total_crawlers']}")
        logger.info(f"âœ… Success: {self.results['success_crawlers']}")
        logger.info(f"â­ï¸  Skipped: {self.results['skipped_crawlers']}")
        logger.info(f"âŒ Failed: {self.results['failed_crawlers']}")
        logger.info(f"â±ï¸  Total Duration: {total_duration:.2f}s ({total_duration/60:.2f} minutes)")
        
        if self.use_incremental and self.results['skipped_crawlers'] > 0:
            time_saved = self.results['skipped_crawlers'] * 30  # å‡è®¾æ¯ä¸ªçˆ¬è™«å¹³å‡30ç§’
            logger.info(f"âš¡ Time Saved (Incremental): ~{time_saved}s")
        
        logger.info("")
        
        if self.results['crawlers']:
            logger.info("Crawler Details:")
            logger.info("-" * 80)
            for crawler in self.results['crawlers']:
                status_icon = {
                    'success': 'âœ…',
                    'failed': 'âŒ',
                    'skipped': 'â­ï¸'
                }.get(crawler['status'], 'â“')
                
                if crawler['status'] == 'success':
                    logger.info(f"{status_icon} {crawler['name']:25} | Duration: {crawler.get('duration', 0):.2f}s")
                elif crawler['status'] == 'skipped':
                    logger.info(f"{status_icon} {crawler['name']:25} | Reason: {crawler.get('reason', 'unknown')}")
                else:
                    logger.info(f"{status_icon} {crawler['name']:25} | Error: {crawler.get('error', 'Unknown')[:50]}")
        
        logger.info("=" * 80)
        logger.info(f"â° End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("ğŸ‰ All crawlers completed!")
        logger.info("=" * 80)


async def run_all_crawlers(
    days: int = 7,
    max_concurrent: int = None,
    use_incremental: bool = None
) -> Dict:
    """
    è¿è¡Œæ‰€æœ‰çˆ¬è™«çš„ä¾¿æ·å‡½æ•°
    
    Args:
        days: çˆ¬å–å¤©æ•°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        use_incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ›´æ–°
        
    Returns:
        æ‰§è¡Œç»“æœå­—å…¸
    """
    scheduler = CrawlerScheduler(
        days=days,
        max_concurrent=max_concurrent,
        use_incremental=use_incremental
    )
    results = await scheduler.run_all()
    return results


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(run_all_crawlers(days=7, max_concurrent=3, use_incremental=True))

