# Crawleræ¨¡å—å¿«é€Ÿå‚è€ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è¿è¡Œæ‰€æœ‰çˆ¬è™«
```bash
python main.py
```

### è¿è¡Œç‰¹å®šçˆ¬è™«
```bash
python main.py --crawler qbitai
python main.py --crawler company  # æ‰€æœ‰å…¬å¸çˆ¬è™«
python main.py --crawler news     # æ‰€æœ‰æ–°é—»çˆ¬è™«
```

### å¹¶å‘æ¨¡å¼
```bash
python main.py --concurrent --max-concurrent 5
```

## ğŸ“¦ å¸¸ç”¨å¯¼å…¥

```python
# è°ƒåº¦å™¨
from crawler.scheduler import run_all_crawlers, CrawlerScheduler

# æ³¨å†Œä¸­å¿ƒ
from crawler import get_global_registry, CrawlerType

# åŸºç±»
from crawler import BaseWebScraper

# å·¥å…·
from crawler import setup_logger, get_current_timestamp

# é…ç½®
from crawler.constants import (
    CRAWLER_CONFIGS,
    DEFAULT_CRAWLER_CONFIG,
    SCHEDULER_CONFIG
)
```

## ğŸ”§ å¸¸ç”¨æ“ä½œ

### åˆ—å‡ºæ‰€æœ‰çˆ¬è™«
```python
from crawler import get_global_registry

registry = get_global_registry()
registry.list_crawlers()
```

### è¿è¡Œæ‰€æœ‰çˆ¬è™«
```python
from crawler.scheduler import run_all_crawlers

results = await run_all_crawlers(
    days=7,
    max_concurrent=3,
    use_incremental=True
)
```

### è¿è¡Œç‰¹å®šç±»å‹çˆ¬è™«
```python
from crawler.scheduler import CrawlerScheduler
from crawler import CrawlerType

scheduler = CrawlerScheduler(days=7)
await scheduler.run_crawlers_by_type(CrawlerType.COMPANY)
```

### åŠ¨æ€è¿è¡Œå•ä¸ªçˆ¬è™«
```python
from crawler import get_global_registry

registry = get_global_registry()
runner = registry.get_crawler_runner('qbitai')
if runner:
    await runner(days=7)
```

## ğŸ†• æ·»åŠ æ–°çˆ¬è™«

### 1. åˆ›å»ºçˆ¬è™«æ–‡ä»¶
```python
# crawler/my_scraper.py
from crawler import BaseWebScraper
from typing import Dict, List, Optional

class MyScraper(BaseWebScraper):
    def __init__(self):
        super().__init__(
            base_url="https://example.com",
            company_name="example"
        )
    
    async def get_article_list(self, page: int = 1) -> List[Dict]:
        # å®ç°è·å–æ–‡ç« åˆ—è¡¨
        html = await self.fetch_page(self.base_url)
        # ... è§£æé€»è¾‘
        return articles
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        # å®ç°è·å–æ–‡ç« è¯¦æƒ…
        html = await self.fetch_page(url)
        # ... è§£æé€»è¾‘
        return article

async def run_my_crawler(days: int = 7):
    """è¿è¡Œçˆ¬è™«çš„å…¥å£å‡½æ•°"""
    async with MyScraper() as scraper:
        articles = await scraper.get_article_list()
        # ... å¤„ç†å’Œä¿å­˜é€»è¾‘
```

### 2. æ·»åŠ é…ç½®
```python
# crawler/constants.py
CRAWLER_CONFIGS = [
    # ... ç°æœ‰é…ç½® ...
    {
        'key': 'my_crawler',
        'name': 'My Crawler',
        'module': 'crawler.my_scraper',
        'class': 'MyScraper',
        'runner': 'run_my_crawler',
        'type': 'company',  # æˆ– 'news', 'tools'
        'enabled': True,
        'priority': 1,
        'description': 'æˆ‘çš„çˆ¬è™«',
        'db_table': 'company_article',
    },
]
```

### 3. è¿è¡Œæµ‹è¯•
```bash
python main.py --crawler my_crawler --days 1
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
crawler/
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ constants.py             # é…ç½®å¸¸é‡ â­
â”œâ”€â”€ crawler_registry.py      # æ³¨å†Œä¸­å¿ƒ â­
â”œâ”€â”€ scheduler.py             # è°ƒåº¦å™¨ â­
â”œâ”€â”€ base_scraper.py          # åŸºç±» â­
â”œâ”€â”€ utils.py                 # å·¥å…·å‡½æ•°
â”œâ”€â”€ proxy_pool.py            # ä»£ç†æ± 
â”‚
â”œâ”€â”€ anthropic_scraper.py     # å…·ä½“çˆ¬è™«å®ç°
â”œâ”€â”€ google_ai_scraper.py
â”œâ”€â”€ meta_microsoft_scraper.py
â”œâ”€â”€ openai_scraper.py
â”œâ”€â”€ ai_companies_scraper.py
â”œâ”€â”€ qbitai_scraper.py
â””â”€â”€ ai_tools_scraper.py
```

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### çˆ¬è™«ç±»å‹
- `COMPANY` - AIå…¬å¸å®˜ç½‘
- `NEWS` - æ–°é—»åª’ä½“
- `TOOLS` - AIå·¥å…·åšå®¢

### é…ç½®æ–‡ä»¶
- `constants.py` - æ‰€æœ‰é…ç½®çš„å”¯ä¸€æ¥æº
- ä¿®æ”¹é…ç½®åæ— éœ€é‡å¯ï¼ˆéƒ¨åˆ†é…ç½®ï¼‰

### æ³¨å†Œä¸­å¿ƒ
- è‡ªåŠ¨å‘ç°å’ŒåŠ è½½çˆ¬è™«
- åŠ¨æ€å¯¼å…¥ï¼Œæ— ç¡¬ç¼–ç 

### è°ƒåº¦å™¨
- æ”¯æŒå¹¶å‘æ‰§è¡Œ
- æ”¯æŒå¢é‡æ›´æ–°
- è‡ªåŠ¨è·³è¿‡æœ€è¿‘æ›´æ–°çš„æ•°æ®æº

## ğŸ” è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹æ‰€æœ‰æ³¨å†Œçš„çˆ¬è™«
```bash
python -c "from crawler import get_global_registry; get_global_registry().list_crawlers()"
```

### æµ‹è¯•å•ä¸ªçˆ¬è™«
```python
from crawler import get_global_registry

registry = get_global_registry()
crawler_info = registry.get_crawler('qbitai')
print(crawler_info)

# è·å–çˆ¬è™«ç±»
CrawlerClass = registry.get_crawler_class('qbitai')
print(CrawlerClass)

# è·å–runnerå‡½æ•°
runner = registry.get_crawler_runner('qbitai')
print(runner)
```

### æµ‹è¯•çˆ¬è™«è¿è¡Œ
```bash
# åªçˆ¬å–1å¤©çš„æ•°æ®ï¼Œå¿«é€Ÿæµ‹è¯•
python main.py --crawler qbitai --days 1 --skip-report
```

## âš™ï¸ é…ç½®å‚æ•°

### è°ƒåº¦å™¨é…ç½®
```python
SCHEDULER_CONFIG = {
    'max_concurrent': 3,            # æœ€å¤§å¹¶å‘æ•°
    'use_incremental': True,        # å¢é‡æ›´æ–°
    'crawler_delay': 2,             # çˆ¬è™«é—´å»¶è¿Ÿ(ç§’)
    'incremental_threshold': 3600,  # å¢é‡é˜ˆå€¼(ç§’)
}
```

### çˆ¬è™«é…ç½®
```python
DEFAULT_CRAWLER_CONFIG = {
    'days': 7,                      # çˆ¬å–å¤©æ•°
    'max_articles_per_source': 20,  # æ¯æºæœ€å¤§æ–‡ç« æ•°
    'request_delay': 2,             # è¯·æ±‚å»¶è¿Ÿ(ç§’)
    'timeout': 30,                  # è¶…æ—¶(ç§’)
    'retry_times': 3,               # é‡è¯•æ¬¡æ•°
}
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: æ‰¾ä¸åˆ°çˆ¬è™«ï¼Ÿ
```python
# æ£€æŸ¥çˆ¬è™«æ˜¯å¦æ³¨å†Œ
from crawler import get_global_registry
registry = get_global_registry()
registry.list_crawlers()
```

### Q: çˆ¬è™«è¿è¡Œå¤±è´¥ï¼Ÿ
```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
python main.py --crawler xxx --days 1
```

### Q: å¦‚ä½•ç¦ç”¨æŸä¸ªçˆ¬è™«ï¼Ÿ
```python
# åœ¨ constants.py ä¸­è®¾ç½®
{
    'key': 'xxx',
    'enabled': False,  # ç¦ç”¨
    # ...
}
```

### Q: å¦‚ä½•è°ƒæ•´å¹¶å‘æ•°ï¼Ÿ
```bash
python main.py --concurrent --max-concurrent 5
```

## ğŸ“š æ›´å¤šæ–‡æ¡£

- **å®Œæ•´æ–‡æ¡£**: `crawler/README.md`
- **è¿ç§»æŒ‡å—**: `MIGRATION_GUIDE.md`
- **é‡æ„æ€»ç»“**: `REFACTORING_SUMMARY.md`

## ğŸ’¡ æœ€ä½³å®è·µ

1. âœ… ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
2. âœ… ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
3. âœ… ä½¿ç”¨æ³¨å†Œä¸­å¿ƒåŠ¨æ€åŠ è½½
4. âœ… æ·»åŠ å®Œæ•´çš„ç±»å‹æç¤º
5. âœ… ç¼–å†™æ¸…æ™°çš„docstring
6. âœ… æµ‹è¯•å•ä¸ªçˆ¬è™«åå†æ‰¹é‡è¿è¡Œ

## ğŸ‰ å¿«é€Ÿå‘½ä»¤

```bash
# å¼€å‘æµ‹è¯•
python main.py --crawler qbitai --days 1 --skip-report

# ç”Ÿäº§è¿è¡Œ
python main.py --concurrent --max-concurrent 3

# åªç”ŸæˆæŠ¥å‘Š
python main.py --skip-crawl

# åªçˆ¬å–æ•°æ®
python main.py --skip-report

# æŸ¥çœ‹å¸®åŠ©
python main.py --help
```

---

**æç¤º**: è¿™æ˜¯å¿«é€Ÿå‚è€ƒï¼Œè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ï¼

