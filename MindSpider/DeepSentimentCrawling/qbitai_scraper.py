#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡å­ä½(QbitAI)ç½‘ç«™çˆ¬è™« - ç›´æ¥çˆ¬å–è„šæœ¬
æ— éœ€ç™»é™†ï¼Œç›´æ¥çˆ¬å–è¿‘ä¸¤å‘¨çš„æ–‡ç« åˆ°æ•°æ®åº“
"""

import asyncio
import json
import sys
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    # 1. å…ˆå¯¼å…¥ MindSpider çš„é…ç½®
    import config as mindspider_config
    settings = mindspider_config.settings
    
    # 2. å…³é”®æ­¥éª¤ï¼šä» sys.modules ä¸­ç§»é™¤ config
    # è¿™æ ·åç»­ MediaCrawler å¯¼å…¥ config æ—¶ï¼Œä¼šé‡æ–°åŠ è½½ä¸º MediaCrawler/config åŒ…
    # è€Œä¸æ˜¯å¤ç”¨ MindSpider/config.py æ¨¡å—
    if 'config' in sys.modules:
        del sys.modules['config']
        
    from loguru import logger
    import httpx
    from bs4 import BeautifulSoup
    from sqlalchemy import select, text
    from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
    from sqlalchemy.orm import sessionmaker
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·è¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)

# å¯¼å…¥æ•°æ®åº“æ¨¡å‹
# ä½¿ç”¨ insert(0) ç¡®ä¿ä¼˜å…ˆä» MediaCrawler ç›®å½•æŸ¥æ‰¾ config åŒ…
media_crawler_path = project_root / "DeepSentimentCrawling/MediaCrawler"
sys.path.insert(0, str(media_crawler_path))

from database.models import QbitaiArticle, QbitaiArticleComment, Base
from database.db_session import get_session
from tools import utils


class QbitaiWebScraper:
    """é‡å­ä½ç½‘ç«™ç›´æ¥çˆ¬è™«"""
    
    def __init__(self):
        self.base_url = "https://www.qbitai.com"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
        self.session = None
    
    async def init(self):
        """åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯"""
        self.session = httpx.AsyncClient(headers=self.headers, timeout=30)
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        if self.session:
            await self.session.aclose()
    
    async def fetch_page(self, url: str, **kwargs) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = await self.session.get(url, **kwargs)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    async def get_article_list(self, page: int = 1) -> List[Dict]:
        """è·å–æ–‡ç« åˆ—è¡¨"""
        try:
            logger.info(f"è·å–ç¬¬ {page} é¡µæ–‡ç« åˆ—è¡¨...")
            
            # é‡å­ä½ä¸»é¡µæˆ–åˆ—è¡¨é¡µ
            if page == 1:
                url = f"{self.base_url}/"
            else:
                url = f"{self.base_url}/?page={page}"
            
            html = await self.fetch_page(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            articles = []
            
            # æŸ¥æ‰¾æ–‡ç« å…ƒç´  - é’ˆå¯¹é‡å­ä½å®˜ç½‘ç»“æ„ä¼˜åŒ–
            # ä¸»è¦ç»“æ„æ˜¯ div.picture_text
            article_elements = soup.find_all('div', class_='picture_text')
            
            if not article_elements:
                # å¤‡ç”¨é€‰æ‹©å™¨
                article_elements = soup.find_all(class_=re.compile(r'article|news|post|item', re.I))
            
            if not article_elements:
                # æœ€åå°è¯•æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                article_elements = soup.select('a[href*="/article"], a[href*="/news"]')
            
            logger.info(f"æ‰¾åˆ° {len(article_elements)} ä¸ªå¯èƒ½çš„æ–‡ç« å…ƒç´ ")
            
            for elem in article_elements[:20]:  # é™åˆ¶æ¯é¡µ20ç¯‡
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    # é’ˆå¯¹ picture_text ç»“æ„
                    if 'picture_text' in elem.get('class', []):
                        title_elem = elem.select_one('.text_box h4 a')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            url = title_elem.get('href', '')
                        else:
                            continue
                    elif elem.name == 'a':
                        title = elem.get_text(strip=True)
                        url = elem.get('href', '')
                    else:
                        title_elem = elem.find(['h2', 'h3', 'h4', 'a'])
                        title = title_elem.get_text(strip=True) if title_elem else ''
                        
                        link_elem = elem.find('a', href=re.compile(r'article|news'))
                        url = link_elem.get('href', '') if link_elem else ''
                    
                    if not url or not title:
                        continue
                    
                    # è§„èŒƒåŒ–URL
                    if not url.startswith('http'):
                        url = urljoin(self.base_url, url)
                    
                    # æå–æ–‡ç« ID
                    article_id = self._extract_article_id(url)
                    if not article_id:
                        continue
                    
                    articles.append({
                        'article_id': article_id,
                        'title': title[:500],
                        'url': url,
                    })
                    logger.debug(f"æå–æ–‡ç« : {article_id} - {title[:30]}")
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–‡ç« å…ƒç´ å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ç¬¬ {page} é¡µå…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
            return articles
        
        except Exception as e:
            logger.error(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def get_article_detail(self, article_id: str, url: str) -> Optional[Dict]:
        """è·å–æ–‡ç« è¯¦æƒ…"""
        try:
            logger.info(f"è·å–æ–‡ç« è¯¦æƒ…: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            article = {
                'article_id': article_id,
                'url': url,
            }
            
            # æ ‡é¢˜
            title_elem = soup.find(['h1', 'h2'], class_=re.compile(r'title', re.I))
            article['title'] = title_elem.get_text(strip=True) if title_elem else ''
            
            # å†…å®¹
            content_elem = soup.find(class_=re.compile(r'content|article-body|main', re.I))
            article['content'] = content_elem.get_text(strip=True) if content_elem else ''
            
            # æè¿°/æ‘˜è¦
            desc_elem = soup.find(class_=re.compile(r'desc|summary|intro', re.I))
            article['description'] = desc_elem.get_text(strip=True) if desc_elem else article['content'][:200]
            
            # ä½œè€…
            author_elem = soup.find(class_=re.compile(r'author', re.I))
            article['author'] = author_elem.get_text(strip=True) if author_elem else ''
            
            # å‘å¸ƒæ—¶é—´
            time_elem = soup.find(['time', 'span'], class_=re.compile(r'time|date|pub', re.I))
            if not time_elem:
                time_elem = soup.find('meta', attrs={'property': 'article:published_time'})
                time_str = time_elem.get('content') if time_elem else datetime.now().isoformat()
            else:
                time_str = time_elem.get_text(strip=True) if time_elem.name != 'meta' else time_elem.get('content')
            
            article['publish_time'] = self._parse_timestamp(time_str)
            article['publish_date'] = datetime.fromtimestamp(article['publish_time']).strftime('%Y-%m-%d')
            
            # åˆ†ç±»
            cat_elem = soup.find(class_=re.compile(r'category|cat', re.I))
            article['category'] = cat_elem.get_text(strip=True) if cat_elem else ''
            
            # æ ‡ç­¾
            tags = []
            for tag_elem in soup.find_all(class_=re.compile(r'tag', re.I)):
                tag_text = tag_elem.get_text(strip=True)
                if tag_text:
                    tags.append(tag_text)
            article['tags'] = json.dumps(tags, ensure_ascii=False) if tags else ''
            
            # å°é¢å›¾ç‰‡
            img_elem = soup.find('img', class_=re.compile(r'cover|featured', re.I))
            article['cover_image'] = img_elem.get('src') if img_elem else ''
            
            # ç‚¹èµã€è¯„è®ºç­‰æ•°æ®
            article['read_count'] = 0
            article['like_count'] = 0
            article['comment_count'] = 0
            article['share_count'] = 0
            article['collect_count'] = 0
            article['is_original'] = 1
            
            logger.info(f"æˆåŠŸè·å–æ–‡ç« è¯¦æƒ…: {article['title'][:50]}")
            return article
        
        except Exception as e:
            logger.error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥ {article_id}: {e}")
            return None
    
    async def get_comments(self, article_id: str, url: str) -> List[Dict]:
        """è·å–æ–‡ç« è¯„è®º"""
        try:
            logger.info(f"è·å–æ–‡ç« è¯„è®º: {article_id}")
            
            html = await self.fetch_page(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            comments = []
            
            # æŸ¥æ‰¾è¯„è®ºå…ƒç´ 
            comment_elements = soup.find_all(class_=re.compile(r'comment', re.I))
            logger.info(f"æ‰¾åˆ° {len(comment_elements)} æ¡è¯„è®º")
            
            for idx, elem in enumerate(comment_elements[:50]):  # é™åˆ¶50æ¡è¯„è®º
                try:
                    # ç”¨æˆ·å
                    user_elem = elem.find(class_=re.compile(r'user|author', re.I))
                    user_name = user_elem.get_text(strip=True) if user_elem else f'ç”¨æˆ·{idx}'
                    
                    # è¯„è®ºå†…å®¹
                    content_elem = elem.find(class_=re.compile(r'content|text', re.I))
                    if not content_elem:
                        content_elem = elem.find('p')
                    content = content_elem.get_text(strip=True) if content_elem else ''
                    
                    if not content:
                        continue
                    
                    # å¤´åƒ
                    avatar_elem = elem.find('img', class_=re.compile(r'avatar', re.I))
                    user_avatar = avatar_elem.get('src') if avatar_elem else ''
                    
                    # æ—¶é—´
                    time_elem = elem.find(class_=re.compile(r'time|date', re.I))
                    time_str = time_elem.get_text(strip=True) if time_elem else datetime.now().isoformat()
                    publish_time = self._parse_timestamp(time_str)
                    
                    # ç‚¹èµæ•°
                    like_elem = elem.find(class_=re.compile(r'like', re.I))
                    like_count = 0
                    if like_elem:
                        match = re.search(r'\d+', like_elem.get_text())
                        like_count = int(match.group()) if match else 0
                    
                    comment = {
                        'comment_id': f"{article_id}_comment_{idx}",
                        'article_id': article_id,
                        'user_name': user_name,
                        'user_avatar': user_avatar,
                        'content': content,
                        'publish_time': publish_time,
                        'publish_date': datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d'),
                        'like_count': like_count,
                        'sub_comment_count': 0,
                        'parent_comment_id': None,
                    }
                    comments.append(comment)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†è¯„è®ºå¤±è´¥: {e}")
                    continue
            
            logger.info(f"æˆåŠŸæå– {len(comments)} æ¡è¯„è®º")
            return comments
        
        except Exception as e:
            logger.error(f"è·å–è¯„è®ºå¤±è´¥ {article_id}: {e}")
            return []
    
    def _extract_article_id(self, url: str) -> Optional[str]:
        """ä»URLæå–æ–‡ç« ID"""
        patterns = [
            r'/article/(\d+)',
            r'/news/(\d+)',
            r'/(\d+)\.html',
            r'/article/([^/]+)',
            r'article[=/]([^&/?]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # æœ€åä½¿ç”¨URLçš„hash
        return url.split('/')[-1].split('.')[0] if url else None
    
    def _parse_timestamp(self, time_str: str) -> int:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºæ—¶é—´æˆ³"""
        try:
            if not time_str:
                return int(datetime.now().timestamp())
            
            time_str = time_str.strip()
            now = datetime.now()
            
            # å¤„ç†ç›¸å¯¹æ—¶é—´
            if 'åˆšåˆš' in time_str:
                return int(now.timestamp())
            elif 'åˆ†é’Ÿå‰' in time_str:
                minutes = int(re.search(r'(\d+)', time_str).group(1))
                return int((now - timedelta(minutes=minutes)).timestamp())
            elif 'å°æ—¶å‰' in time_str:
                hours = int(re.search(r'(\d+)', time_str).group(1))
                return int((now - timedelta(hours=hours)).timestamp())
            elif 'å¤©å‰' in time_str:
                days = int(re.search(r'(\d+)', time_str).group(1))
                return int((now - timedelta(days=days)).timestamp())
            elif 'æ˜¨å¤©' in time_str:
                # æ˜¨å¤© 15:28
                time_part = re.search(r'(\d{1,2}:\d{1,2})', time_str)
                if time_part:
                    dt_str = f"{(now - timedelta(days=1)).strftime('%Y-%m-%d')} {time_part.group(1)}"
                    return int(datetime.strptime(dt_str, '%Y-%m-%d %H:%M').timestamp())
                else:
                    return int((now - timedelta(days=1)).timestamp())
            elif 'å‰å¤©' in time_str:
                return int((now - timedelta(days=2)).timestamp())
            
            # å°è¯•å¤šç§æ ¼å¼
            formats = [
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d %H:%M',
                '%Y/%m/%d %H:%M:%S',
                '%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S',
                '%Yå¹´%mæœˆ%dæ—¥',
                '%Y-%m-%d',
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(time_str[:19], fmt)
                    return int(dt.timestamp())
                except:
                    pass
            
            return int(datetime.now().timestamp())
        except:
            return int(datetime.now().timestamp())


async def save_article_to_db(article: Dict):
    """ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“"""
    try:
        async with get_session() as session:
            article_id = article.get('article_id')
            
            if 'url' in article:
                article['article_url'] = article.pop('url')
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            stmt = select(QbitaiArticle).where(QbitaiArticle.article_id == article_id)
            result = await session.execute(stmt)
            existing = result.scalar_one_or_none()
            
            if existing:
                logger.info(f"æ–‡ç« å·²å­˜åœ¨ï¼Œæ›´æ–°: {article_id}")
                existing.last_modify_ts = utils.get_current_timestamp()
                for key, value in article.items():
                    if hasattr(existing, key) and key not in ['id', 'add_ts']:
                        setattr(existing, key, value)
            else:
                logger.info(f"ä¿å­˜æ–°æ–‡ç« : {article_id}")
                article['add_ts'] = utils.get_current_timestamp()
                article['last_modify_ts'] = utils.get_current_timestamp()
                
                # è¿‡æ»¤æ‰ä¸åœ¨æ¨¡å‹ä¸­çš„å­—æ®µ
                valid_keys = {c.name for c in QbitaiArticle.__table__.columns}
                filtered_article = {k: v for k, v in article.items() if k in valid_keys}
                
                db_article = QbitaiArticle(**filtered_article)
                session.add(db_article)
            
            await session.commit()
            logger.info(f"æ–‡ç« ä¿å­˜æˆåŠŸ: {article_id}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        raise


async def save_comment_to_db(comment: Dict):
    """ä¿å­˜è¯„è®ºåˆ°æ•°æ®åº“"""
    try:
        async with get_session() as session:
            comment_id = comment.get('comment_id')
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            stmt = select(QbitaiArticleComment).where(QbitaiArticleComment.comment_id == comment_id)
            result = await session.execute(stmt)
            existing = result.scalar_one_or_none()
            
            if existing:
                logger.info(f"è¯„è®ºå·²å­˜åœ¨ï¼Œæ›´æ–°: {comment_id}")
                existing.last_modify_ts = utils.get_current_timestamp()
                for key, value in comment.items():
                    if hasattr(existing, key) and key not in ['id', 'add_ts']:
                        setattr(existing, key, value)
            else:
                logger.info(f"ä¿å­˜æ–°è¯„è®º: {comment_id}")
                comment['add_ts'] = utils.get_current_timestamp()
                comment['last_modify_ts'] = utils.get_current_timestamp()
                
                # è¿‡æ»¤æ‰ä¸åœ¨æ¨¡å‹ä¸­çš„å­—æ®µ
                valid_keys = {c.name for c in QbitaiArticleComment.__table__.columns}
                filtered_comment = {k: v for k, v in comment.items() if k in valid_keys}
                
                db_comment = QbitaiArticleComment(**filtered_comment)
                session.add(db_comment)
            
            await session.commit()
            logger.info(f"è¯„è®ºä¿å­˜æˆåŠŸ: {comment_id}")
    except Exception as e:
        logger.error(f"ä¿å­˜è¯„è®ºå¤±è´¥: {e}")


async def main():
    """ä¸»çˆ¬å–æµç¨‹"""
    logger.info("=" * 60)
    logger.info("ğŸš€ é‡å­ä½(QbitAI)çˆ¬è™«å¯åŠ¨")
    logger.info(f"ğŸ“ ç½‘å€: https://www.qbitai.com/")
    logger.info(f"ğŸ“… çˆ¬å–å‘¨æœŸ: è¿‘ä¸¤å‘¨å†…å®¹")
    logger.info("=" * 60)
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    end_date = datetime.now()
    start_date = end_date - timedelta(days=1)
    logger.info(f"â° æ—¶é—´èŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
    
    scraper = QbitaiWebScraper()
    await scraper.init()
    
    try:
        total_articles = 0
        total_comments = 0
        page = 1
        
        while True:
            # è·å–æ–‡ç« åˆ—è¡¨
            articles = await scraper.get_article_list(page=page)
            
            if not articles:
                logger.info("å·²åˆ°è¾¾æœ€åä¸€é¡µæˆ–æ²¡æœ‰æ›´å¤šæ–‡ç« ")
                break
            
            for article_item in articles:
                try:
                    # è·å–å®Œæ•´æ–‡ç« è¯¦æƒ…
                    article = await scraper.get_article_detail(
                        article_item['article_id'],
                        article_item['url']
                    )
                    
                    if article:
                        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                        article_date = article.get('publish_date')
                        if article_date < str(start_date.date()):
                            logger.info(f"æ–‡ç« æ—¥æœŸ {article_date} å·²è¶…å‡ºæ—¶é—´èŒƒå›´ï¼Œåœæ­¢çˆ¬å–")
                            await scraper.close()
                            return total_articles, total_comments
                        
                        # ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“
                        await save_article_to_db(article)
                        total_articles += 1
                        
                        # è·å–è¯„è®º
                        try:
                            comments = await scraper.get_comments(
                                article_item['article_id'],
                                article_item['url']
                            )
                            
                            for comment in comments:
                                try:
                                    await save_comment_to_db(comment)
                                    total_comments += 1
                                except Exception as e:
                                    logger.warning(f"ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
                        except Exception as e:
                            logger.warning(f"è·å–è¯„è®ºå¤±è´¥: {e}")
                        
                        # ç¤¼è²Œå»¶è¿Ÿ
                        await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                    continue
            
            page += 1
            # åˆ—è¡¨é¡µå»¶è¿Ÿ
            await asyncio.sleep(2)
        
        return total_articles, total_comments
    
    finally:
        await scraper.close()


if __name__ == "__main__":
    try:
        articles, comments = asyncio.run(main())
        logger.info("=" * 60)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ!")
        logger.info(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        logger.info(f"   - æ–‡ç« æ€»æ•°: {articles}")
        logger.info(f"   - è¯„è®ºæ€»æ•°: {comments}")
        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“")
        logger.info("=" * 60)
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        sys.exit(0)
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±è´¥: {e}")
        sys.exit(1)
