"""
GeminiAIReportAgent - æ™ºèƒ½æ–°é—»å¤„ç†Agent
å®ç°ï¼šè¿‡æ»¤ -> å½’ç±» -> å»é‡ -> æ’åº -> æŠ¥å‘Šç”Ÿæˆ çš„å¤šè½®æµç¨‹
"""

import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

from openai import OpenAI
import httpx
from sqlalchemy import select, desc, or_

from database.models import QbitaiArticle, CompanyArticle
from database.db_session import get_session
import config
from crawler import utils

settings = config.settings
logger = utils.logger


class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    def __init__(self, article_id: str, title: str, description: str, 
                 content: str, url: str, source: str, publish_time: int,
                 reference_links: Optional[str] = None):
        self.article_id = article_id
        self.title = title
        self.description = description
        self.content = content  # ä¿å­˜å®Œæ•´å†…å®¹ï¼Œåœ¨å…·ä½“ä½¿ç”¨æ—¶å†æŒ‰éœ€æˆªå–
        self.url = url
        self.source = source  # æ¥æºï¼šqbitai, openai, googleç­‰
        self.publish_time = publish_time
        self.reference_links = reference_links
        
        # å¤„ç†ç»“æœå­—æ®µ
        self.filter_decision = None  # "ä¿ç•™" or "å‰”é™¤"
        self.filter_reason = None
        self.event_id = None
        self.event_count = 0
        self.dedup_decision = None  # "ä¿ç•™" or "åˆ é™¤"
        self.dedup_reason = None
        self.tech_impact = 0
        self.industry_scope = 0
        self.hype_score = 0
        self.final_score = 0.0
        self.ranking_level = "C"
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "article_id": self.article_id,
            "title": self.title,
            "description": self.description,
            "content": self.content,
            "url": self.url,
            "source": self.source,
            "publish_time": self.publish_time,
            "reference_links": self.reference_links,
            "filter_decision": self.filter_decision,
            "filter_reason": self.filter_reason,
            "event_id": self.event_id,
            "event_count": self.event_count,
            "dedup_decision": self.dedup_decision,
            "dedup_reason": self.dedup_reason,
            "tech_impact": self.tech_impact,
            "industry_scope": self.industry_scope,
            "hype_score": self.hype_score,
            "final_score": self.final_score,
            "ranking_level": self.ranking_level
        }


class GeminiAIReportAgent:
    """åŸºäº Gemini çš„æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ Agent"""
    
    def __init__(self, max_retries: int = 5):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            max_retries: æ¯ä¸ªæ­¥éª¤çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.api_key = settings.REPORT_ENGINE_API_KEY
        self.base_url = settings.REPORT_ENGINE_BASE_URL
        self.model_name = settings.REPORT_ENGINE_MODEL_NAME or "gemini-3-pro-preview"
        self.max_retries = max_retries
        
        if not self.api_key:
            raise ValueError("API Key not configured in settings")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=httpx.Client(verify=False, timeout=120.0)
        )
        
        self.template_path = Path(__file__).parent / "templates" / "AIReport_example.md"
        
    async def fetch_articles_from_db(self, days: int = 3, limit: int = 200) -> List[NewsItem]:
        """
        ä»æ•°æ®åº“è·å–æ–°é—»æ•°æ®
        
        Args:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            limit: æ¯ä¸ªè¡¨çš„æœ€å¤§æ¡æ•°
            
        Returns:
            æ–°é—»æ¡ç›®åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨ä»æ•°æ®åº“è·å–æœ€è¿‘ {days} å¤©çš„æ–°é—»æ•°æ®...")
        
        cutoff_date = (datetime.now() - timedelta(days=days)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        cutoff_ts = int(cutoff_date.timestamp())
        
        news_items = []
        
        async with get_session() as session:
            # è·å–é‡å­ä½æ–‡ç« 
            stmt = (
                select(QbitaiArticle)
                .where(QbitaiArticle.publish_time >= cutoff_ts)
                .order_by(desc(QbitaiArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            qbitai_articles = result.scalars().all()
            
            for art in qbitai_articles:
                news_items.append(NewsItem(
                    article_id=f"qbitai_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source="é‡å­ä½",
                    publish_time=art.publish_time,
                    reference_links=art.reference_links
                ))
            
            # è·å–å…¬å¸å®˜æ–¹æ–‡ç« 
            stmt = (
                select(CompanyArticle)
                .where(CompanyArticle.publish_time >= cutoff_ts)
                .order_by(desc(CompanyArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            company_articles = result.scalars().all()
            
            for art in company_articles:
                news_items.append(NewsItem(
                    article_id=f"{art.company}_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source=art.company.upper(),
                    publish_time=art.publish_time,
                    reference_links=art.reference_links
                ))
        
        logger.info(f"å…±è·å– {len(news_items)} æ¡æ–°é—»æ•°æ®")
        return news_items
    
    def _call_llm(self, prompt: str, temperature: float = 0.1) -> Optional[str]:
        """
        è°ƒç”¨ LLM API
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            LLM å“åº”å†…å®¹
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"LLM API è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _parse_json_response(self, response: str) -> Optional[List[Dict]]:
        """
        è§£æ JSON å“åº”ï¼Œæ”¯æŒæå– markdown ä»£ç å—ä¸­çš„ JSON
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„ JSON åˆ—è¡¨
        """
        if not response:
            return None
        
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå– markdown ä»£ç å—ä¸­çš„ JSON
            import re
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except:
                    pass
            
            logger.error(f"æ— æ³•è§£æ JSON å“åº”: {response[:200]}")
            return None
    
    async def step1_filter(self, news_items: List[NewsItem], batch_size: int = 20) -> List[NewsItem]:
        """
        ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤ (Filtering)
        å‰”é™¤ä¸ AI æ ¸å¿ƒæŠ€æœ¯è¿›å±•æ— å…³çš„å™ªéŸ³ä¿¡æ¯
        
        Args:
            news_items: åŸå§‹æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬ä¸€æ­¥ã€‘å¼€å§‹è¿‡æ»¤ (Filtering)...")

        # 1. é¢„è¿‡æ»¤ï¼šå‰”é™¤å†…å®¹è¿‡çŸ­æˆ–æ— å†…å®¹çš„æ–°é—»
        valid_news_items = []
        for item in news_items:
            # ç®€å•çš„è§„åˆ™è¿‡æ»¤ï¼šå†…å®¹é•¿åº¦å°‘äº100å­—ç¬¦è§†ä¸ºæ— æ•ˆå†…å®¹
            # æ³¨æ„ï¼šNewsItem åˆå§‹åŒ–æ—¶å·²æˆªå–å‰1000å­—ç¬¦ï¼Œè¿™é‡Œåˆ¤æ–­çš„æ˜¯æˆªå–åçš„é•¿åº¦
            # ä½†å¦‚æœåŸå†…å®¹æœ¬èº«å°±å¾ˆå°‘ï¼Œè¿™é‡Œä¹Ÿèƒ½æ£€æµ‹å‡ºæ¥
            if item.content and len(item.content.strip()) >= 50:
                valid_news_items.append(item)
            else:
                logger.info(f"é¢„è¿‡æ»¤å‰”é™¤ï¼ˆå†…å®¹è¿‡å°‘ï¼‰: {item.title} (ID: {item.article_id})")
        
        news_items = valid_news_items
        logger.info(f"å¾…å¤„ç†æ–°é—»æ•°: {len(news_items)}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        filtered_items = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "content_snippet": item.content[:300],
                    "source": item.source,
                    "url": item.url
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯å†…å®¹ç­›é€‰ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œè¿‡æ»¤åˆ¤æ–­ã€‚

**è¿‡æ»¤è§„åˆ™ï¼š**

ã€ä¿ç•™æ¡ä»¶ã€‘ï¼ˆé€»è¾‘æˆ–ï¼Œæ»¡è¶³å…¶ä¸€å³ä¿ç•™ï¼‰:
1. æŠ€æœ¯/èƒ½åŠ›è¿›å±•: æ ¸å¿ƒå†…å®¹æ˜¯å…³äº AI æŠ€æœ¯ã€æ¨¡å‹ã€ç³»ç»Ÿã€å·¥ç¨‹æˆ–åº”ç”¨èƒ½åŠ›çš„å…·ä½“è¿›å±•ã€‚
2. å…³é”®é¢†åŸŸ: æ˜ç¡®æ¶‰åŠåŸºç¡€æ¨¡å‹ã€è®­ç»ƒ/æ¨ç†æ–¹æ³•ã€æ•°æ®å·¥ç¨‹ã€AI Infraã€Agent æ¡†æ¶æˆ–ç›¸å…³æŠ€æœ¯äº§å“ã€‚
3. æƒå¨æ¥æº: ä¿¡æ¯æ¥æºä¸ºå­¦æœ¯è®ºæ–‡ (å¦‚ arXiv)ã€å®˜æ–¹æŠ€æœ¯åšå®¢ (å¦‚ OpenAI Blog)ã€å®˜æ–¹äº§å“å‘å¸ƒé¡µæˆ– GitHub Release Notesã€‚

ã€å‰”é™¤æ¡ä»¶ã€‘ï¼ˆé€»è¾‘æˆ–ï¼Œæ»¡è¶³å…¶ä¸€å³å‰”é™¤ï¼‰:
1. å•†ä¸š/é‡‘è: è‚¡ä»·ã€å¸‚å€¼ã€èèµ„ã€IPOã€ä¼°å€¼ã€è´¢æŠ¥ã€æ”¶å…¥ã€ç”¨æˆ·è§„æ¨¡ã€æ”¶è´­ã€å¹¶è´­ã€‚
2. å¸‚åœºåˆ†æ: æŠ•èµ„è§‚ç‚¹ã€å¸‚åœºæƒ…ç»ªã€èµ„æœ¬åŠ¨å‘ã€æ— ç›´æ¥æŠ€æœ¯å…³è”çš„å•†ä¸šåˆä½œæ–°é—»ã€‚
3. äºŒæ¬¡è§£è¯»: ä¸ªäººè§‚ç‚¹ã€KOL é•¿ç¯‡åˆ†æã€æ— å¼•ç”¨çš„æ¨æ–‡æ€»ç»“ã€‚
4. ä¿¡æºä¸æ˜: æœªæ ‡æ³¨æ˜ç¡®æ¥æºã€æ¥æºä¸ºåŒ¿åè®ºå›æˆ–ç¤¾äº¤ç¾¤èŠæˆªå›¾ã€‚

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- filter_decision: "ä¿ç•™" æˆ– "å‰”é™¤"
- filter_reason: åˆ¤æ–­ç†ç”±ï¼ˆä¸€å¥è¯ç®€è¿°ï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "filter_decision": "ä¿ç•™", "filter_reason": "æ¶‰åŠå¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯çªç ´"}},
  {{"article_id": "yyy", "filter_decision": "å‰”é™¤", "filter_reason": "ä¸»è¦è®¨è®ºèèµ„å’Œå¸‚å€¼"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    # æ›´æ–°æ–°é—»æ¡ç›®
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.filter_decision = r.get("filter_decision")
                            item.filter_reason = r.get("filter_reason")
                            
                            if item.filter_decision == "ä¿ç•™":
                                filtered_items.append(item)
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è§£æå¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥ï¼Œè·³è¿‡")
            
            # é¿å… API é™æµ
            await asyncio.sleep(1)
        
        logger.info(f"è¿‡æ»¤å®Œæˆï¼šä¿ç•™ {len(filtered_items)}/{len(news_items)} æ¡æ–°é—»")
        return filtered_items
    
    async def step2_cluster(self, news_items: List[NewsItem], batch_size: int = 30) -> List[NewsItem]:
        """
        ç¬¬äºŒæ­¥ï¼šå½’ç±» (Clustering)
        å°†æè¿°åŒä¸€äº‹ä»¶çš„æ–°é—»èšåˆåœ¨ä¸€èµ·
        
        Args:
            news_items: è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å½’ç±»åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äºŒæ­¥ã€‘å¼€å§‹å½’ç±» (Clustering)...")
        logger.info(f"å¾…å¤„ç†æ–°é—»æ•°: {len(news_items)}")
        
        # åˆ†æ‰¹å¤„ç†
        all_events = {}  # event_id -> [NewsItem]
        
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source
                })
            
            # åŒ…å«ä¹‹å‰å·²è¯†åˆ«çš„äº‹ä»¶ä¿¡æ¯
            existing_events_info = ""
            if all_events:
                existing_events_info = "\nå·²è¯†åˆ«çš„äº‹ä»¶IDåˆ—è¡¨ï¼ˆä¾›å‚è€ƒï¼‰:\n"
                for event_id, items in all_events.items():
                    existing_events_info += f"- {event_id}: {items[0].title[:50]}...\n"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»äº‹ä»¶èšç±»ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œè¯­ä¹‰å½’ç±»ã€‚

**å½’ç±»æ ‡å‡†ï¼š**
- æŒ‰"åŒä¸€æŠ€æœ¯äº‹ä»¶ / æ¨¡å‹ç‰ˆæœ¬ / äº§å“å‘å¸ƒ / å…³é”®è®ºæ–‡"è¿›è¡Œè¯­ä¹‰å½’ç±»ã€‚
- ä¾‹å¦‚ï¼š"GPT-5 å‘å¸ƒ"ã€"Llama 3.1 å¼€æº"ã€"DeepMind æå‡ºæ–° AlphaFold ç®—æ³•"ç­‰å‡å±äºç‹¬ç«‹çš„è¯­ä¹‰äº‹ä»¶ã€‚
- å¦‚æœå¤šæ¡æ–°é—»è®¨è®ºçš„æ˜¯åŒä¸€ä¸ªäº‹ä»¶ï¼ˆå¦‚åŒä¸€ä¸ªæ¨¡å‹å‘å¸ƒã€åŒä¸€ç¯‡è®ºæ–‡ã€åŒä¸€ä¸ªæŠ€æœ¯çªç ´ï¼‰ï¼Œå®ƒä»¬åº”è¯¥è¢«å½’ä¸ºåŒä¸€ä¸ª event_idã€‚
- event_id åº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„è‹±æ–‡çŸ­è¯­ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Œä¾‹å¦‚ï¼šgpt5_release_2025_q4, llama3_1_opensource

{existing_events_info}

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- event_id: äº‹ä»¶IDï¼ˆä½¿ç”¨æœ‰æ„ä¹‰çš„è‹±æ–‡çŸ­è¯­ï¼Œå¦‚æœä¸å·²è¯†åˆ«çš„äº‹ä»¶ç›¸åŒï¼Œè¯·ä½¿ç”¨ç›¸åŒçš„event_idï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "event_id": "gpt5_release"}},
  {{"article_id": "yyy", "event_id": "llama3_1_opensource"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    # æ›´æ–°æ–°é—»æ¡ç›®
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            event_id = result_map[item.article_id].get("event_id")
                            item.event_id = event_id
                            
                            if event_id not in all_events:
                                all_events[event_id] = []
                            all_events[event_id].append(item)
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è§£æå¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥")
            
            await asyncio.sleep(1)
        
        # æ›´æ–°æ¯æ¡æ–°é—»çš„ event_count
        for event_id, items in all_events.items():
            count = len(items)
            for item in items:
                item.event_count = count
        
        logger.info(f"å½’ç±»å®Œæˆï¼šè¯†åˆ«å‡º {len(all_events)} ä¸ªç‹¬ç«‹äº‹ä»¶")
        for event_id, items in sorted(all_events.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            logger.info(f"  - {event_id}: {len(items)} æ¡æ–°é—»")
        
        return news_items
    
    async def step3_deduplicate(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """
        ç¬¬ä¸‰æ­¥ï¼šå»é‡ (Deduplication)
        åœ¨æ¯ä¸ªäº‹ä»¶ä¸­ï¼Œä»…ä¿ç•™ä¸€æ¡æœ€æƒå¨ã€ä¿¡æ¯è´¨é‡æœ€é«˜çš„æ–°é—»
        
        Args:
            news_items: å½’ç±»åçš„æ–°é—»åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬ä¸‰æ­¥ã€‘å¼€å§‹å»é‡ (Deduplication)...")
        
        # æŒ‰ event_id åˆ†ç»„
        events = defaultdict(list)
        for item in news_items:
            events[item.event_id].append(item)
        
        logger.info(f"å¾…å¤„ç†äº‹ä»¶æ•°: {len(events)}")
        
        deduplicated_items = []
        
        for event_id, items in events.items():
            if len(items) == 1:
                # åªæœ‰ä¸€æ¡æ–°é—»ï¼Œç›´æ¥ä¿ç•™
                items[0].dedup_decision = "ä¿ç•™"
                items[0].dedup_reason = "å”¯ä¸€æ¥æº"
                deduplicated_items.append(items[0])
                continue
            
            logger.info(f"å¤„ç†äº‹ä»¶: {event_id} ({len(items)} æ¡æ–°é—»)")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in items:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source,
                    "url": item.url,
                    "publish_time": datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»å»é‡ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯æè¿°åŒä¸€äº‹ä»¶çš„å¤šæ¡æ–°é—»ï¼Œè¯·é€‰å‡ºæœ€æƒå¨ã€ä¿¡æ¯è´¨é‡æœ€é«˜çš„ä¸€æ¡ã€‚

**ä¿ç•™ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š**
1. å®˜æ–¹æ ¸å¿ƒä¿¡æº: å®˜ç½‘å‘å¸ƒã€å®˜æ–¹åšå®¢ã€arXiv è®ºæ–‡ã€GitHub Release
2. æ ¸å¿ƒäººå‘˜è§£è¯»: ä½œè€…ã€æ ¸å¿ƒå·¥ç¨‹å¸ˆæˆ–å®˜æ–¹ç ”ç©¶å‘˜çš„æ·±åº¦è§£è¯»
3. æƒå¨æŠ€æœ¯åª’ä½“: å¯¹ä¸Šè¿°ä¿¡æºçš„æ·±åº¦ã€å¿«é€Ÿè½¬è¿°æŠ¥é“
4. ç¤¾äº¤åª’ä½“/æ™®é€šè½¬è¿°: ä¼˜å…ˆçº§æœ€ä½

**äº‹ä»¶ID:** {event_id}

**æ–°é—»åˆ—è¡¨ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·é€‰æ‹©ä¸€æ¡æœ€æƒå¨çš„æ–°é—»ä¿ç•™ï¼Œå…¶ä½™æ ‡è®°ä¸ºåˆ é™¤ã€‚ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼š
- article_id: æ–‡ç« ID
- dedup_decision: "ä¿ç•™" æˆ– "åˆ é™¤"
- dedup_reason: åˆ¤æ–­ç†ç”±ï¼ˆä¸€å¥è¯ï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "dedup_decision": "ä¿ç•™", "dedup_reason": "å®˜æ–¹åšå®¢é¦–å‘"}},
  {{"article_id": "yyy", "dedup_decision": "åˆ é™¤", "dedup_reason": "äºŒæ¬¡è½¬è¿°"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    result_map = {r["article_id"]: r for r in results}
                    for item in items:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.dedup_decision = r.get("dedup_decision")
                            item.dedup_reason = r.get("dedup_reason")
                            
                            if item.dedup_decision == "ä¿ç•™":
                                deduplicated_items.append(item)
                    break
                else:
                    logger.warning(f"äº‹ä»¶ {event_id} å»é‡å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        # å¤±è´¥æ—¶ä¿ç•™ç¬¬ä¸€æ¡
                        logger.error(f"äº‹ä»¶ {event_id} å»é‡å¤±è´¥ï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€æ¡")
                        items[0].dedup_decision = "ä¿ç•™"
                        items[0].dedup_reason = "å»é‡å¤±è´¥ï¼Œé»˜è®¤ä¿ç•™"
                        deduplicated_items.append(items[0])
            
            await asyncio.sleep(0.5)
        
        logger.info(f"å»é‡å®Œæˆï¼šä¿ç•™ {len(deduplicated_items)}/{len(news_items)} æ¡æ–°é—»")
        return deduplicated_items
    
    async def step4_rank(self, news_items: List[NewsItem], batch_size: int = 20) -> List[NewsItem]:
        """
        ç¬¬å››æ­¥ï¼šæ’åº (Ranking)
        å¯¹æœ€ç»ˆä¿ç•™çš„æ–°é—»æ¡ç›®è¿›è¡Œä»·å€¼åˆ¤æ–­
        
        Args:
            news_items: å»é‡åçš„æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ’åºåçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬å››æ­¥ã€‘å¼€å§‹æ’åº (Ranking)...")
        logger.info(f"å¾…è¯„åˆ†æ–°é—»æ•°: {len(news_items)}")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source,
                    "event_count": item.event_count
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯å½±å“åŠ›è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œä»·å€¼è¯„åˆ†ã€‚

**è¯„åˆ†ç»´åº¦ï¼š**

1. **æŠ€æœ¯å½±å“åŠ› (tech_impact)** [1-5åˆ†]:
   - 5åˆ† (èŒƒå¼è½¬æ¢): æå‡ºå…¨æ–°æ¶æ„æˆ–ç†è®ºï¼Œå¯èƒ½æ”¹å˜ä¸€ä¸ªé¢†åŸŸçš„èµ°å‘ (å¦‚ Transformer)
   - 4åˆ† (é‡å¤§çªç ´): åœ¨å…³é”®èƒ½åŠ›ä¸Šæœ‰å·¨å¤§æå‡æˆ–å¼€æºäº†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹
   - 3åˆ† (æ˜¾è‘—æ”¹è¿›): ç°æœ‰æ–¹æ³•ä¸Šçš„é‡è¦æ”¹è¿›ï¼Œæˆ–å‘å¸ƒäº†éå¸¸æœ‰ç”¨çš„å·¥å…·/æ¡†æ¶
   - 2åˆ† (å¸¸è§„ä¼˜åŒ–): æ€§èƒ½çš„å°å¹…æå‡æˆ–å¸¸è§„ç‰ˆæœ¬è¿­ä»£
   - 1åˆ† (å¾®å°æ”¹è¿›): å¢é‡å¼æ›´æ–°

2. **è¡Œä¸šå½±å“èŒƒå›´ (industry_scope)** [1-5åˆ†]:
   - 5åˆ† (å…¨è¡Œä¸š): å¯¹å‡ ä¹æ‰€æœ‰ AI åº”ç”¨å¼€å‘è€…å’Œå…¬å¸éƒ½äº§ç”Ÿå½±å“
   - 4åˆ† (å¤šé¢†åŸŸ): å½±å“å¤šä¸ªä¸»è¦ AI åº”ç”¨é¢†åŸŸ (å¦‚ NLP, CV)
   - 3åˆ† (ç‰¹å®šé¢†åŸŸ): æ·±åº¦å½±å“ä¸€ä¸ªå‚ç›´é¢†åŸŸ (å¦‚ AI for Science)
   - 2åˆ† (ç‰¹å®šä»»åŠ¡): ä¸»è¦å½±å“ä¸€ä¸ªæˆ–å°‘æ•°å‡ ä¸ªå…·ä½“ä»»åŠ¡
   - 1åˆ† (å°ä¼—åœºæ™¯): å½±å“èŒƒå›´éå¸¸æœ‰é™

3. **çƒ­åº¦ (hype_score)** [1-5åˆ†]:
   - æ ¹æ® event_count æ˜ å°„ï¼š
     * 1-2ç¯‡ â†’ 1åˆ†
     * 3-5ç¯‡ â†’ 2åˆ†
     * 6-10ç¯‡ â†’ 3åˆ†
     * 11-20ç¯‡ â†’ 4åˆ†
     * >20ç¯‡ â†’ 5åˆ†

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- tech_impact: æŠ€æœ¯å½±å“åŠ›è¯„åˆ† (1-5)
- industry_scope: è¡Œä¸šå½±å“èŒƒå›´è¯„åˆ† (1-5)
- hype_score: çƒ­åº¦è¯„åˆ† (1-5ï¼Œæ ¹æ®event_countè®¡ç®—)

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "tech_impact": 5, "industry_scope": 5, "hype_score": 4}},
  {{"article_id": "yyy", "tech_impact": 3, "industry_scope": 3, "hype_score": 2}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.tech_impact = r.get("tech_impact", 2)
                            item.industry_scope = r.get("industry_scope", 2)
                            item.hype_score = r.get("hype_score", 1)
                            
                            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
                            item.final_score = (
                                item.tech_impact * 0.5 +
                                item.industry_scope * 0.3 +
                                item.hype_score * 0.2
                            )
                            
                            # è¯„çº§æ˜ å°„
                            if item.final_score >= 4.2:
                                item.ranking_level = "S"
                            elif item.final_score >= 3.5:
                                item.ranking_level = "A"
                            elif item.final_score >= 2.8:
                                item.ranking_level = "B"
                            else:
                                item.ranking_level = "C"
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è¯„åˆ†å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} è¯„åˆ†å¤±è´¥")
            
            await asyncio.sleep(1)
        
        # æŒ‰è¯„åˆ†æ’åº
        news_items.sort(key=lambda x: x.final_score, reverse=True)
        
        logger.info(f"æ’åºå®Œæˆï¼š")
        logger.info(f"  Sçº§: {sum(1 for x in news_items if x.ranking_level == 'S')} æ¡")
        logger.info(f"  Açº§: {sum(1 for x in news_items if x.ranking_level == 'A')} æ¡")
        logger.info(f"  Bçº§: {sum(1 for x in news_items if x.ranking_level == 'B')} æ¡")
        logger.info(f"  Cçº§: {sum(1 for x in news_items if x.ranking_level == 'C')} æ¡")
        
        return news_items
    
    def _validate_news_item_format(self, content: str) -> Tuple[bool, str]:
        """éªŒè¯æ–°é—»æ¡ç›®çš„ Markdown æ ¼å¼"""
        required_patterns = [
            (r"### \*\*.*?\*\*", "æ ‡é¢˜æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º ### **æ ‡é¢˜**"),
            (r"\[é˜…è¯»åŸæ–‡\]\(.*?\)", "ç¼ºå°‘é˜…è¯»åŸæ–‡é“¾æ¥æˆ–æ ¼å¼é”™è¯¯"),
            (r"> \*\*æ¦‚è¦\*\*:.*", "ç¼ºå°‘æ¦‚è¦æˆ–æ ¼å¼é”™è¯¯"),
            (r"\*\*ğŸ’¡å†…å®¹è¯¦è§£\*\*", "ç¼ºå°‘'ğŸ’¡å†…å®¹è¯¦è§£'åˆ†èŠ‚"),
            (r"- \*\*.*?\*\*", "ç¼ºå°‘è¦ç‚¹æ ‡é¢˜æˆ–æ ¼å¼é”™è¯¯")
        ]
        
        import re
        for pattern, error_msg in required_patterns:
            if not re.search(pattern, content, re.MULTILINE):
                return False, error_msg
        return True, ""

    async def _generate_news_entries_batch(self, batch_items: List[NewsItem]) -> List[Dict[str, str]]:
        """
        åˆ†æ‰¹ç”Ÿæˆæ–°é—»æ¡ç›®å†…å®¹ (å¹¶å‘å¤„ç†)
        
        Args:
            batch_items: è¿™ä¸€æ‰¹çš„æ–°é—»åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ¡ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {"article_id", "category", "markdown_content"}
        """
        batch_data = []
        for item in batch_items:
            pub_date = datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
            batch_data.append({
                "article_id": item.article_id,
                "title": item.title,
                "source": item.source,
                "url": item.url,
                "publish_time": pub_date,
                "content": item.content,  # ä½¿ç”¨å®Œæ•´å†…å®¹è¿›è¡Œæ·±åº¦é˜…è¯»
            })

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯åˆ†æå¸ˆã€‚è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆç¬¦åˆæŠ¥å‘Šæ ¼å¼çš„Markdownå†…å®¹å—ã€‚

**è¾“å‡ºè¦æ±‚ï¼š**
å¯¹äºæ¯ä¸€æ¡æ–°é—»ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. **åˆ†ç±»**ï¼šå°†å…¶å½’å…¥ä»¥ä¸‹ä¸‰ç±»ä¹‹ä¸€ï¼š
   - "Infrastructure" (AIåŸºç¡€è®¾æ–½: èŠ¯ç‰‡, ç®—åŠ›, æ¡†æ¶, æ•°æ®å·¥ç¨‹ç­‰)
   - "Model" (AIæ¨¡å‹ä¸æŠ€æœ¯: åŸºç¡€æ¨¡å‹, ç®—æ³•åˆ›æ–°, è®­ç»ƒæŠ€æœ¯ç­‰)
   - "Application" (AIåº”ç”¨ä¸æ™ºèƒ½ä½“: å…·ä½“åº”ç”¨, Agent, è¡Œä¸šè½åœ°ç­‰)

2. **ç”ŸæˆMarkdownå†…å®¹**ï¼šä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownæ ¼å¼æ¨¡æ¿ç”Ÿæˆå†…å®¹ã€‚
   
   **æ¨¡æ¿æ ¼å¼ï¼š**
   ```markdown
   ### **[æ–°é—»æ ‡é¢˜]**
   
   [é˜…è¯»åŸæ–‡]([URL])  `[Publish_Time]`
   
   > **æ¦‚è¦**: [3-4å¥è¯ç®€ç»ƒæ¦‚æ‹¬æ ¸å¿ƒäº‹ä»¶]
   
   **ğŸ’¡å†…å®¹è¯¦è§£**
   (å†…å®¹è¯¦è§£æ˜¯å¯¹å…³é”®æŠ€æœ¯çš„ç½—åˆ—ï¼Œå…³é”®ç‚¹æ•°é‡è‡³å°‘å¤§äº3ç‚¹ï¼Œè¯·å¯¹å…³é”®æŠ€æœ¯è¿›è¡Œè¯¦ç»†è§£è¯»ï¼Œæ­¤å¤„ä¸ç”¨æ·»åŠ æ¦‚è¿°)

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 1**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - [å…³é”®ç‚¹è§£é‡Š1]
        è¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œä¸è¶…è¿‡200å­—
        - [å…³é”®ç‚¹è§£é‡Š2]
        è¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œä¸è¶…è¿‡200å­—
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 2**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - [å…³é”®ç‚¹è§£é‡Š1]
        è¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œä¸è¶…è¿‡200å­—
        - [å…³é”®ç‚¹è§£é‡Š2]
        è¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œä¸è¶…è¿‡200å­—
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 3**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - [å…³é”®ç‚¹è§£é‡Š1]
        è¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œä¸è¶…è¿‡200å­—
        - [å…³é”®ç‚¹è§£é‡Š2]
        è¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œä¸è¶…è¿‡200å­—
        â€¦â€¦
    â€¦â€¦
   ```

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¿”å›æ ¼å¼ï¼š**
è¯·è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«æ¯æ¡æ–°é—»çš„ç”Ÿæˆç»“æœï¼š
```json
[
  {{
    "article_id": "xxx",
    "category": "Infrastructure", 
    "markdown_content": "### **æ ‡é¢˜**..."
  }},
  ...
]
```
è¯·åªè¿”å› JSONã€‚
"""
        
        for retry in range(self.max_retries):
            response = self._call_llm(prompt, temperature=0.3)
            results = self._parse_json_response(response)
            if results:
                # éªŒè¯æ ¼å¼
                valid_results = []
                errors = []
                for item in results:
                    is_valid, error = self._validate_news_item_format(item.get("markdown_content", ""))
                    if is_valid:
                        valid_results.append(item)
                    else:
                        errors.append(f"æ–‡ç«  '{item.get('title', 'Unknown')}' æ ¼å¼é”™è¯¯: {error}")
                
                if not errors:
                    return valid_results
                
                # å¦‚æœæœ‰é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œå°†é”™è¯¯åŠ å…¥ prompt é‡è¯•
                logger.warning(f"æ‰¹æ¬¡ç”Ÿæˆå­˜åœ¨æ ¼å¼é”™è¯¯: {'; '.join(errors)}")
                if retry < self.max_retries - 1:
                    prompt += f"\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆå­˜åœ¨ä»¥ä¸‹æ ¼å¼é”™è¯¯ï¼Œè¯·ä¸¥æ ¼ä¿®æ­£ï¼Œç¡®ä¿Markdownæ ¼å¼å®Œå…¨ç¬¦åˆæ¨¡æ¿ï¼š\n" + "\n".join(errors)
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•ï¼Œä»…è¿”å›æœ‰æ•ˆçš„
                    return valid_results
            
            await asyncio.sleep(1)
            
        return []

    async def generate_final_report(self, news_items: List[NewsItem], quality_check: bool = True) -> Optional[str]:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (å¤šè½®ç”Ÿæˆæ¨¡å¼)
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äº”æ­¥ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (å¤šè½®ç”Ÿæˆæ¨¡å¼)...")
        
        if not news_items:
            logger.warning("æ²¡æœ‰æ–°é—»å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return None

        # 1. å‡†å¤‡æ•°æ®ï¼šç­›é€‰ S/A/B çº§æ–°é—»è¿›å…¥æ­£æ–‡
        valid_items = [item for item in news_items if item.ranking_level in ["S", "A", "B"]]
        # å¦‚æœ S/A/B å¤ªå°‘ï¼Œè€ƒè™‘æŠŠ C çº§çš„å‰å‡ ååŠ è¿›æ¥
        if len(valid_items) < 5:
             c_items = [item for item in news_items if item.ranking_level == "C"]
             valid_items.extend(c_items[:5])
        
        # ç¡®ä¿æŒ‰åˆ†æ•°æ’åº
        valid_items.sort(key=lambda x: x.final_score, reverse=True)
        
        logger.info(f"å°†ä¸º {len(valid_items)} æ¡é«˜ä»·å€¼æ–°é—»ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")

        # 2. åˆ†æ‰¹ç”Ÿæˆå†…å®¹ (Batch Processing)
        batch_size = 5
        generated_entries = []
        
        for i in range(0, len(valid_items), batch_size):
            batch = valid_items[i:i + batch_size]
            logger.info(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Šè¯¦æƒ…ï¼šæ‰¹æ¬¡ {i // batch_size + 1} (å…± {len(batch)} æ¡)")
            
            # å¹¶å‘ç”Ÿæˆè¯¥æ‰¹æ¬¡çš„å†…å®¹
            entries = await self._generate_news_entries_batch(batch)
            if entries:
                generated_entries.extend(entries)
            else:
                logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} ç”Ÿæˆå¤±è´¥")

        # 3. ç»„ç»‡å†…å®¹
        # å»ºç«‹ article_id åˆ° news_item çš„æ˜ å°„ï¼Œæ–¹ä¾¿è·å–é¢å¤–ä¿¡æ¯
        item_map = {item.article_id: item for item in valid_items}
        
        category_map = {
            "Infrastructure": [],
            "Model": [],
            "Application": []
        }
        
        for entry in generated_entries:
            cat = entry.get("category", "Model")
            if cat not in category_map:
                cat = "Model"  # Fallback
            category_map[cat].append(entry.get("markdown_content", ""))

        # 4. ç”Ÿæˆâ€œæœ¬æœŸé€Ÿè§ˆâ€ (Top 10)
        top_items = valid_items[:10]
        overview_prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆâ€œæœ¬æœŸé€Ÿè§ˆâ€åˆ—è¡¨ã€‚
è¦æ±‚ï¼š
- æ¯æ¡æ–°é—»ç”¨ä¸€è¡Œ Markdown åˆ—è¡¨é¡¹è¡¨ç¤ºã€‚
- æ ¼å¼ï¼š* **[[æ ‡ç­¾]]** [**æ–°é—»æ ‡é¢˜**]: [1-2å¥è¯æ ¸å¿ƒçœ‹ç‚¹]
- æ ‡ç­¾ç¤ºä¾‹ï¼š[å¤§æ¨¡å‹], [èŠ¯ç‰‡], [åº”ç”¨]ç­‰
- å¿…é¡»ä¸¥æ ¼éµå®ˆä¸Šè¿°æ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚

æ–°é—»æ•°æ®ï¼š
{json.dumps([{"title": item.title, "description": item.description} for item in top_items], ensure_ascii=False, indent=2)}

è¯·ç›´æ¥è¿”å› Markdown åˆ—è¡¨ã€‚
"""
        overview_content = self._call_llm(overview_prompt) or "ç”Ÿæˆå¤±è´¥"
        
        # ç®€å•éªŒè¯æ¦‚è§ˆæ ¼å¼
        if "**[[" not in overview_content:
             logger.warning("æ¦‚è§ˆæ ¼å¼å¯èƒ½ä¸ç¬¦åˆè¦æ±‚ï¼Œå°è¯•ä¿®å¤...")
             # ç®€å•çš„é‡è¯•é€»è¾‘
             overview_prompt += "\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ç¡®ä¿æ¯è¡Œä»¥ '* **[[æ ‡ç­¾]]**' å¼€å¤´ã€‚"
             retry_content = self._call_llm(overview_prompt)
             if retry_content and "**[[" in retry_content:
                 overview_content = retry_content

        # 5. ç”Ÿæˆâ€œæ‹“å±•é˜…è¯»â€ (Reference Links)
        # è¿™é‡Œæ”¶é›†æ‰€æœ‰æ–°é—»ï¼ˆåŒ…æ‹¬ C çº§ï¼‰çš„å‚è€ƒé“¾æ¥
        reference_section = ""
        all_ref_links = []
        seen_urls = set()
        
        for item in news_items:
            if item.reference_links:
                try:
                    refs = json.loads(item.reference_links)
                    for ref in refs:
                        if ref['url'] not in seen_urls:
                            all_ref_links.append(f"* [{ref['title']}]({ref['url']})")
                            seen_urls.add(ref['url'])
                except:
                    pass
        
        if all_ref_links:
            reference_section = "\\n".join(all_ref_links[:30]) # é™åˆ¶æ•°é‡é˜²æ­¢è¿‡é•¿

        # 6. æœ€ç»ˆç»„è£…
        publish_times = [item.publish_time for item in news_items if item.publish_time]
        if publish_times:
            date_range_start = datetime.fromtimestamp(min(publish_times)).strftime('%Y-%m-%d')
            date_range_end = datetime.fromtimestamp(max(publish_times)).strftime('%Y-%m-%d')
        else:
            today_str = datetime.now().strftime('%Y-%m-%d')
            date_range_start = today_str
            date_range_end = today_str

        final_report = f"""# AI å‰æ²¿åŠ¨æ€é€ŸæŠ¥ ({date_range_start} è‡³ {date_range_end})

## âš¡ æœ¬æœŸé€Ÿè§ˆ

{overview_content}

---

## 1. AI åŸºç¡€è®¾æ–½

{chr(10).join(category_map["Infrastructure"]) if category_map["Infrastructure"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## 2. AI æ¨¡å‹ä¸æŠ€æœ¯

{chr(10).join(category_map["Model"]) if category_map["Model"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## 3. AI åº”ç”¨ä¸æ™ºèƒ½ä½“

{chr(10).join(category_map["Application"]) if category_map["Application"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## æ‹“å±•é˜…è¯»

*(ç²¾é€‰ç›¸å…³è®ºæ–‡ä¸åŸå§‹é“¾æ¥)*

{reference_section}
"""
        return final_report

    def generate_final_report_old(self, news_items: List[NewsItem], quality_check: bool = True) -> Optional[str]:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äº”æ­¥ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        
        if not news_items:
            logger.warning("æ²¡æœ‰æ–°é—»å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return None
        
        # è¯»å–æ¨¡æ¿
        try:
            with open(self.template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        except Exception as e:
            logger.error(f"è¯»å–æ¨¡æ¿å¤±è´¥: {e}")
            return None
        
        # æŒ‰è¯„çº§åˆ†ç»„
        news_by_level = defaultdict(list)
        for item in news_items:
            news_by_level[item.ranking_level].append(item)
        
        # è®¡ç®—è¦†ç›–æ—¥æœŸèŒƒå›´ï¼ˆç”¨äºæç¤ºæ¨¡å‹æ­£ç¡®å¡«å†™å¤´éƒ¨åŒºé—´ï¼‰
        publish_times = [item.publish_time for item in news_items if item.publish_time]
        if publish_times:
            date_range_start = datetime.fromtimestamp(min(publish_times)).strftime('%Y-%m-%d')
            date_range_end = datetime.fromtimestamp(max(publish_times)).strftime('%Y-%m-%d')
        else:
            today_str = datetime.now().strftime('%Y-%m-%d')
            date_range_start = today_str
            date_range_end = today_str

        # æ ¼å¼åŒ–æ–°é—»æ•°æ®
        formatted_news = ""
        for level in ["S", "A", "B", "C"]:
            items = news_by_level[level]
            if items:
                formatted_news += f"\n## {level}çº§æ–°é—» ({len(items)}æ¡)\n\n"
                for i, item in enumerate(items, 1):
                    pub_date = datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
                    formatted_news += f"### [{i}] {item.title}\n"
                    formatted_news += f"- **æ¥æº**: {item.source}\n"
                    formatted_news += f"- **é“¾æ¥**: {item.url}\n"
                    formatted_news += f"- **å‘å¸ƒæ—¶é—´**: {pub_date}\n"
                    formatted_news += f"- **è¯„åˆ†**: {item.final_score:.2f} (æŠ€æœ¯å½±å“:{item.tech_impact}, è¡Œä¸šèŒƒå›´:{item.industry_scope}, çƒ­åº¦:{item.hype_score})\n"
                    formatted_news += f"- **äº‹ä»¶çƒ­åº¦**: {item.event_count} æ¡ç›¸å…³æŠ¥é“\n"
                    formatted_news += f"- **æ‘˜è¦**: {item.description}\n"
                    
                    # æ·»åŠ å‚è€ƒé“¾æ¥
                    if item.reference_links:
                        try:
                            ref_links = json.loads(item.reference_links)
                            if ref_links:
                                formatted_news += f"- **åŸå§‹æ¥æº**:\n"
                                for ref in ref_links:
                                    formatted_news += f"  - [{ref['title']}]({ref['url']}) (ç±»å‹: {ref['type']})\n"
                        except:
                            pass
                    
                    formatted_news += "\n"
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå‰æ²¿ç§‘æŠ€åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ç»è¿‡ç­›é€‰ã€å½’ç±»ã€å»é‡å’Œæ’åºçš„æ–°é—»æ•°æ®ï¼Œç¼–å†™ä¸€ä»½é«˜è´¨é‡çš„AIå‰æ²¿åŠ¨æ€é€ŸæŠ¥ã€‚

**å½“å‰æ—¥æœŸ**: {today_str}
**æŠ¥å‘Šè¦†ç›–æ—¥æœŸèŒƒå›´ï¼ˆåŠ¡å¿…ç”¨äºæ–‡é¦–æ‹¬å·åŒºé—´ï¼‰**: {date_range_start} è‡³ {date_range_end}

**æŠ¥å‘Šæ¨¡æ¿å’Œè¦æ±‚:**
{template_content}

**ç»è¿‡å¤„ç†çš„æ–°é—»æ•°æ®:**
{formatted_news}

**ç‰¹åˆ«æŒ‡ä»¤:**
1. ä¸¥æ ¼éµå¾ªæ¨¡æ¿æ ¼å¼ï¼Œä¸€çº§æ ‡é¢˜å’ŒäºŒçº§æ ‡é¢˜å¿…é¡»ä¸æ¨¡æ¿ä¸€è‡´
2. æ–°é—»å·²æŒ‰ S/A/B/C çº§åˆ«æ’åºï¼Œä¼˜å…ˆå…³æ³¨ S çº§å’Œ A çº§æ–°é—»
3. æ·±åº¦è§£è¯»éƒ¨åˆ†è¦æœ‰å®è´¨å†…å®¹ï¼Œç»“åˆæŠ€æœ¯èƒŒæ™¯å’Œè¡Œä¸šå½±å“è¿›è¡Œåˆ†æ
4. å¦‚æœæ–°é—»æ•°æ®ä¸­æä¾›äº†"åŸå§‹æ¥æº"ï¼Œé˜…è¯»åŸæ–‡çš„é“¾æ¥å¿…é¡»ä½¿ç”¨åŸå§‹æ¥æºURLï¼Œç¦æ­¢ä½¿ç”¨é‡å­ä½è‡ªèº«é“¾æ¥
5. Source å­—æ®µä¼˜å…ˆå¡«å†™åŸå§‹æ¥æºåç§°ï¼ˆå¦‚ OpenAI, arXiv ç­‰ï¼‰
6. è¯­è¨€é£æ ¼è¦ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ´å¯ŸåŠ›
7. è¾“å‡ºå¿…é¡»æ˜¯ Markdown æ ¼å¼
8. ä¸‰çº§æ ‡é¢˜åŸºäºå†…å®¹åˆ†æç”Ÿæˆï¼Œè¦æœ‰ä¸ªæ€§åŒ–å’Œæ´å¯ŸåŠ›

è¯·ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Šå†…å®¹ã€‚
"""
        
        # è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
        max_attempts = 3 if quality_check else 1
        
        for attempt in range(max_attempts):
            logger.info(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Š (å°è¯• {attempt + 1}/{max_attempts})...")
            
            report_content = self._call_llm(prompt, temperature=0.3)
            
            if not report_content:
                logger.error("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                continue
            
            # è´¨é‡æ£€æŸ¥
            if quality_check and attempt < max_attempts - 1:
                if self._check_report_quality(report_content, template_content):
                    logger.info("æŠ¥å‘Šè´¨é‡æ£€æŸ¥é€šè¿‡")
                    return report_content
                else:
                    logger.warning(f"æŠ¥å‘Šè´¨é‡æ£€æŸ¥æœªé€šè¿‡ï¼Œé‡æ–°ç”Ÿæˆ (å°è¯• {attempt + 2}/{max_attempts})")
                    # æ·»åŠ è´¨é‡åé¦ˆåˆ°æç¤ºè¯
                    prompt += "\n\n**è´¨é‡é—®é¢˜**: ä¸Šä¸€æ¬¡ç”Ÿæˆçš„æŠ¥å‘Šæ ¼å¼æˆ–å†…å®¹ä¸ç¬¦åˆè¦æ±‚ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç”Ÿæˆã€‚"
            else:
                return report_content
        
        return report_content
    
    def _check_report_quality(self, report: str, template: str) -> bool:
        """
        æ£€æŸ¥æŠ¥å‘Šè´¨é‡
        
        Args:
            report: ç”Ÿæˆçš„æŠ¥å‘Š
            template: æ¨¡æ¿å†…å®¹
            
        Returns:
            æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥
        """
        # ç®€å•æ£€æŸ¥ï¼šæ˜¯å¦åŒ…å«å…³é”®ç« èŠ‚
        required_sections = ["AIå‰æ²¿åŠ¨æ€é€ŸæŠ¥", "æœ¬å‘¨ç„¦ç‚¹", "æ·±åº¦è§£è¯»"]
        
        for section in required_sections:
            if section not in report:
                logger.warning(f"æŠ¥å‘Šç¼ºå°‘å¿…éœ€ç« èŠ‚: {section}")
                return False
        
        # æ£€æŸ¥æŠ¥å‘Šé•¿åº¦
        if len(report) < 500:
            logger.warning("æŠ¥å‘Šå†…å®¹è¿‡çŸ­")
            return False
        
        return True
    
    async def run(self, days: int = 3, save_intermediate: bool = True) -> Optional[str]:
        """
        è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹
        
        Args:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            æœ€ç»ˆæŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("GeminiAIReportAgent å¼€å§‹è¿è¡Œ")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        # 1. è·å–æ•°æ®
        news_items = await self.fetch_articles_from_db(days=days)
        if not news_items:
            logger.error("æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return None
        
        # 2. è¿‡æ»¤
        news_items = await self.step1_filter(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "01_filtered")
        
        # 3. å½’ç±»
        news_items = await self.step2_cluster(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "02_clustered")
        
        # 4. å»é‡
        news_items = await self.step3_deduplicate(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "03_deduplicated")
        
        # 5. æ’åº
        news_items = await self.step4_rank(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "04_ranked")
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report_content = await self.generate_final_report(news_items, quality_check=True)
        
        if report_content:
            # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
            output_dir = Path("final_reports")
            output_dir.mkdir(exist_ok=True)
            output_file = output_dir / f"AI_Report_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.md"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info("=" * 60)
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼è€—æ—¶: {elapsed:.2f}ç§’")
            logger.info(f"æŠ¥å‘Šä¿å­˜è‡³: {output_file}")
            logger.info("=" * 60)
            
            return report_content
        else:
            logger.error("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            return None
    
    def _save_intermediate_results(self, news_items: List[NewsItem], stage: str):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        output_dir = Path("final_reports") / "intermediate"
        output_dir.mkdir(exist_ok=True, parents=True)
        
        output_file = output_dir / f"{stage}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.json"
        
        data = [item.to_dict() for item in news_items]
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    async def test_agent():
        agent = GeminiAIReportAgent(max_retries=2)
        await agent.run(days=3, save_intermediate=True)
    
    asyncio.run(test_agent())

