"""
GeminiAIReportAgent - æ™ºèƒ½æ–°é—»å¤„ç†Agent
å®ç°ï¼šè¿‡æ»¤ -> å½’ç±» -> å»é‡ -> æ’åº -> æŠ¥å‘Šç”Ÿæˆ çš„å¤šè½®æµç¨‹
"""

import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
import re
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import time

from openai import OpenAI
import httpx
from sqlalchemy import select, desc, or_, delete

from database.models import QbitaiArticle, CompanyArticle
from database.db_session import get_session
import config
from crawler import utils

settings = config.settings
logger = utils.logger


class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    def __init__(self, article_id: str, title: str, description: str, 
                 content: str, url: str, source: str, publish_time: int,
                 reference_links: Optional[str] = None,
                 original_id: Optional[str] = None,
                 source_table: Optional[str] = None):
        self.article_id = article_id
        self.title = title
        self.description = description
        self.content = content  # ä¿å­˜å®Œæ•´å†…å®¹ï¼Œåœ¨å…·ä½“ä½¿ç”¨æ—¶å†æŒ‰éœ€æˆªå–
        self.url = url
        self.source = source  # æ¥æºï¼šqbitai, openai, googleç­‰
        self.publish_time = publish_time
        self.reference_links = reference_links
        
        # æ•°æ®åº“å›æº¯å­—æ®µ
        self.original_id = original_id
        self.source_table = source_table
        
        # å¤„ç†ç»“æœå­—æ®µ
        self.filter_decision = None  # "ä¿ç•™" or "å‰”é™¤"
        self.filter_reason = None
        self.event_id = None
        self.event_count = 0
        self.dedup_decision = None  # "ä¿ç•™" or "åˆ é™¤"
        self.dedup_reason = None
        self.tech_impact = 0
        self.industry_scope = 0
        self.hype_score = 0
        self.final_score = 0.0
        self.ranking_level = "C"
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "article_id": self.article_id,
            "title": self.title,
            "description": self.description,
            "content": self.content,
            "url": self.url,
            "source": self.source,
            "publish_time": self.publish_time,
            "reference_links": self.reference_links,
            "filter_decision": self.filter_decision,
            "filter_reason": self.filter_reason,
            "event_id": self.event_id,
            "event_count": self.event_count,
            "dedup_decision": self.dedup_decision,
            "dedup_reason": self.dedup_reason,
            "tech_impact": self.tech_impact,
            "industry_scope": self.industry_scope,
            "hype_score": self.hype_score,
            "final_score": self.final_score,
            "ranking_level": self.ranking_level
        }


class GeminiAIReportAgent:
    """åŸºäº Gemini çš„æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ Agent"""
    
    def __init__(self, max_retries: int = 5):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            max_retries: æ¯ä¸ªæ­¥éª¤çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.api_key = settings.REPORT_ENGINE_API_KEY
        self.base_url = settings.REPORT_ENGINE_BASE_URL
        self.model_name = settings.REPORT_ENGINE_MODEL_NAME or "gemini-3-pro-preview"
        self.max_retries = max_retries
        
        if not self.api_key:
            raise ValueError("API Key not configured in settings")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=httpx.Client(verify=False, timeout=120.0)
        )
        
        self.template_path = Path(__file__).parent / "templates" / "AIReport_example.md"
        
    async def fetch_articles_from_db(self, days: int = 3, limit: int = 200) -> List[NewsItem]:
        """
        ä»æ•°æ®åº“è·å–æ–°é—»æ•°æ®
        
        Args:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            limit: æ¯ä¸ªè¡¨çš„æœ€å¤§æ¡æ•°
            
        Returns:
            æ–°é—»æ¡ç›®åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨ä»æ•°æ®åº“è·å–æœ€è¿‘ {days} å¤©çš„æ–°é—»æ•°æ®...")
        
        cutoff_date = (datetime.now() - timedelta(days=days)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        cutoff_ts = int(cutoff_date.timestamp())
        
        news_items = []
        
        async with get_session() as session:
            # è·å–é‡å­ä½æ–‡ç« 
            stmt = (
                select(QbitaiArticle)
                .where(QbitaiArticle.publish_time >= cutoff_ts)
                .order_by(desc(QbitaiArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            qbitai_articles = result.scalars().all()
            
            for art in qbitai_articles:
                news_items.append(NewsItem(
                    article_id=f"qbitai_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source="é‡å­ä½",
                    publish_time=art.publish_time,
                    reference_links=art.reference_links,
                    original_id=art.article_id,
                    source_table="qbitai_article"
                ))
            
            # è·å–å…¬å¸å®˜æ–¹æ–‡ç« 
            stmt = (
                select(CompanyArticle)
                .where(CompanyArticle.publish_time >= cutoff_ts)
                .order_by(desc(CompanyArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            company_articles = result.scalars().all()
            
            for art in company_articles:
                news_items.append(NewsItem(
                    article_id=f"{art.company}_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source=art.company.upper(),
                    publish_time=art.publish_time,
                    reference_links=art.reference_links,
                    original_id=art.article_id,
                    source_table="company_article"
                ))
        
        logger.info(f"å…±è·å– {len(news_items)} æ¡æ–°é—»æ•°æ®")
        return news_items
    
    async def _delete_articles_from_db(self, items_to_delete: List[NewsItem]):
        """
        ä»æ•°æ®åº“ä¸­åˆ é™¤æŒ‡å®šçš„æ–‡ç« 
        
        Args:
            items_to_delete: éœ€è¦åˆ é™¤çš„æ–°é—»æ¡ç›®åˆ—è¡¨
        """
        if not items_to_delete:
            return

        logger.info(f"æ­£åœ¨ä»æ•°æ®åº“åˆ é™¤ {len(items_to_delete)} æ¡æ— æ•ˆ/é‡å¤æ•°æ®...")
        
        # æŒ‰è¡¨åˆ†ç»„
        qbitai_ids = []
        company_ids = []
        
        for item in items_to_delete:
            if not item.original_id or not item.source_table:
                logger.warning(f"æ— æ³•åˆ é™¤æ–‡ç«  {item.article_id}: ç¼ºå°‘åŸå§‹IDæˆ–è¡¨ä¿¡æ¯")
                continue
                
            if item.source_table == "qbitai_article":
                qbitai_ids.append(item.original_id)
            elif item.source_table == "company_article":
                company_ids.append(item.original_id)
        
        async with get_session() as session:
            try:
                if qbitai_ids:
                    stmt = delete(QbitaiArticle).where(QbitaiArticle.article_id.in_(qbitai_ids))
                    result = await session.execute(stmt)
                    logger.info(f"å·²åˆ é™¤ {result.rowcount} æ¡ Qbitai æ•°æ®")
                
                if company_ids:
                    stmt = delete(CompanyArticle).where(CompanyArticle.article_id.in_(company_ids))
                    result = await session.execute(stmt)
                    logger.info(f"å·²åˆ é™¤ {result.rowcount} æ¡ Company æ•°æ®")
                
                await session.commit()
            except Exception as e:
                logger.error(f"åˆ é™¤æ•°æ®åº“æ•°æ®å¤±è´¥: {e}")
                await session.rollback()

    def _call_llm(self, prompt: str, temperature: float = 0.1) -> Optional[str]:
        """
        è°ƒç”¨ LLM API
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            LLM å“åº”å†…å®¹
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"LLM API è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _parse_json_response(self, response: str) -> Optional[List[Dict]]:
        """
        è§£æ JSON å“åº”ï¼Œæ”¯æŒæå– markdown ä»£ç å—ä¸­çš„ JSON
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„ JSON åˆ—è¡¨
        """
        if not response:
            return None
        
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå– markdown ä»£ç å—ä¸­çš„ JSON
            import re
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except:
                    pass
            
            logger.error(f"æ— æ³•è§£æ JSON å“åº”: {response[:200]}")
            return None
    
    async def step1_filter(self, news_items: List[NewsItem], batch_size: int = 20) -> List[NewsItem]:
        """
        ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤ (Filtering)
        å‰”é™¤ä¸ AI æ ¸å¿ƒæŠ€æœ¯è¿›å±•æ— å…³çš„å™ªéŸ³ä¿¡æ¯
        
        Args:
            news_items: åŸå§‹æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬ä¸€æ­¥ã€‘å¼€å§‹è¿‡æ»¤ (Filtering)...")

        items_to_delete = []

        # 1. é¢„è¿‡æ»¤ï¼šå‰”é™¤å†…å®¹è¿‡çŸ­æˆ–æ— å†…å®¹çš„æ–°é—»
        valid_news_items = []
        for item in news_items:
            # ç®€å•çš„è§„åˆ™è¿‡æ»¤ï¼šå†…å®¹é•¿åº¦å°‘äº100å­—ç¬¦è§†ä¸ºæ— æ•ˆå†…å®¹
            # æ³¨æ„ï¼šNewsItem åˆå§‹åŒ–æ—¶å·²æˆªå–å‰1000å­—ç¬¦ï¼Œè¿™é‡Œåˆ¤æ–­çš„æ˜¯æˆªå–åçš„é•¿åº¦
            # ä½†å¦‚æœåŸå†…å®¹æœ¬èº«å°±å¾ˆå°‘ï¼Œè¿™é‡Œä¹Ÿèƒ½æ£€æµ‹å‡ºæ¥
            if item.content and len(item.content.strip()) >= 50:
                valid_news_items.append(item)
            else:
                logger.info(f"é¢„è¿‡æ»¤å‰”é™¤ï¼ˆå†…å®¹è¿‡å°‘ï¼‰: {item.title} (ID: {item.article_id})")
                items_to_delete.append(item)
        
        # åˆ é™¤é¢„è¿‡æ»¤æ‰çš„æ•°æ®
        if items_to_delete:
            await self._delete_articles_from_db(items_to_delete)
            items_to_delete = [] # æ¸…ç©ºåˆ—è¡¨ä»¥ä¾¿å¤ç”¨

        news_items = valid_news_items
        logger.info(f"å¾…å¤„ç†æ–°é—»æ•°: {len(news_items)}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        filtered_items = []
        rejected_items = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "content_snippet": item.content[:300],
                    "source": item.source,
                    "url": item.url
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯å†…å®¹ç­›é€‰ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œè¿‡æ»¤åˆ¤æ–­ã€‚

**è¿‡æ»¤è§„åˆ™ï¼š**

ã€ä¿ç•™æ¡ä»¶ã€‘ï¼ˆé€»è¾‘æˆ–ï¼Œæ»¡è¶³å…¶ä¸€å³ä¿ç•™ï¼‰:
1. æŠ€æœ¯/èƒ½åŠ›è¿›å±•: æ ¸å¿ƒå†…å®¹æ˜¯å…³äº AI æŠ€æœ¯ã€æ¨¡å‹ã€ç³»ç»Ÿã€å·¥ç¨‹æˆ–åº”ç”¨èƒ½åŠ›çš„å…·ä½“è¿›å±•ã€‚
2. å…³é”®é¢†åŸŸ: æ˜ç¡®æ¶‰åŠåŸºç¡€æ¨¡å‹ã€è®­ç»ƒ/æ¨ç†æ–¹æ³•ã€æ•°æ®å·¥ç¨‹ã€AI Infraã€Agent æ¡†æ¶æˆ–ç›¸å…³æŠ€æœ¯äº§å“ã€‚
3. æƒå¨æ¥æº: ä¿¡æ¯æ¥æºä¸ºå­¦æœ¯è®ºæ–‡ (å¦‚ arXiv)ã€å®˜æ–¹æŠ€æœ¯åšå®¢ (å¦‚ OpenAI Blog)ã€å®˜æ–¹äº§å“å‘å¸ƒé¡µæˆ– GitHub Release Notesã€‚

ã€å‰”é™¤æ¡ä»¶ã€‘ï¼ˆé€»è¾‘æˆ–ï¼Œæ»¡è¶³å…¶ä¸€å³å‰”é™¤ï¼‰:
1. å•†ä¸š/é‡‘è: è‚¡ä»·ã€å¸‚å€¼ã€èèµ„ã€IPOã€ä¼°å€¼ã€è´¢æŠ¥ã€æ”¶å…¥ã€ç”¨æˆ·è§„æ¨¡ã€æ”¶è´­ã€å¹¶è´­ã€‚
2. å¸‚åœºåˆ†æ: æŠ•èµ„è§‚ç‚¹ã€å¸‚åœºæƒ…ç»ªã€èµ„æœ¬åŠ¨å‘ã€æ— ç›´æ¥æŠ€æœ¯å…³è”çš„å•†ä¸šåˆä½œæ–°é—»ã€‚
3. äºŒæ¬¡è§£è¯»: ä¸ªäººè§‚ç‚¹ã€KOL é•¿ç¯‡åˆ†æã€æ— å¼•ç”¨çš„æ¨æ–‡æ€»ç»“ã€‚
4. ä¿¡æºä¸æ˜: æœªæ ‡æ³¨æ˜ç¡®æ¥æºã€æ¥æºä¸ºåŒ¿åè®ºå›æˆ–ç¤¾äº¤ç¾¤èŠæˆªå›¾ã€‚

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- filter_decision: "ä¿ç•™" æˆ– "å‰”é™¤"
- filter_reason: åˆ¤æ–­ç†ç”±ï¼ˆä¸€å¥è¯ç®€è¿°ï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "filter_decision": "ä¿ç•™", "filter_reason": "æ¶‰åŠå¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯çªç ´"}},
  {{"article_id": "yyy", "filter_decision": "å‰”é™¤", "filter_reason": "ä¸»è¦è®¨è®ºèèµ„å’Œå¸‚å€¼"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    # æ›´æ–°æ–°é—»æ¡ç›®
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.filter_decision = r.get("filter_decision")
                            item.filter_reason = r.get("filter_reason")
                            
                            if item.filter_decision == "ä¿ç•™":
                                filtered_items.append(item)
                            else:
                                rejected_items.append(item)
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è§£æå¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥ï¼Œè·³è¿‡")
            
            # é¿å… API é™æµ
            await asyncio.sleep(1)
        
        # åˆ é™¤è¢«å‰”é™¤çš„æ•°æ®
        if rejected_items:
            logger.info(f"æ­£åœ¨åˆ é™¤ {len(rejected_items)} æ¡è¢«è¿‡æ»¤çš„æ–°é—»...")
            await self._delete_articles_from_db(rejected_items)
            
        logger.info(f"è¿‡æ»¤å®Œæˆï¼šä¿ç•™ {len(filtered_items)}/{len(news_items)} æ¡æ–°é—»")
        return filtered_items
    
    async def step2_cluster(self, news_items: List[NewsItem], batch_size: int = 30) -> List[NewsItem]:
        """
        ç¬¬äºŒæ­¥ï¼šå½’ç±» (Clustering)
        å°†æè¿°åŒä¸€äº‹ä»¶çš„æ–°é—»èšåˆåœ¨ä¸€èµ·
        
        Args:
            news_items: è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å½’ç±»åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äºŒæ­¥ã€‘å¼€å§‹å½’ç±» (Clustering)...")
        logger.info(f"å¾…å¤„ç†æ–°é—»æ•°: {len(news_items)}")
        
        # åˆ†æ‰¹å¤„ç†
        all_events = {}  # event_id -> [NewsItem]
        
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source
                })
            
            # åŒ…å«ä¹‹å‰å·²è¯†åˆ«çš„äº‹ä»¶ä¿¡æ¯
            existing_events_info = ""
            if all_events:
                existing_events_info = "\nå·²è¯†åˆ«çš„äº‹ä»¶IDåˆ—è¡¨ï¼ˆä¾›å‚è€ƒï¼‰:\n"
                for event_id, items in all_events.items():
                    existing_events_info += f"- {event_id}: {items[0].title[:50]}...\n"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»äº‹ä»¶èšç±»ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œè¯­ä¹‰å½’ç±»ã€‚

**å½’ç±»æ ‡å‡†ï¼š**
- æŒ‰"åŒä¸€æŠ€æœ¯äº‹ä»¶ / æ¨¡å‹ç‰ˆæœ¬ / äº§å“å‘å¸ƒ / å…³é”®è®ºæ–‡"è¿›è¡Œè¯­ä¹‰å½’ç±»ã€‚
- ä¾‹å¦‚ï¼š"GPT-5 å‘å¸ƒ"ã€"Llama 3.1 å¼€æº"ã€"DeepMind æå‡ºæ–° AlphaFold ç®—æ³•"ç­‰å‡å±äºç‹¬ç«‹çš„è¯­ä¹‰äº‹ä»¶ã€‚
- å¦‚æœå¤šæ¡æ–°é—»è®¨è®ºçš„æ˜¯åŒä¸€ä¸ªäº‹ä»¶ï¼ˆå¦‚åŒä¸€ä¸ªæ¨¡å‹å‘å¸ƒã€åŒä¸€ç¯‡è®ºæ–‡ã€åŒä¸€ä¸ªæŠ€æœ¯çªç ´ï¼‰ï¼Œå®ƒä»¬åº”è¯¥è¢«å½’ä¸ºåŒä¸€ä¸ª event_idã€‚
- event_id åº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„è‹±æ–‡çŸ­è¯­ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Œä¾‹å¦‚ï¼šgpt5_release_2025_q4, llama3_1_opensource

{existing_events_info}

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- event_id: äº‹ä»¶IDï¼ˆä½¿ç”¨æœ‰æ„ä¹‰çš„è‹±æ–‡çŸ­è¯­ï¼Œå¦‚æœä¸å·²è¯†åˆ«çš„äº‹ä»¶ç›¸åŒï¼Œè¯·ä½¿ç”¨ç›¸åŒçš„event_idï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "event_id": "gpt5_release"}},
  {{"article_id": "yyy", "event_id": "llama3_1_opensource"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    # æ›´æ–°æ–°é—»æ¡ç›®
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            event_id = result_map[item.article_id].get("event_id")
                            item.event_id = event_id
                            
                            if event_id not in all_events:
                                all_events[event_id] = []
                            all_events[event_id].append(item)
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è§£æå¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥")
            
            await asyncio.sleep(1)
        
        # æ›´æ–°æ¯æ¡æ–°é—»çš„ event_count
        for event_id, items in all_events.items():
            count = len(items)
            for item in items:
                item.event_count = count
        
        logger.info(f"å½’ç±»å®Œæˆï¼šè¯†åˆ«å‡º {len(all_events)} ä¸ªç‹¬ç«‹äº‹ä»¶")
        for event_id, items in sorted(all_events.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            logger.info(f"  - {event_id}: {len(items)} æ¡æ–°é—»")
        
        return news_items
    
    async def step3_deduplicate(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """
        ç¬¬ä¸‰æ­¥ï¼šå»é‡ (Deduplication)
        åœ¨æ¯ä¸ªäº‹ä»¶ä¸­ï¼Œä»…ä¿ç•™ä¸€æ¡æœ€æƒå¨ã€ä¿¡æ¯è´¨é‡æœ€é«˜çš„æ–°é—»
        
        Args:
            news_items: å½’ç±»åçš„æ–°é—»åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬ä¸‰æ­¥ã€‘å¼€å§‹å»é‡ (Deduplication)...")
        
        # æŒ‰ event_id åˆ†ç»„
        events = defaultdict(list)
        for item in news_items:
            events[item.event_id].append(item)
        
        logger.info(f"å¾…å¤„ç†äº‹ä»¶æ•°: {len(events)}")
        
        deduplicated_items = []
        deleted_items = []
        
        for event_id, items in events.items():
            if len(items) == 1:
                # åªæœ‰ä¸€æ¡æ–°é—»ï¼Œç›´æ¥ä¿ç•™
                items[0].dedup_decision = "ä¿ç•™"
                items[0].dedup_reason = "å”¯ä¸€æ¥æº"
                deduplicated_items.append(items[0])
                continue
            
            logger.info(f"å¤„ç†äº‹ä»¶: {event_id} ({len(items)} æ¡æ–°é—»)")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in items:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source,
                    "url": item.url,
                    "publish_time": datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»å»é‡ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯æè¿°åŒä¸€äº‹ä»¶çš„å¤šæ¡æ–°é—»ï¼Œè¯·é€‰å‡ºæœ€æƒå¨ã€ä¿¡æ¯è´¨é‡æœ€é«˜çš„**æœ€å¤šä¸‰æ¡**ã€‚

**ä¿ç•™ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š**
1. å®˜æ–¹æ ¸å¿ƒä¿¡æº: å®˜ç½‘å‘å¸ƒã€å®˜æ–¹åšå®¢ã€arXiv è®ºæ–‡ã€GitHub Release
2. æ ¸å¿ƒäººå‘˜è§£è¯»: ä½œè€…ã€æ ¸å¿ƒå·¥ç¨‹å¸ˆæˆ–å®˜æ–¹ç ”ç©¶å‘˜çš„æ·±åº¦è§£è¯»
3. æƒå¨æŠ€æœ¯åª’ä½“: å¯¹ä¸Šè¿°ä¿¡æºçš„æ·±åº¦ã€å¿«é€Ÿè½¬è¿°æŠ¥é“
4. ç¤¾äº¤åª’ä½“/æ™®é€šè½¬è¿°: ä¼˜å…ˆçº§æœ€ä½

**äº‹ä»¶ID:** {event_id}

**æ–°é—»åˆ—è¡¨ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·é€‰æ‹©æœ€å¤šä¸‰æ¡æœ€æƒå¨çš„æ–°é—»ä¿ç•™ï¼ˆå¦‚æœåªæœ‰1-3æ¡ï¼Œå¯å…¨éƒ¨ä¿ç•™ï¼›å¦‚æœè¶…è¿‡3æ¡ï¼Œè¯·ç­›é€‰å‡ºæœ€å¥½çš„3æ¡ï¼‰ï¼Œå…¶ä½™æ ‡è®°ä¸ºåˆ é™¤ã€‚ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼š
- article_id: æ–‡ç« ID
- dedup_decision: "ä¿ç•™" æˆ– "åˆ é™¤"
- dedup_reason: åˆ¤æ–­ç†ç”±ï¼ˆä¸€å¥è¯ï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "dedup_decision": "ä¿ç•™", "dedup_reason": "å®˜æ–¹åšå®¢é¦–å‘"}},
  {{"article_id": "yyy", "dedup_decision": "åˆ é™¤", "dedup_reason": "äºŒæ¬¡è½¬è¿°"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    result_map = {r["article_id"]: r for r in results}
                    for item in items:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.dedup_decision = r.get("dedup_decision")
                            item.dedup_reason = r.get("dedup_reason")
                            
                            if item.dedup_decision == "ä¿ç•™":
                                deduplicated_items.append(item)
                            else:
                                deleted_items.append(item)
                    break
                else:
                    logger.warning(f"äº‹ä»¶ {event_id} å»é‡å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        # å¤±è´¥æ—¶ä¿ç•™ç¬¬ä¸€æ¡
                        logger.error(f"äº‹ä»¶ {event_id} å»é‡å¤±è´¥ï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€æ¡")
                        items[0].dedup_decision = "ä¿ç•™"
                        items[0].dedup_reason = "å»é‡å¤±è´¥ï¼Œé»˜è®¤ä¿ç•™"
                        deduplicated_items.append(items[0])
                        # å…¶ä½™çš„æš‚æ—¶ä¸åŠ¨ï¼Œé¿å…è¯¯åˆ 
            
            await asyncio.sleep(0.5)
        
        # åˆ é™¤è¢«å»é‡çš„æ•°æ®
        if deleted_items:
            logger.info(f"æ­£åœ¨åˆ é™¤ {len(deleted_items)} æ¡é‡å¤æ–°é—»...")
            await self._delete_articles_from_db(deleted_items)
            
        logger.info(f"å»é‡å®Œæˆï¼šä¿ç•™ {len(deduplicated_items)}/{len(news_items)} æ¡æ–°é—»")
        return deduplicated_items
    
    async def step4_rank(self, news_items: List[NewsItem], batch_size: int = 20) -> List[NewsItem]:
        """
        ç¬¬å››æ­¥ï¼šæ’åº (Ranking)
        å¯¹æœ€ç»ˆä¿ç•™çš„æ–°é—»æ¡ç›®è¿›è¡Œä»·å€¼åˆ¤æ–­
        
        Args:
            news_items: å»é‡åçš„æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ’åºåçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬å››æ­¥ã€‘å¼€å§‹æ’åº (Ranking)...")
        logger.info(f"å¾…è¯„åˆ†æ–°é—»æ•°: {len(news_items)}")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source,
                    "event_count": item.event_count
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯å½±å“åŠ›è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œä»·å€¼è¯„åˆ†ã€‚

**è¯„åˆ†ç»´åº¦ï¼š**

1. **æŠ€æœ¯å½±å“åŠ› (tech_impact)** [1-5åˆ†]:
   - 5åˆ† (èŒƒå¼è½¬æ¢): æå‡ºå…¨æ–°æ¶æ„æˆ–ç†è®ºï¼Œå¯èƒ½æ”¹å˜ä¸€ä¸ªé¢†åŸŸçš„èµ°å‘ (å¦‚ Transformer)
   - 4åˆ† (é‡å¤§çªç ´): åœ¨å…³é”®èƒ½åŠ›ä¸Šæœ‰å·¨å¤§æå‡æˆ–å¼€æºäº†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹
   - 3åˆ† (æ˜¾è‘—æ”¹è¿›): ç°æœ‰æ–¹æ³•ä¸Šçš„é‡è¦æ”¹è¿›ï¼Œæˆ–å‘å¸ƒäº†éå¸¸æœ‰ç”¨çš„å·¥å…·/æ¡†æ¶
   - 2åˆ† (å¸¸è§„ä¼˜åŒ–): æ€§èƒ½çš„å°å¹…æå‡æˆ–å¸¸è§„ç‰ˆæœ¬è¿­ä»£
   - 1åˆ† (å¾®å°æ”¹è¿›): å¢é‡å¼æ›´æ–°

2. **è¡Œä¸šå½±å“èŒƒå›´ (industry_scope)** [1-5åˆ†]:
   - 5åˆ† (å…¨è¡Œä¸š): å¯¹å‡ ä¹æ‰€æœ‰ AI åº”ç”¨å¼€å‘è€…å’Œå…¬å¸éƒ½äº§ç”Ÿå½±å“
   - 4åˆ† (å¤šé¢†åŸŸ): å½±å“å¤šä¸ªä¸»è¦ AI åº”ç”¨é¢†åŸŸ (å¦‚ NLP, CV)
   - 3åˆ† (ç‰¹å®šé¢†åŸŸ): æ·±åº¦å½±å“ä¸€ä¸ªå‚ç›´é¢†åŸŸ (å¦‚ AI for Science)
   - 2åˆ† (ç‰¹å®šä»»åŠ¡): ä¸»è¦å½±å“ä¸€ä¸ªæˆ–å°‘æ•°å‡ ä¸ªå…·ä½“ä»»åŠ¡
   - 1åˆ† (å°ä¼—åœºæ™¯): å½±å“èŒƒå›´éå¸¸æœ‰é™

3. **çƒ­åº¦ (hype_score)** [1-5åˆ†]:
   - æ ¹æ® event_count æ˜ å°„ï¼š
     * 1-2ç¯‡ â†’ 1åˆ†
     * 3-5ç¯‡ â†’ 2åˆ†
     * 6-10ç¯‡ â†’ 3åˆ†
     * 11-20ç¯‡ â†’ 4åˆ†
     * >20ç¯‡ â†’ 5åˆ†

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- tech_impact: æŠ€æœ¯å½±å“åŠ›è¯„åˆ† (1-5)
- industry_scope: è¡Œä¸šå½±å“èŒƒå›´è¯„åˆ† (1-5)
- hype_score: çƒ­åº¦è¯„åˆ† (1-5ï¼Œæ ¹æ®event_countè®¡ç®—)

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "tech_impact": 5, "industry_scope": 5, "hype_score": 4}},
  {{"article_id": "yyy", "tech_impact": 3, "industry_scope": 3, "hype_score": 2}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.tech_impact = r.get("tech_impact", 2)
                            item.industry_scope = r.get("industry_scope", 2)
                            item.hype_score = r.get("hype_score", 1)
                            
                            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
                            item.final_score = (
                                item.tech_impact * 0.5 +
                                item.industry_scope * 0.3 +
                                item.hype_score * 0.2
                            )
                            
                            # è¯„çº§æ˜ å°„
                            if item.final_score >= 4.2:
                                item.ranking_level = "S"
                            elif item.final_score >= 3.5:
                                item.ranking_level = "A"
                            elif item.final_score >= 2.8:
                                item.ranking_level = "B"
                            else:
                                item.ranking_level = "C"
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è¯„åˆ†å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} è¯„åˆ†å¤±è´¥")
            
            await asyncio.sleep(1)
        
        # æŒ‰è¯„åˆ†æ’åº
        news_items.sort(key=lambda x: x.final_score, reverse=True)
        
        logger.info(f"æ’åºå®Œæˆï¼š")
        logger.info(f"  Sçº§: {sum(1 for x in news_items if x.ranking_level == 'S')} æ¡")
        logger.info(f"  Açº§: {sum(1 for x in news_items if x.ranking_level == 'A')} æ¡")
        logger.info(f"  Bçº§: {sum(1 for x in news_items if x.ranking_level == 'B')} æ¡")
        logger.info(f"  Cçº§: {sum(1 for x in news_items if x.ranking_level == 'C')} æ¡")
        
        return news_items

    def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict[str, str]]:
        """
        æœç´¢ arXiv è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸² (ä¾‹å¦‚ "all:LLM")
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨ [{'title': ..., 'url': ..., 'summary': ...}]
        """
        base_url = "http://export.arxiv.org/api/query"
        
        # ç®€å•æ¸…ç† query
        query = query.replace('"', '%22')
        
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = {
            "search_query": query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "submittedDate",
            "sortOrder": "descending"
        }
        
        # æ‰‹åŠ¨æ‹¼æ¥ URL ä»¥ç¡®ä¿ encoded æ­£ç¡®ï¼Œæˆ–è€…ä½¿ç”¨ urllib.parse.quote ä½†ä¿ç•™ API ç‰¹æ®Šå­—ç¬¦
        # è¿™é‡Œä½¿ç”¨ urllib.parse.urlencode åº”è¯¥è¶³å¤Ÿå®‰å…¨
        try:
            query_string = urllib.parse.urlencode(params, safe=':')
            url = f"{base_url}?{query_string}"
            
            logger.info(f"æ­£åœ¨è°ƒç”¨ arXiv API: {url}")
            
            # éµå®ˆ API è§„åˆ™ï¼Œå¢åŠ å»¶æ—¶
            time.sleep(3)
            
            # è®¾ç½® User-Agent
            req = urllib.request.Request(
                url, 
                headers={'User-Agent': 'AIReportAgent/1.0 (mailto:your_email@example.com)'}
            )
            
            with urllib.request.urlopen(req, timeout=30) as response:
                xml_data = response.read()
                
            root = ET.fromstring(xml_data)
            # å¤„ç† namespace
            # Atom feed é€šå¸¸æœ‰é»˜è®¤ namespaceï¼ŒElementTree è§£ææ—¶éœ€è¦åœ¨ tag å‰åŠ  {uri}
            # è·å– root çš„ namespace
            ns_match = root.tag.split('}')[0] + '}' if '}' in root.tag else ''
            
            papers = []
            entries = root.findall(f'{ns_match}entry')
            
            for entry in entries:
                try:
                    title_elem = entry.find(f'{ns_match}title')
                    id_elem = entry.find(f'{ns_match}id')
                    summary_elem = entry.find(f'{ns_match}summary')
                    published_elem = entry.find(f'{ns_match}published')
                    
                    title = title_elem.text.strip().replace('\n', ' ') if title_elem is not None else "No Title"
                    link = id_elem.text.strip() if id_elem is not None else ""
                    summary = summary_elem.text.strip().replace('\n', ' ') if summary_elem is not None else ""
                    published = published_elem.text.strip() if published_elem is not None else ""
                    
                    # è½¬æ¢ article ID link åˆ° abstract page link
                    # arXiv API id é€šå¸¸æ˜¯ http://arxiv.org/abs/xxxx.xxxx
                    # æœ‰æ—¶æ˜¯ http://arxiv.org/api/xxxx
                    if link:
                        link = link.replace("/api/", "/abs/")
                    
                    papers.append({
                        "title": title,
                        "url": link,
                        "summary": summary[:200] + "...",
                        "published": published,
                        "source": "arXiv",
                        "type": "Paper"
                    })
                except Exception as e:
                    logger.warning(f"è§£æ arXiv entry å¤±è´¥: {e}")
                    continue
                
            return papers
            
        except Exception as e:
            logger.error(f"arXiv æœç´¢å¤±è´¥: {e}")
            return []

    async def step5_fetch_arxiv_papers(self, news_items: List[NewsItem]) -> List[Dict[str, str]]:
        """
        ç¬¬äº”æ­¥ï¼šè·å–ç›¸å…³ arXiv è®ºæ–‡
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            
        Returns:
            ç›¸å…³è®ºæ–‡åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äº”æ­¥ã€‘è·å–ç›¸å…³ arXiv è®ºæ–‡...")
        
        # é€‰å–æ‰€æœ‰ S/A çº§æ–°é—»ï¼Œç¡®ä¿è¦†ç›–é¢
        target_items = [item for item in news_items if item.ranking_level in ["S", "A"]]
        
        # å¦‚æœ S/A çº§å¤ªå°‘ï¼Œè¡¥å…… B çº§å‰å‡ å
        if len(target_items) < 5:
            b_items = [item for item in news_items if item.ranking_level == "B"]
            target_items.extend(b_items[:5 - len(target_items)])
            
        if not target_items:
            target_items = news_items[:5]
            
        if not target_items:
            return []
            
        # 1. æå–æœç´¢å…³é”®è¯
        titles = "\n".join([f"- {item.title}" for item in target_items])
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ AI é¢†åŸŸçš„çƒ­ç‚¹æ–°é—»æ ‡é¢˜ï¼Œæ„å»ºç”¨äº arXiv æœç´¢çš„æŸ¥è¯¢å…³é”®è¯åˆ—è¡¨ã€‚

æ–°é—»æ ‡é¢˜ï¼š
{titles}

è¦æ±‚ï¼š
1. åˆ†ææ¯æ¡æ–°é—»çš„æ ¸å¿ƒæŠ€æœ¯å®ä½“ï¼ˆå¦‚æ¨¡å‹åç§° "Gemini 3", "Claude Sonnet" æˆ–æŠ€æœ¯æœ¯è¯­ "World Model", "Scaling Law"ï¼‰ã€‚
2. æ„å»º 5-8 ä¸ªç‹¬ç«‹çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œæ—¨åœ¨å°½å¯èƒ½è¦†ç›–è¿™äº›æ–°é—»å¯¹åº”çš„ä¸»é¢˜ã€‚
3. æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ `all:` æˆ– `ti:` å‰ç¼€ã€‚
4. ç¤ºä¾‹ï¼š
all:"Gemini 3"
ti:"World Model" AND all:Genie
all:"Large Language Model" AND all:Reasoning

è¯·ç›´æ¥è¿”å›æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚
"""
        response = self._call_llm(prompt)
        if not response:
            queries = ["all:Artificial Intelligence"]
        else:
            queries = [line.strip().strip('`').strip('"') for line in response.split('\n') if line.strip() and not line.strip().startswith('```')]
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
        queries = queries[:8]
        logger.info(f"ç”Ÿæˆçš„ arXiv æŸ¥è¯¢: {queries}")
        
        all_papers = []
        seen_ids = set()
        
        # 2. æ‰§è¡Œæœç´¢ (ä¸²è¡Œæ‰§è¡Œä»¥éµå®ˆé™æµ)
        for query in queries:
            if not query:
                continue
            # æ¸…ç† query
            if "search_query=" in query:
                query = query.replace("search_query=", "")
                
            # æ¯ä¸ª query å– 5 æ¡ï¼Œæ€»å…±å¯èƒ½è·å– 25-40 æ¡
            papers = self.search_arxiv(query, max_results=5) 
            
            for paper in papers:
                # ä½¿ç”¨ URL ä½œä¸ºå»é‡é”®
                if paper['url'] not in seen_ids:
                    all_papers.append(paper)
                    seen_ids.add(paper['url'])
            
        logger.info(f"å…±è·å–åˆ° {len(all_papers)} ç¯‡ä¸é‡å¤çš„ arXiv è®ºæ–‡")
        return all_papers
    
    def _validate_news_item_format(self, content: str) -> Tuple[bool, str]:
        """éªŒè¯æ–°é—»æ¡ç›®çš„ Markdown æ ¼å¼"""
        required_patterns = [
            (r"### \*\*.*?\*\*", "æ ‡é¢˜æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º ### **æ ‡é¢˜**"),
            (r"\[é˜…è¯»åŸæ–‡\]\(.*?\)", "ç¼ºå°‘é˜…è¯»åŸæ–‡é“¾æ¥æˆ–æ ¼å¼é”™è¯¯"),
            (r"> \*\*æ¦‚è¦\*\*:.*", "ç¼ºå°‘æ¦‚è¦æˆ–æ ¼å¼é”™è¯¯"),
            (r"\*\*ğŸ’¡å†…å®¹è¯¦è§£\*\*", "ç¼ºå°‘'ğŸ’¡å†…å®¹è¯¦è§£'åˆ†èŠ‚"),
            (r"- \*\*.*?\*\*", "ç¼ºå°‘è¦ç‚¹æ ‡é¢˜æˆ–æ ¼å¼é”™è¯¯")
        ]
        
        import re
        for pattern, error_msg in required_patterns:
            if not re.search(pattern, content, re.MULTILINE):
                return False, error_msg
        return True, ""

    async def _generate_news_entries_batch(self, batch_items: List[NewsItem], candidate_papers: List[Dict] = None) -> List[Dict[str, str]]:
        """
        åˆ†æ‰¹ç”Ÿæˆæ–°é—»æ¡ç›®å†…å®¹ (å¹¶å‘å¤„ç†)
        
        Args:
            batch_items: è¿™ä¸€æ‰¹çš„æ–°é—»åˆ—è¡¨
            candidate_papers: å€™é€‰ arXiv è®ºæ–‡åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ¡ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {"article_id", "category", "markdown_content"}
        """
        batch_data = []
        for item in batch_items:
            pub_date = datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
            batch_data.append({
                "article_id": item.article_id,
                "title": item.title,
                "source": item.source,
                "url": item.url,
                "publish_time": pub_date,
                "content": item.content,  # ä½¿ç”¨å®Œæ•´å†…å®¹è¿›è¡Œæ·±åº¦é˜…è¯»
            })

        # æ„å»ºå€™é€‰è®ºæ–‡ä¸Šä¸‹æ–‡
        papers_context = ""
        if candidate_papers:
            papers_list = []
            for p in candidate_papers:
                papers_list.append(f"- Title: {p.get('title')}\n  URL: {p.get('url')}")
            papers_context = "\n".join(papers_list)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯åˆ†æå¸ˆã€‚è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆç¬¦åˆæŠ¥å‘Šæ ¼å¼çš„Markdownå†…å®¹å—ã€‚

**å€™é€‰ arXiv è®ºæ–‡åº“ï¼š**
{papers_context if papers_context else "(æ— å€™é€‰è®ºæ–‡)"}

**è¾“å‡ºè¦æ±‚ï¼š**
å¯¹äºæ¯ä¸€æ¡æ–°é—»ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. **åˆ†ç±»**ï¼šå°†å…¶å½’å…¥ä»¥ä¸‹ä¸‰ç±»ä¹‹ä¸€ï¼š
   - "Infrastructure" (AIåŸºç¡€è®¾æ–½: èŠ¯ç‰‡, ç®—åŠ›, æ¡†æ¶, æ•°æ®å·¥ç¨‹ç­‰)
   - "Model" (AIæ¨¡å‹ä¸æŠ€æœ¯: åŸºç¡€æ¨¡å‹, ç®—æ³•åˆ›æ–°, è®­ç»ƒæŠ€æœ¯ç­‰)
   - "Application" (AIåº”ç”¨ä¸æ™ºèƒ½ä½“: å…·ä½“åº”ç”¨, Agent, è¡Œä¸šè½åœ°ç­‰)

2. **ç”ŸæˆMarkdownå†…å®¹**ï¼šä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownæ ¼å¼æ¨¡æ¿ç”Ÿæˆå†…å®¹ã€‚
   
   **æ¨¡æ¿æ ¼å¼ï¼š**
   ```markdown
   ### **[æ–°é—»æ ‡é¢˜]**
   
   [é˜…è¯»åŸæ–‡]([URL])  `[Publish_Time]`
   
   > **æ¦‚è¦**: [3-4å¥è¯ç®€ç»ƒæ¦‚æ‹¬æ ¸å¿ƒäº‹ä»¶]
   
   **ğŸ’¡å†…å®¹è¯¦è§£**
   (å†…å®¹è¯¦è§£æ˜¯å¯¹å…³é”®æŠ€æœ¯çš„ç½—åˆ—ï¼Œå…³é”®ç‚¹æ•°é‡è‡³å°‘å¤§äº3ç‚¹ï¼Œè¯·å¯¹å…³é”®æŠ€æœ¯è¿›è¡Œè¯¦ç»†è§£è¯»ï¼Œæ­¤å¤„ä¸ç”¨æ·»åŠ æ¦‚è¿°)

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 1**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 2**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 3**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰   
        â€¦â€¦
    â€¦â€¦

    [ç›¸å…³è®ºæ–‡]([URL])
   ```

   **å…³äº [ç›¸å…³è®ºæ–‡] çš„ç‰¹åˆ«è¯´æ˜ï¼š**
   - è¯·åœ¨â€œå€™é€‰ arXiv è®ºæ–‡åº“â€ä¸­æŸ¥æ‰¾ä¸å½“å‰æ–°é—»**é«˜åº¦ç›¸å…³**çš„è®ºæ–‡ï¼ˆæ ‡é¢˜æˆ–å†…å®¹åŒ¹é…ï¼‰ã€‚
   - å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡ï¼Œè¯·å°† `[ç›¸å…³è®ºæ–‡]([URL])` æ›¿æ¢ä¸ºå®é™…çš„è®ºæ–‡é“¾æ¥ï¼Œä¾‹å¦‚ `[ç›¸å…³è®ºæ–‡](https://arxiv.org/abs/2412.xxxxx)`ã€‚
   - å¦‚æœæœ‰å¤šç¯‡ç›¸å…³ï¼Œå¯ä»¥åˆ—å‡ºå¤šè¡Œï¼Œæ ¼å¼å‡ä¸º `[ç›¸å…³è®ºæ–‡](URL)` æˆ– `[ç›¸å…³è®ºæ–‡: Title](URL)`ã€‚
   - **å¦‚æœæ²¡æœ‰æ‰¾åˆ°é«˜åº¦ç›¸å…³çš„è®ºæ–‡ï¼Œè¯·åŠ¡å¿…åˆ é™¤ `[ç›¸å…³è®ºæ–‡]([URL])` è¿™ä¸€è¡Œï¼Œä¸è¦ä¿ç•™ç©ºè¡Œæˆ–å ä½ç¬¦ã€‚**

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¿”å›æ ¼å¼ï¼š**
è¯·è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«æ¯æ¡æ–°é—»çš„ç”Ÿæˆç»“æœï¼š
```json
[
  {{
    "article_id": "xxx",
    "category": "Infrastructure", 
    "markdown_content": "### **æ ‡é¢˜**..."
  }},
  ...
]
```
è¯·åªè¿”å› JSONã€‚
"""
        
        for retry in range(self.max_retries):
            response = self._call_llm(prompt, temperature=0.3)
            results = self._parse_json_response(response)
            if results:
                # éªŒè¯æ ¼å¼
                valid_results = []
                errors = []
                for item in results:
                    is_valid, error = self._validate_news_item_format(item.get("markdown_content", ""))
                    if is_valid:
                        valid_results.append(item)
                    else:
                        errors.append(f"æ–‡ç«  '{item.get('title', 'Unknown')}' æ ¼å¼é”™è¯¯: {error}")
                
                if not errors:
                    return valid_results
                
                # å¦‚æœæœ‰é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œå°†é”™è¯¯åŠ å…¥ prompt é‡è¯•
                logger.warning(f"æ‰¹æ¬¡ç”Ÿæˆå­˜åœ¨æ ¼å¼é”™è¯¯: {'; '.join(errors)}")
                if retry < self.max_retries - 1:
                    prompt += f"\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆå­˜åœ¨ä»¥ä¸‹æ ¼å¼é”™è¯¯ï¼Œè¯·ä¸¥æ ¼ä¿®æ­£ï¼Œç¡®ä¿Markdownæ ¼å¼å®Œå…¨ç¬¦åˆæ¨¡æ¿ï¼š\n" + "\n".join(errors)
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•ï¼Œä»…è¿”å›æœ‰æ•ˆçš„
                    return valid_results
            
            await asyncio.sleep(1)
            
        return []

    async def generate_final_report(self, news_items: List[NewsItem], arxiv_papers: List[Dict] = None, quality_check: bool = True) -> Optional[str]:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (å¤šè½®ç”Ÿæˆæ¨¡å¼)
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            arxiv_papers: ç›¸å…³ arXiv è®ºæ–‡åˆ—è¡¨
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬å…­æ­¥ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (å¤šè½®ç”Ÿæˆæ¨¡å¼)...")
        
        if not news_items:
            logger.warning("æ²¡æœ‰æ–°é—»å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return None

        # 1. å‡†å¤‡æ•°æ®ï¼šç­›é€‰ S/A/B çº§æ–°é—»è¿›å…¥æ­£æ–‡
        valid_items = [item for item in news_items if item.ranking_level in ["S", "A", "B"]]
        # å¦‚æœ S/A/B å¤ªå°‘ï¼Œè€ƒè™‘æŠŠ C çº§çš„å‰å‡ ååŠ è¿›æ¥
        if len(valid_items) < 5:
             c_items = [item for item in news_items if item.ranking_level == "C"]
             valid_items.extend(c_items[:5])
        
        # ç¡®ä¿æŒ‰åˆ†æ•°æ’åº
        valid_items.sort(key=lambda x: x.final_score, reverse=True)
        
        logger.info(f"å°†ä¸º {len(valid_items)} æ¡é«˜ä»·å€¼æ–°é—»ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")

        # 2. åˆ†æ‰¹ç”Ÿæˆå†…å®¹ (Batch Processing)
        batch_size = 5
        generated_entries = []
        
        for i in range(0, len(valid_items), batch_size):
            batch = valid_items[i:i + batch_size]
            logger.info(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Šè¯¦æƒ…ï¼šæ‰¹æ¬¡ {i // batch_size + 1} (å…± {len(batch)} æ¡)")
            
            # å¹¶å‘ç”Ÿæˆè¯¥æ‰¹æ¬¡çš„å†…å®¹
            entries = await self._generate_news_entries_batch(batch, candidate_papers=arxiv_papers)
            if entries:
                generated_entries.extend(entries)
            else:
                logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} ç”Ÿæˆå¤±è´¥")

        # 3. ç»„ç»‡å†…å®¹ å¹¶ æå–å·²ä½¿ç”¨çš„é“¾æ¥
        # å»ºç«‹ article_id åˆ° news_item çš„æ˜ å°„ï¼Œæ–¹ä¾¿è·å–é¢å¤–ä¿¡æ¯
        item_map = {item.article_id: item for item in valid_items}
        
        category_map = {
            "Infrastructure": [],
            "Model": [],
            "Application": []
        }
        
        # ç”¨äºè®°å½•åœ¨æ­£æ–‡ä¸­å·²ç»å‡ºç°è¿‡çš„é“¾æ¥ï¼Œé¿å…åœ¨æ‹“å±•é˜…è¯»ä¸­é‡å¤
        used_urls = set()
        link_pattern = re.compile(r'\[.*?\]\((https?://.*?)\)')
        
        for entry in generated_entries:
            cat = entry.get("category", "Model")
            if cat not in category_map:
                cat = "Model"  # Fallback
            
            content = entry.get("markdown_content", "")
            category_map[cat].append(content)
            
            # æå–æ­£æ–‡ä¸­çš„æ‰€æœ‰é“¾æ¥
            found_links = link_pattern.findall(content)
            for link in found_links:
                # ç®€å•æ ‡å‡†åŒ–
                clean_link = link.strip().rstrip('/')
                used_urls.add(clean_link)
                # é’ˆå¯¹ arXivï¼ŒåŒæ—¶è®°å½• abs å’Œ pdf ç‰ˆæœ¬ä»¥é˜²ä¸‡ä¸€
                if "arxiv.org/abs/" in clean_link:
                    used_urls.add(clean_link.replace("/abs/", "/pdf/"))
                elif "arxiv.org/pdf/" in clean_link:
                    used_urls.add(clean_link.replace("/pdf/", "/abs/"))

        # 4. ç”Ÿæˆâ€œæœ¬æœŸé€Ÿè§ˆâ€ (Top 10)
        top_items = valid_items[:10]
        if len(top_items) < 10:
            # éœ€è¦ä» C çº§ä¸­è¡¥è¶³ï¼ŒæŒ‰æ€»åˆ†æ’åºå–å‰è‹¥å¹²
            c_pool = [item for item in news_items if item.ranking_level == "C" and item not in top_items]
            # news_items åœ¨ step4_rank å·²æŒ‰ final_score æ’åºï¼Œè¿™é‡Œä¿æŒé¡ºåºè¿½åŠ 
            for c_item in c_pool:
                if len(top_items) >= 10:
                    break
                top_items.append(c_item)
        overview_prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆâ€œæœ¬æœŸé€Ÿè§ˆâ€åˆ—è¡¨ã€‚
è¦æ±‚ï¼š
- æ¯æ¡æ–°é—»ç”¨ä¸€è¡Œ Markdown åˆ—è¡¨é¡¹è¡¨ç¤ºã€‚
- æ ¼å¼ï¼š* **[[æ ‡ç­¾]]** [**æ–°é—»æ ‡é¢˜**]: [1-2å¥è¯æ ¸å¿ƒçœ‹ç‚¹]
- æ ‡ç­¾ç¤ºä¾‹ï¼š[å¤§æ¨¡å‹], [èŠ¯ç‰‡], [åº”ç”¨]ç­‰
- å¿…é¡»ä¸¥æ ¼éµå®ˆä¸Šè¿°æ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚

æ–°é—»æ•°æ®ï¼š
{json.dumps([{"title": item.title, "description": item.description} for item in top_items], ensure_ascii=False, indent=2)}

è¯·ç›´æ¥è¿”å› Markdown åˆ—è¡¨ã€‚
"""
        overview_content = self._call_llm(overview_prompt) or "ç”Ÿæˆå¤±è´¥"
        
        # ç®€å•éªŒè¯æ¦‚è§ˆæ ¼å¼
        if "**[[" not in overview_content:
             logger.warning("æ¦‚è§ˆæ ¼å¼å¯èƒ½ä¸ç¬¦åˆè¦æ±‚ï¼Œå°è¯•ä¿®å¤...")
             # ç®€å•çš„é‡è¯•é€»è¾‘
             overview_prompt += "\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ç¡®ä¿æ¯è¡Œä»¥ '* **[[æ ‡ç­¾]]**' å¼€å¤´ã€‚"
             retry_content = self._call_llm(overview_prompt)
             if retry_content and "**[[" in retry_content:
                 overview_content = retry_content

        # 5. è§£æâ€œæœ¬æœŸé€Ÿè§ˆâ€æ ‡ç­¾ï¼Œæ„å»º æ ‡é¢˜->æ ‡ç­¾ æ˜ å°„ï¼Œä¾¿äºæ‹“å±•é˜…è¯»åˆ†ç»„
        title_tag_map = {}
        tag_line_pattern = re.compile(r"\*\s+\*\*\[\[(?P<tag>.+?)\]\]\*\*\s+\[\*\*(?P<title>.+?)\*\*\]")
        for line in overview_content.splitlines():
            m = tag_line_pattern.search(line)
            if m:
                title_tag_map[m.group("title").strip()] = m.group("tag").strip()

        # å»ºç«‹ article_id -> category çš„æ˜ å°„ (ç”¨äºé Top 10 æ–°é—»çš„æ ‡ç­¾å›é€€)
        id_category_map = {}
        for entry in generated_entries:
            cat = entry.get("category", "Model")
            # æ˜ å°„è‹±æ–‡åˆ†ç±»åˆ°ä¸­æ–‡æ ‡ç­¾
            cn_cat = {
                "Infrastructure": "[åŸºç¡€è®¾æ–½]",
                "Model": "[æ¨¡å‹ä¸æŠ€æœ¯]",
                "Application": "[åº”ç”¨ä¸æ™ºèƒ½ä½“]"
            }.get(cat, "[å…¶ä»–]")
            id_category_map[entry.get("article_id")] = cn_cat

        # 6. ç”Ÿæˆâ€œæ‹“å±•é˜…è¯»â€ (Reference Links)
        # è¿™é‡Œæ”¶é›†æ‰€æœ‰æ–°é—»ï¼ˆåŒ…æ‹¬ C çº§ï¼‰çš„å‚è€ƒé“¾æ¥ï¼Œä»¥åŠ arXiv è®ºæ–‡
        reference_section = ""
        # å€™é€‰é“¾æ¥åˆ—è¡¨ï¼Œç»“æ„: {'markdown': str, 'type': 'arxiv'|'other', 'tag': str}
        candidates = []
        
        seen_urls = set(used_urls)
        used_arxiv_urls = {u for u in used_urls if "arxiv.org" in u}
        
        def is_valid_ref_link(url: str, title: str) -> bool:
            if not url or not title:
                return False
            # è¿‡æ»¤ç¤¾äº¤åˆ†äº«é“¾æ¥
            if any(x in url for x in ["facebook.com/sharer", "twitter.com/intent", "linkedin.com/share", "reddit.com/submit", "weibo.com", "service.weibo.com"]):
                return False
            # è¿‡æ»¤é€šç”¨ä¸»é¡µ (ä¾‹å¦‚ https://blog.google/ )
            # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœ URL å¾ˆçŸ­æˆ–è€…æ˜¯æ ¹åŸŸåï¼Œå¯èƒ½ä¸æ˜¯å…·ä½“çš„æ–‡ç« 
            if url.count('/') < 3: 
                 return False
            return True
        
        # 6.1 æ”¶é›†ç‹¬ç«‹ arXiv è®ºæ–‡
        if arxiv_papers:
            for paper in arxiv_papers:
                url = paper['url']
                # å¦‚æœè¯¥è®ºæ–‡å·²åœ¨æ­£æ–‡ä¸­å¼•ç”¨ï¼Œåˆ™ä¸æ”¾å…¥æ‹“å±•é˜…è¯»
                if url in used_arxiv_urls:
                    continue
                    
                if url not in seen_urls:
                    candidates.append({
                        "markdown": f"* [{paper['title']}]({url}) - arXiv",
                        "type": "arxiv",
                        "tag": "[å‰æ²¿ç ”ç©¶]"
                    })
                    seen_urls.add(url)
        
        # 6.2 æ”¶é›†æ–°é—»å‚è€ƒé“¾æ¥
        for item in news_items:
            if not item.reference_links:
                continue
                
            # ç¡®å®šæ ‡ç­¾
            tag = title_tag_map.get(item.title)
            if not tag:
                # å›é€€åˆ°åˆ†ç±»
                tag = id_category_map.get(item.article_id, "[è¡Œä¸šåŠ¨æ€]")
            
            try:
                refs = json.loads(item.reference_links)
                for ref in refs:
                    url = ref.get('url', '')
                    title = ref.get('title', 'Ref')
                    
                    if url and url not in seen_urls and is_valid_ref_link(url, title):
                        is_arxiv = "arxiv.org" in url
                        source_tag = f" - {ref.get('type', 'Reference')}"
                        
                        candidates.append({
                            "markdown": f"* [{title}]({url}){source_tag}",
                            "type": "arxiv" if is_arxiv else "other",
                            "tag": tag
                        })
                        seen_urls.add(url)
            except:
                pass
        
        # 6.3 ç­›é€‰é€»è¾‘ (Total 25, Arxiv 15, Other 10)
        MAX_TOTAL = 25
        TARGET_ARXIV = 15 # 60%
        
        arxiv_candidates = [c for c in candidates if c['type'] == 'arxiv']
        other_candidates = [c for c in candidates if c['type'] == 'other']
        
        final_list = []
        
        # 1. ä¼˜å…ˆå– arXiv
        take_arxiv = min(len(arxiv_candidates), TARGET_ARXIV)
        final_list.extend(arxiv_candidates[:take_arxiv])
        
        # 2. å¡«è¡¥å…¶ä»–
        remaining_slots = MAX_TOTAL - len(final_list)
        take_other = min(len(other_candidates), remaining_slots)
        final_list.extend(other_candidates[:take_other])
        
        # 3. å¦‚æœè¿˜æœ‰ç©ºä½ä¸”æœ‰å‰©ä½™ arXivï¼Œç»§ç»­å¡«
        if len(final_list) < MAX_TOTAL and len(arxiv_candidates) > take_arxiv:
            rest_slots = MAX_TOTAL - len(final_list)
            final_list.extend(arxiv_candidates[take_arxiv : take_arxiv + rest_slots])
            
        # 6.4 æŒ‰æ ‡ç­¾åˆ†ç»„è¾“å‡º
        from collections import defaultdict
        grouped_links = defaultdict(list)
        for item in final_list:
            grouped_links[item['tag']].append(item['markdown'])
            
        sections = []
        # è¾“å‡º Top æ ‡ç­¾ (æŒ‰é€Ÿè§ˆé¡ºåº)
        sorted_top_tags = []
        for t in title_tag_map.values():
            if t not in sorted_top_tags: sorted_top_tags.append(t)
            
        for tag in sorted_top_tags:
            if tag in grouped_links:
                sections.append(f"### {tag}\n" + chr(10).join(grouped_links[tag]))
                del grouped_links[tag]
        
        # è¾“å‡º [å‰æ²¿ç ”ç©¶] (arXiv)
        if "[å‰æ²¿ç ”ç©¶]" in grouped_links:
             sections.append(f"### [å‰æ²¿ç ”ç©¶]\n" + chr(10).join(grouped_links["[å‰æ²¿ç ”ç©¶]"]))
             del grouped_links["[å‰æ²¿ç ”ç©¶]"]
             
        # è¾“å‡ºå‰©ä½™
        for tag, links in grouped_links.items():
            sections.append(f"### {tag}\n" + chr(10).join(links))
            
        reference_section = "\n\n".join(sections)

        # 6. æœ€ç»ˆç»„è£…
        publish_times = [item.publish_time for item in news_items if item.publish_time]
        if publish_times:
            date_range_start = datetime.fromtimestamp(min(publish_times)).strftime('%Y-%m-%d')
            date_range_end = datetime.fromtimestamp(max(publish_times)).strftime('%Y-%m-%d')
        else:
            today_str = datetime.now().strftime('%Y-%m-%d')
            date_range_start = today_str
            date_range_end = today_str

        final_report = f"""# AI å‰æ²¿åŠ¨æ€é€ŸæŠ¥ ({date_range_start} è‡³ {date_range_end})

## âš¡ æœ¬æœŸé€Ÿè§ˆ

{overview_content}

---

## 1. AI åŸºç¡€è®¾æ–½

{chr(10).join(category_map["Infrastructure"]) if category_map["Infrastructure"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## 2. AI æ¨¡å‹ä¸æŠ€æœ¯

{chr(10).join(category_map["Model"]) if category_map["Model"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## 3. AI åº”ç”¨ä¸æ™ºèƒ½ä½“

{chr(10).join(category_map["Application"]) if category_map["Application"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## æ‹“å±•é˜…è¯»

{reference_section}
"""
        return final_report

    def generate_final_report_old(self, news_items: List[NewsItem], quality_check: bool = True) -> Optional[str]:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äº”æ­¥ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        
        if not news_items:
            logger.warning("æ²¡æœ‰æ–°é—»å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return None
        
        # è¯»å–æ¨¡æ¿
        try:
            with open(self.template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        except Exception as e:
            logger.error(f"è¯»å–æ¨¡æ¿å¤±è´¥: {e}")
            return None
        
        # æŒ‰è¯„çº§åˆ†ç»„
        news_by_level = defaultdict(list)
        for item in news_items:
            news_by_level[item.ranking_level].append(item)
        
        # è®¡ç®—è¦†ç›–æ—¥æœŸèŒƒå›´ï¼ˆç”¨äºæç¤ºæ¨¡å‹æ­£ç¡®å¡«å†™å¤´éƒ¨åŒºé—´ï¼‰
        publish_times = [item.publish_time for item in news_items if item.publish_time]
        if publish_times:
            date_range_start = datetime.fromtimestamp(min(publish_times)).strftime('%Y-%m-%d')
            date_range_end = datetime.fromtimestamp(max(publish_times)).strftime('%Y-%m-%d')
        else:
            today_str = datetime.now().strftime('%Y-%m-%d')
            date_range_start = today_str
            date_range_end = today_str

        # æ ¼å¼åŒ–æ–°é—»æ•°æ®
        formatted_news = ""
        for level in ["S", "A", "B", "C"]:
            items = news_by_level[level]
            if items:
                formatted_news += f"\n## {level}çº§æ–°é—» ({len(items)}æ¡)\n\n"
                for i, item in enumerate(items, 1):
                    pub_date = datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
                    formatted_news += f"### [{i}] {item.title}\n"
                    formatted_news += f"- **æ¥æº**: {item.source}\n"
                    formatted_news += f"- **é“¾æ¥**: {item.url}\n"
                    formatted_news += f"- **å‘å¸ƒæ—¶é—´**: {pub_date}\n"
                    formatted_news += f"- **è¯„åˆ†**: {item.final_score:.2f} (æŠ€æœ¯å½±å“:{item.tech_impact}, è¡Œä¸šèŒƒå›´:{item.industry_scope}, çƒ­åº¦:{item.hype_score})\n"
                    formatted_news += f"- **äº‹ä»¶çƒ­åº¦**: {item.event_count} æ¡ç›¸å…³æŠ¥é“\n"
                    formatted_news += f"- **æ‘˜è¦**: {item.description}\n"
                    
                    # æ·»åŠ å‚è€ƒé“¾æ¥
                    if item.reference_links:
                        try:
                            ref_links = json.loads(item.reference_links)
                            if ref_links:
                                formatted_news += f"- **åŸå§‹æ¥æº**:\n"
                                for ref in ref_links:
                                    formatted_news += f"  - [{ref['title']}]({ref['url']}) (ç±»å‹: {ref['type']})\n"
                        except:
                            pass
                    
                    formatted_news += "\n"
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå‰æ²¿ç§‘æŠ€åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ç»è¿‡ç­›é€‰ã€å½’ç±»ã€å»é‡å’Œæ’åºçš„æ–°é—»æ•°æ®ï¼Œç¼–å†™ä¸€ä»½é«˜è´¨é‡çš„AIå‰æ²¿åŠ¨æ€é€ŸæŠ¥ã€‚

**å½“å‰æ—¥æœŸ**: {today_str}
**æŠ¥å‘Šè¦†ç›–æ—¥æœŸèŒƒå›´ï¼ˆåŠ¡å¿…ç”¨äºæ–‡é¦–æ‹¬å·åŒºé—´ï¼‰**: {date_range_start} è‡³ {date_range_end}

**æŠ¥å‘Šæ¨¡æ¿å’Œè¦æ±‚:**
{template_content}

**ç»è¿‡å¤„ç†çš„æ–°é—»æ•°æ®:**
{formatted_news}

**ç‰¹åˆ«æŒ‡ä»¤:**
1. ä¸¥æ ¼éµå¾ªæ¨¡æ¿æ ¼å¼ï¼Œä¸€çº§æ ‡é¢˜å’ŒäºŒçº§æ ‡é¢˜å¿…é¡»ä¸æ¨¡æ¿ä¸€è‡´
2. æ–°é—»å·²æŒ‰ S/A/B/C çº§åˆ«æ’åºï¼Œä¼˜å…ˆå…³æ³¨ S çº§å’Œ A çº§æ–°é—»
3. æ·±åº¦è§£è¯»éƒ¨åˆ†è¦æœ‰å®è´¨å†…å®¹ï¼Œç»“åˆæŠ€æœ¯èƒŒæ™¯å’Œè¡Œä¸šå½±å“è¿›è¡Œåˆ†æ
4. å¦‚æœæ–°é—»æ•°æ®ä¸­æä¾›äº†"åŸå§‹æ¥æº"ï¼Œé˜…è¯»åŸæ–‡çš„é“¾æ¥å¿…é¡»ä½¿ç”¨åŸå§‹æ¥æºURLï¼Œç¦æ­¢ä½¿ç”¨é‡å­ä½è‡ªèº«é“¾æ¥
5. Source å­—æ®µä¼˜å…ˆå¡«å†™åŸå§‹æ¥æºåç§°ï¼ˆå¦‚ OpenAI, arXiv ç­‰ï¼‰
6. è¯­è¨€é£æ ¼è¦ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ´å¯ŸåŠ›
7. è¾“å‡ºå¿…é¡»æ˜¯ Markdown æ ¼å¼
8. ä¸‰çº§æ ‡é¢˜åŸºäºå†…å®¹åˆ†æç”Ÿæˆï¼Œè¦æœ‰ä¸ªæ€§åŒ–å’Œæ´å¯ŸåŠ›

è¯·ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Šå†…å®¹ã€‚
"""
        
        # è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
        max_attempts = 3 if quality_check else 1
        
        for attempt in range(max_attempts):
            logger.info(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Š (å°è¯• {attempt + 1}/{max_attempts})...")
            
            report_content = self._call_llm(prompt, temperature=0.3)
            
            if not report_content:
                logger.error("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                continue
            
            # è´¨é‡æ£€æŸ¥
            if quality_check and attempt < max_attempts - 1:
                if self._check_report_quality(report_content, template_content):
                    logger.info("æŠ¥å‘Šè´¨é‡æ£€æŸ¥é€šè¿‡")
                    return report_content
                else:
                    logger.warning(f"æŠ¥å‘Šè´¨é‡æ£€æŸ¥æœªé€šè¿‡ï¼Œé‡æ–°ç”Ÿæˆ (å°è¯• {attempt + 2}/{max_attempts})")
                    # æ·»åŠ è´¨é‡åé¦ˆåˆ°æç¤ºè¯
                    prompt += "\n\n**è´¨é‡é—®é¢˜**: ä¸Šä¸€æ¬¡ç”Ÿæˆçš„æŠ¥å‘Šæ ¼å¼æˆ–å†…å®¹ä¸ç¬¦åˆè¦æ±‚ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç”Ÿæˆã€‚"
            else:
                return report_content
        
        return report_content
    
    def _check_report_quality(self, report: str, template: str) -> bool:
        """
        æ£€æŸ¥æŠ¥å‘Šè´¨é‡
        
        Args:
            report: ç”Ÿæˆçš„æŠ¥å‘Š
            template: æ¨¡æ¿å†…å®¹
            
        Returns:
            æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥
        """
        # ç®€å•æ£€æŸ¥ï¼šæ˜¯å¦åŒ…å«å…³é”®ç« èŠ‚
        required_sections = ["AIå‰æ²¿åŠ¨æ€é€ŸæŠ¥", "æœ¬å‘¨ç„¦ç‚¹", "æ·±åº¦è§£è¯»"]
        
        for section in required_sections:
            if section not in report:
                logger.warning(f"æŠ¥å‘Šç¼ºå°‘å¿…éœ€ç« èŠ‚: {section}")
                return False
        
        # æ£€æŸ¥æŠ¥å‘Šé•¿åº¦
        if len(report) < 500:
            logger.warning("æŠ¥å‘Šå†…å®¹è¿‡çŸ­")
            return False
        
        return True
    
    async def run(self, days: int = 3, save_intermediate: bool = True) -> Optional[str]:
        """
        è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹
        
        Args:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            æœ€ç»ˆæŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("GeminiAIReportAgent å¼€å§‹è¿è¡Œ")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        # 1. è·å–æ•°æ®
        news_items = await self.fetch_articles_from_db(days=days)
        if not news_items:
            logger.error("æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return None
        
        # 2. è¿‡æ»¤
        news_items = await self.step1_filter(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "01_filtered")
        
        # 3. å½’ç±»
        news_items = await self.step2_cluster(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "02_clustered")
        
        # 4. å»é‡
        news_items = await self.step3_deduplicate(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "03_deduplicated")
        
        # 5. æ’åº
        news_items = await self.step4_rank(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "04_ranked")
            
        # 6. è·å– arXiv è®ºæ–‡
        arxiv_papers = await self.step5_fetch_arxiv_papers(news_items)
        if save_intermediate:
            # ä¿å­˜ arXiv ç»“æœï¼ˆç®€å•åŒ…è£…ä¸€ä¸‹ä»¥ä¾¿å¤ç”¨ä¿å­˜é€»è¾‘ï¼Œæˆ–è€…ç›´æ¥å­˜jsonï¼‰
            arxiv_output_dir = Path("final_reports") / "intermediate"
            arxiv_output_dir.mkdir(exist_ok=True, parents=True)
            arxiv_output_file = arxiv_output_dir / f"05_arxiv_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.json"
            with open(arxiv_output_file, 'w', encoding='utf-8') as f:
                json.dump(arxiv_papers, f, ensure_ascii=False, indent=2)
            logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {arxiv_output_file}")
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        report_content = await self.generate_final_report(news_items, arxiv_papers=arxiv_papers, quality_check=True)
        
        if report_content:
            # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
            output_dir = Path("final_reports")
            output_dir.mkdir(exist_ok=True)
            output_file = output_dir / f"AI_Report_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.md"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info("=" * 60)
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼è€—æ—¶: {elapsed:.2f}ç§’")
            logger.info(f"æŠ¥å‘Šä¿å­˜è‡³: {output_file}")
            logger.info("=" * 60)
            
            return report_content
        else:
            logger.error("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            return None
    
    def _save_intermediate_results(self, news_items: List[NewsItem], stage: str):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        output_dir = Path("final_reports") / "intermediate"
        output_dir.mkdir(exist_ok=True, parents=True)
        
        output_file = output_dir / f"{stage}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.json"
        
        data = [item.to_dict() for item in news_items]
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    async def test_agent():
        agent = GeminiAIReportAgent(max_retries=2)
        await agent.run(days=3, save_intermediate=True)
    
    asyncio.run(test_agent())

