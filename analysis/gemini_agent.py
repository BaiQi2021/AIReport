"""
GeminiAIReportAgent - æ™ºèƒ½æ–°é—»å¤„ç†Agent
å®ç°ï¼šè¿‡æ»¤ -> å½’ç±» -> å»é‡ -> æ’åº -> æŠ¥å‘Šç”Ÿæˆ çš„å¤šè½®æµç¨‹
"""

import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
import re
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
import time

from openai import OpenAI
import httpx
from sqlalchemy import select, desc, or_, delete

from database.models import QbitaiArticle, CompanyArticle, AibaseArticle, BaaiHubArticle
from database.db_session import get_session
import config
from crawler import utils

settings = config.settings
logger = utils.logger


class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    def __init__(self, article_id: str, title: str, description: str, 
                 content: str, url: str, source: str, publish_time: int,
                 reference_links: Optional[str] = None,
                 original_id: Optional[str] = None,
                 source_table: Optional[str] = None):
        self.article_id = article_id
        self.title = title
        self.description = description
        self.content = content  # ä¿å­˜å®Œæ•´å†…å®¹ï¼Œåœ¨å…·ä½“ä½¿ç”¨æ—¶å†æŒ‰éœ€æˆªå–
        self.url = url
        self.source = source  # æ¥æºï¼šqbitai, openai, googleç­‰
        self.publish_time = publish_time
        self.reference_links = reference_links
        
        # æ•°æ®åº“å›æº¯å­—æ®µ
        self.original_id = original_id
        self.source_table = source_table
        
        # å¤„ç†ç»“æœå­—æ®µ
        self.filter_decision = None  # "ä¿ç•™" or "å‰”é™¤"
        self.filter_reason = None
        self.event_id = None
        self.event_count = 0
        self.dedup_decision = None  # "ä¿ç•™" or "åˆ é™¤"
        self.dedup_reason = None
        self.tech_impact = 0
        self.industry_scope = 0
        self.hype_score = 0
        self.final_score = 0.0
        self.ranking_level = "C"
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "article_id": self.article_id,
            "title": self.title,
            "description": self.description,
            "content": self.content,
            "url": self.url,
            "source": self.source,
            "publish_time": self.publish_time,
            "reference_links": self.reference_links,
            "filter_decision": self.filter_decision,
            "filter_reason": self.filter_reason,
            "event_id": self.event_id,
            "event_count": self.event_count,
            "dedup_decision": self.dedup_decision,
            "dedup_reason": self.dedup_reason,
            "tech_impact": self.tech_impact,
            "industry_scope": self.industry_scope,
            "hype_score": self.hype_score,
            "final_score": self.final_score,
            "ranking_level": self.ranking_level
        }


class GeminiAIReportAgent:
    """åŸºäº Gemini çš„æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ Agent"""
    
    def __init__(self, max_retries: int = 5):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            max_retries: æ¯ä¸ªæ­¥éª¤çš„æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ litellm é»˜è®¤å€¼
        self.api_key = settings.REPORT_ENGINE_API_KEY
        self.base_url = settings.REPORT_ENGINE_BASE_URL
        self.model_name = settings.REPORT_ENGINE_MODEL_NAME
        self.max_retries = max_retries
        
        if not self.api_key:
            raise ValueError("API Key not configured in settings")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            http_client=httpx.Client(verify=False, timeout=120.0)
        )
        
        self.template_path = Path(__file__).parent / "templates" / "AIReport_example.md"
        
    async def fetch_articles_from_db(self, days: int = 3, limit: int = 200) -> List[NewsItem]:
        """
        ä»æ•°æ®åº“è·å–æ–°é—»æ•°æ®
        
        Args:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            limit: æ¯ä¸ªè¡¨çš„æœ€å¤§æ¡æ•°
            
        Returns:
            æ–°é—»æ¡ç›®åˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨ä»æ•°æ®åº“è·å–æœ€è¿‘ {days} å¤©çš„æ–°é—»æ•°æ®...")
        
        cutoff_date = (datetime.now() - timedelta(days=days)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )
        cutoff_ts = int(cutoff_date.timestamp())
        
        news_items = []
        
        async with get_session() as session:
            # è·å–é‡å­ä½æ–‡ç« 
            stmt = (
                select(QbitaiArticle)
                .where(QbitaiArticle.publish_time >= cutoff_ts)
                .order_by(desc(QbitaiArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            qbitai_articles = result.scalars().all()
            
            for art in qbitai_articles:
                news_items.append(NewsItem(
                    article_id=f"qbitai_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source="é‡å­ä½",
                    publish_time=art.publish_time,
                    reference_links=art.reference_links,
                    original_id=art.article_id,
                    source_table="qbitai_article"
                ))
            
            # è·å–å…¬å¸å®˜æ–¹æ–‡ç« 
            stmt = (
                select(CompanyArticle)
                .where(CompanyArticle.publish_time >= cutoff_ts)
                .order_by(desc(CompanyArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            company_articles = result.scalars().all()
            
            for art in company_articles:
                news_items.append(NewsItem(
                    article_id=f"{art.company}_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source=art.company.upper(),
                    publish_time=art.publish_time,
                    reference_links=art.reference_links,
                    original_id=art.article_id,
                    source_table="company_article"
                ))

            # è·å– Aibase æ–‡ç« 
            stmt = (
                select(AibaseArticle)
                .where(AibaseArticle.publish_time >= cutoff_ts)
                .order_by(desc(AibaseArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            aibase_articles = result.scalars().all()
            
            for art in aibase_articles:
                news_items.append(NewsItem(
                    article_id=f"aibase_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source="AIbase",
                    publish_time=art.publish_time,
                    reference_links=art.reference_links,
                    original_id=art.article_id,
                    source_table="aibase_article"
                ))

            # è·å– BAAI Hub æ–‡ç« 
            stmt = (
                select(BaaiHubArticle)
                .where(BaaiHubArticle.publish_time >= cutoff_ts)
                .order_by(desc(BaaiHubArticle.publish_time))
                .limit(limit)
            )
            result = await session.execute(stmt)
            baai_articles = result.scalars().all()
            
            for art in baai_articles:
                news_items.append(NewsItem(
                    article_id=f"baai_{art.article_id}",
                    title=art.title,
                    description=art.description or "",
                    content=art.content or "",
                    url=art.article_url,
                    source="BAAI Hub",
                    publish_time=art.publish_time,
                    reference_links=art.reference_links,
                    original_id=art.article_id,
                    source_table="baai_hub_article"
                ))
        
        logger.info(f"å…±è·å– {len(news_items)} æ¡æ–°é—»æ•°æ®")
        return news_items
    
    async def _delete_articles_from_db(self, items_to_delete: List[NewsItem]):
        """
        ä»æ•°æ®åº“ä¸­åˆ é™¤æŒ‡å®šçš„æ–‡ç« 
        
        Args:
            items_to_delete: éœ€è¦åˆ é™¤çš„æ–°é—»æ¡ç›®åˆ—è¡¨
        """
        if not items_to_delete:
            return

        logger.info(f"æ­£åœ¨ä»æ•°æ®åº“åˆ é™¤ {len(items_to_delete)} æ¡æ— æ•ˆ/é‡å¤æ•°æ®...")
        
        # æŒ‰è¡¨åˆ†ç»„
        qbitai_ids = []
        company_ids = []
        aibase_ids = []
        baai_ids = []
        
        for item in items_to_delete:
            if not item.original_id or not item.source_table:
                logger.warning(f"æ— æ³•åˆ é™¤æ–‡ç«  {item.article_id}: ç¼ºå°‘åŸå§‹IDæˆ–è¡¨ä¿¡æ¯")
                continue
                
            if item.source_table == "qbitai_article":
                qbitai_ids.append(item.original_id)
            elif item.source_table == "company_article":
                company_ids.append(item.original_id)
            elif item.source_table == "aibase_article":
                aibase_ids.append(item.original_id)
            elif item.source_table == "baai_hub_article":
                baai_ids.append(item.original_id)
        
        async with get_session() as session:
            try:
                if qbitai_ids:
                    stmt = delete(QbitaiArticle).where(QbitaiArticle.article_id.in_(qbitai_ids))
                    result = await session.execute(stmt)
                    logger.info(f"å·²åˆ é™¤ {result.rowcount} æ¡ Qbitai æ•°æ®")
                
                if company_ids:
                    stmt = delete(CompanyArticle).where(CompanyArticle.article_id.in_(company_ids))
                    result = await session.execute(stmt)
                    logger.info(f"å·²åˆ é™¤ {result.rowcount} æ¡ Company æ•°æ®")

                if aibase_ids:
                    stmt = delete(AibaseArticle).where(AibaseArticle.article_id.in_(aibase_ids))
                    result = await session.execute(stmt)
                    logger.info(f"å·²åˆ é™¤ {result.rowcount} æ¡ Aibase æ•°æ®")

                if baai_ids:
                    stmt = delete(BaaiHubArticle).where(BaaiHubArticle.article_id.in_(baai_ids))
                    result = await session.execute(stmt)
                    logger.info(f"å·²åˆ é™¤ {result.rowcount} æ¡ BAAI Hub æ•°æ®")
                
                await session.commit()
            except Exception as e:
                logger.error(f"åˆ é™¤æ•°æ®åº“æ•°æ®å¤±è´¥: {e}")
                await session.rollback()

    def _call_llm(self, prompt: str, temperature: float = 0.1) -> Optional[str]:
        """
        è°ƒç”¨ LLM API
        
        Args:
            prompt: æç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            LLM å“åº”å†…å®¹
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                extra_body={
                    "metadata": {
                        "generation_name": "gemini-agent-generation",
                        "generation_id": f"gen-{int(time.time())}",
                        "trace_id": f"trace-{int(time.time())}",
                        "trace_user_id": "gemini-agent-user"
                    }
                }
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"LLM API è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _parse_json_response(self, response: str) -> Optional[List[Dict]]:
        """
        è§£æ JSON å“åº”ï¼Œæ”¯æŒæå– markdown ä»£ç å—ä¸­çš„ JSON
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„ JSON åˆ—è¡¨
        """
        if not response:
            return None
        
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(response)
        except json.JSONDecodeError:
            # å°è¯•æå– markdown ä»£ç å—ä¸­çš„ JSON
            import re
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except:
                    pass
            
            logger.error(f"æ— æ³•è§£æ JSON å“åº”: {response[:200]}")
            return None
    
    async def step1_filter(self, news_items: List[NewsItem], batch_size: int = 20) -> List[NewsItem]:
        """
        ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤ (Filtering)
        å‰”é™¤ä¸ AI æ ¸å¿ƒæŠ€æœ¯è¿›å±•æ— å…³çš„å™ªéŸ³ä¿¡æ¯
        
        Args:
            news_items: åŸå§‹æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬ä¸€æ­¥ã€‘å¼€å§‹è¿‡æ»¤ (Filtering)...")
        logger.info(f"æ“ä½œå‰ï¼šå…± {len(news_items)} æ¡æ–°é—»")
        if news_items:
            logger.info("æ“ä½œå‰æ ‡é¢˜åˆ—è¡¨ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for idx, item in enumerate(news_items[:10], 1):
                logger.info(f"  [{idx}] {item.title[:60]}... (æ¥æº: {item.source})")

        items_to_delete = []

        # 1. é¢„è¿‡æ»¤ï¼šå‰”é™¤å†…å®¹è¿‡çŸ­æˆ–æ— å†…å®¹çš„æ–°é—»
        valid_news_items = []
        for item in news_items:
            # ç®€å•çš„è§„åˆ™è¿‡æ»¤ï¼šå†…å®¹é•¿åº¦å°‘äº100å­—ç¬¦è§†ä¸ºæ— æ•ˆå†…å®¹
            # æ³¨æ„ï¼šNewsItem åˆå§‹åŒ–æ—¶å·²æˆªå–å‰1000å­—ç¬¦ï¼Œè¿™é‡Œåˆ¤æ–­çš„æ˜¯æˆªå–åçš„é•¿åº¦
            # ä½†å¦‚æœåŸå†…å®¹æœ¬èº«å°±å¾ˆå°‘ï¼Œè¿™é‡Œä¹Ÿèƒ½æ£€æµ‹å‡ºæ¥
            if item.content and len(item.content.strip()) >= 50:
                valid_news_items.append(item)
            else:
                logger.info(f"é¢„è¿‡æ»¤å‰”é™¤ï¼ˆå†…å®¹è¿‡å°‘ï¼‰: {item.title} (ID: {item.article_id})")
                items_to_delete.append(item)
        
        # åˆ é™¤é¢„è¿‡æ»¤æ‰çš„æ•°æ®
        if items_to_delete:
            logger.info(f"é¢„è¿‡æ»¤é˜¶æ®µï¼šå‰”é™¤ {len(items_to_delete)} æ¡ï¼ˆå†…å®¹è¿‡å°‘ï¼‰ï¼Œå‰©ä½™ {len(valid_news_items)} æ¡")
            await self._delete_articles_from_db(items_to_delete)
            items_to_delete = [] # æ¸…ç©ºåˆ—è¡¨ä»¥ä¾¿å¤ç”¨

        news_items = valid_news_items
        logger.info(f"å¾…å¤„ç†æ–°é—»æ•°: {len(news_items)}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        filtered_items = []
        rejected_items = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "content_snippet": item.content[:300],
                    "source": item.source,
                    "url": item.url
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯å†…å®¹ç­›é€‰ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œè¿‡æ»¤åˆ¤æ–­ã€‚

**è¿‡æ»¤è§„åˆ™ï¼š**

ã€ä¿ç•™æ¡ä»¶ã€‘ï¼ˆé€»è¾‘æˆ–ï¼Œæ»¡è¶³å…¶ä¸€å³ä¿ç•™ï¼‰:
1. æŠ€æœ¯/èƒ½åŠ›è¿›å±•: æ ¸å¿ƒå†…å®¹æ˜¯å…³äº AI æŠ€æœ¯ã€æ¨¡å‹ã€ç³»ç»Ÿã€å·¥ç¨‹æˆ–åº”ç”¨èƒ½åŠ›çš„å…·ä½“è¿›å±•ã€‚
2. å…³é”®é¢†åŸŸ: æ˜ç¡®æ¶‰åŠåŸºç¡€æ¨¡å‹ã€è®­ç»ƒ/æ¨ç†æ–¹æ³•ã€æ•°æ®å·¥ç¨‹ã€AI Infraã€Agent æ¡†æ¶æˆ–ç›¸å…³æŠ€æœ¯äº§å“ã€‚
3. æƒå¨æ¥æº: ä¿¡æ¯æ¥æºä¸ºå­¦æœ¯è®ºæ–‡ (å¦‚ arXiv)ã€å®˜æ–¹æŠ€æœ¯åšå®¢ (å¦‚ OpenAI Blog)ã€å®˜æ–¹äº§å“å‘å¸ƒé¡µæˆ– GitHub Release Notesã€‚

ã€å‰”é™¤æ¡ä»¶ã€‘ï¼ˆé€»è¾‘æˆ–ï¼Œæ»¡è¶³å…¶ä¸€å³å‰”é™¤ï¼‰:
1. å•†ä¸š/é‡‘è: è‚¡ä»·ã€å¸‚å€¼ã€èèµ„ã€IPOã€ä¼°å€¼ã€è´¢æŠ¥ã€æ”¶å…¥ã€ç”¨æˆ·è§„æ¨¡ã€æ”¶è´­ã€å¹¶è´­ã€‚
2. å¸‚åœºåˆ†æ: æŠ•èµ„è§‚ç‚¹ã€å¸‚åœºæƒ…ç»ªã€èµ„æœ¬åŠ¨å‘ã€æ— ç›´æ¥æŠ€æœ¯å…³è”çš„å•†ä¸šåˆä½œæ–°é—»ã€‚
3. äºŒæ¬¡è§£è¯»: ä¸ªäººè§‚ç‚¹ã€KOL é•¿ç¯‡åˆ†æã€æ— å¼•ç”¨çš„æ¨æ–‡æ€»ç»“ã€‚
4. ä¿¡æºä¸æ˜: æœªæ ‡æ³¨æ˜ç¡®æ¥æºã€æ¥æºä¸ºåŒ¿åè®ºå›æˆ–ç¤¾äº¤ç¾¤èŠæˆªå›¾ã€‚
5. èšåˆç±»: æ–°é—»èšåˆã€çƒ­ç‚¹æ±‡æ€»ã€æ’è¡Œæ¦œå•ç­‰æ— æ–°å¢æŠ€æœ¯ä¿¡æ¯çš„å†…å®¹ã€‚

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- filter_decision: "ä¿ç•™" æˆ– "å‰”é™¤"
- filter_reason: åˆ¤æ–­ç†ç”±ï¼ˆä¸€å¥è¯ç®€è¿°ï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "filter_decision": "ä¿ç•™", "filter_reason": "æ¶‰åŠå¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯çªç ´"}},
  {{"article_id": "yyy", "filter_decision": "å‰”é™¤", "filter_reason": "ä¸»è¦è®¨è®ºèèµ„å’Œå¸‚å€¼"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    # æ›´æ–°æ–°é—»æ¡ç›®
                    result_map = {r["article_id"]: r for r in results}
                    batch_kept = []
                    batch_rejected = []
                    for item in batch:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.filter_decision = r.get("filter_decision")
                            item.filter_reason = r.get("filter_reason")
                            
                            if item.filter_decision == "ä¿ç•™":
                                filtered_items.append(item)
                                batch_kept.append(item)
                            else:
                                rejected_items.append(item)
                                batch_rejected.append(item)
                    
                    # è¾“å‡ºæ‰¹æ¬¡å¤„ç†ç»“æœ
                    logger.info(f"  æ‰¹æ¬¡ç»“æœï¼šä¿ç•™ {len(batch_kept)} æ¡ï¼Œå‰”é™¤ {len(batch_rejected)} æ¡")
                    if batch_rejected:
                        logger.info(f"  å‰”é™¤çš„æ ‡é¢˜ï¼š")
                        for item in batch_rejected:
                            logger.info(f"    - {item.title[:60]}... (ç†ç”±: {item.filter_reason})")
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è§£æå¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥ï¼Œè·³è¿‡")
            
            # é¿å… API é™æµ
            await asyncio.sleep(1)
        
        # åˆ é™¤è¢«å‰”é™¤çš„æ•°æ®
        if rejected_items:
            logger.info(f"æ­£åœ¨åˆ é™¤ {len(rejected_items)} æ¡è¢«è¿‡æ»¤çš„æ–°é—»...")
            await self._delete_articles_from_db(rejected_items)
        
        logger.info("=" * 60)
        logger.info(f"è¿‡æ»¤å®Œæˆï¼šä¿ç•™ {len(filtered_items)}/{len(news_items)} æ¡æ–°é—»")
        logger.info(f"æ“ä½œåï¼šå…± {len(filtered_items)} æ¡æ–°é—»")
        if filtered_items:
            logger.info("æ“ä½œåä¿ç•™çš„æ ‡é¢˜åˆ—è¡¨ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for idx, item in enumerate(filtered_items[:10], 1):
                logger.info(f"  [{idx}] {item.title[:60]}... (æ¥æº: {item.source})")
        logger.info("=" * 60)
        return filtered_items
    
    async def step2_cluster(self, news_items: List[NewsItem], batch_size: int = 30) -> List[NewsItem]:
        """
        ç¬¬äºŒæ­¥ï¼šå½’ç±» (Clustering)
        å°†æè¿°åŒä¸€äº‹ä»¶çš„æ–°é—»èšåˆåœ¨ä¸€èµ·
        
        Args:
            news_items: è¿‡æ»¤åçš„æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å½’ç±»åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äºŒæ­¥ã€‘å¼€å§‹å½’ç±» (Clustering)...")
        logger.info(f"æ“ä½œå‰ï¼šå…± {len(news_items)} æ¡æ–°é—»")
        if news_items:
            logger.info("æ“ä½œå‰æ ‡é¢˜åˆ—è¡¨ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for idx, item in enumerate(news_items[:10], 1):
                logger.info(f"  [{idx}] {item.title[:60]}... (æ¥æº: {item.source})")
        logger.info(f"å¾…å¤„ç†æ–°é—»æ•°: {len(news_items)}")
        
        # åˆ†æ‰¹å¤„ç†
        all_events = {}  # event_id -> [NewsItem]
        
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source
                })
            
            # åŒ…å«ä¹‹å‰å·²è¯†åˆ«çš„äº‹ä»¶ä¿¡æ¯
            existing_events_info = ""
            if all_events:
                existing_events_info = "\nå·²è¯†åˆ«çš„äº‹ä»¶IDåˆ—è¡¨ï¼ˆä¾›å‚è€ƒï¼‰:\n"
                for event_id, items in all_events.items():
                    existing_events_info += f"- {event_id}: {items[0].title[:50]}...\n"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»äº‹ä»¶èšç±»ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œè¯­ä¹‰å½’ç±»ã€‚

**å½’ç±»æ ‡å‡†ï¼š**
- æŒ‰"åŒä¸€æŠ€æœ¯äº‹ä»¶ / æ¨¡å‹ç‰ˆæœ¬ / äº§å“å‘å¸ƒ / å…³é”®è®ºæ–‡"è¿›è¡Œè¯­ä¹‰å½’ç±»ã€‚
- ä¾‹å¦‚ï¼š"GPT-5 å‘å¸ƒ"ã€"Llama 3.1 å¼€æº"ã€"DeepMind æå‡ºæ–° AlphaFold ç®—æ³•"ç­‰å‡å±äºç‹¬ç«‹çš„è¯­ä¹‰äº‹ä»¶ã€‚
- å¦‚æœå¤šæ¡æ–°é—»è®¨è®ºçš„æ˜¯åŒä¸€ä¸ªäº‹ä»¶ï¼ˆå¦‚åŒä¸€ä¸ªæ¨¡å‹å‘å¸ƒã€åŒä¸€ç¯‡è®ºæ–‡ã€åŒä¸€ä¸ªæŠ€æœ¯çªç ´ï¼‰ï¼Œå®ƒä»¬åº”è¯¥è¢«å½’ä¸ºåŒä¸€ä¸ª event_idã€‚
- event_id åº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„è‹±æ–‡çŸ­è¯­ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼Œä¾‹å¦‚ï¼šgpt5_release_2025_q4, llama3_1_opensource

{existing_events_info}

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- event_id: äº‹ä»¶IDï¼ˆä½¿ç”¨æœ‰æ„ä¹‰çš„è‹±æ–‡çŸ­è¯­ï¼Œå¦‚æœä¸å·²è¯†åˆ«çš„äº‹ä»¶ç›¸åŒï¼Œè¯·ä½¿ç”¨ç›¸åŒçš„event_idï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "event_id": "gpt5_release"}},
  {{"article_id": "yyy", "event_id": "llama3_1_opensource"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    # æ›´æ–°æ–°é—»æ¡ç›®
                    result_map = {r["article_id"]: r for r in results}
                    batch_events = {}
                    for item in batch:
                        if item.article_id in result_map:
                            event_id = result_map[item.article_id].get("event_id")
                            item.event_id = event_id
                            
                            if event_id not in all_events:
                                all_events[event_id] = []
                            all_events[event_id].append(item)
                            
                            # è®°å½•æ‰¹æ¬¡å†…çš„äº‹ä»¶
                            if event_id not in batch_events:
                                batch_events[event_id] = []
                            batch_events[event_id].append(item)
                    
                    # è¾“å‡ºæ‰¹æ¬¡å½’ç±»ç»“æœ
                    logger.info(f"  æ‰¹æ¬¡å½’ç±»ç»“æœï¼šè¯†åˆ«å‡º {len(batch_events)} ä¸ªäº‹ä»¶")
                    for event_id, items in sorted(batch_events.items(), key=lambda x: len(x[1]), reverse=True):
                        logger.info(f"    - {event_id}: {len(items)} æ¡æ–°é—»")
                        for item in items[:3]:  # åªæ˜¾ç¤ºå‰3æ¡æ ‡é¢˜
                            logger.info(f"      * {item.title[:50]}...")
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è§£æå¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} å¤„ç†å¤±è´¥")
            
            await asyncio.sleep(1)
        
        # æ›´æ–°æ¯æ¡æ–°é—»çš„ event_count
        for event_id, items in all_events.items():
            count = len(items)
            for item in items:
                item.event_count = count
        
        logger.info("=" * 60)
        logger.info(f"å½’ç±»å®Œæˆï¼šè¯†åˆ«å‡º {len(all_events)} ä¸ªç‹¬ç«‹äº‹ä»¶")
        logger.info(f"æ“ä½œåï¼šå…± {len(news_items)} æ¡æ–°é—»ï¼Œå½’ç±»ä¸º {len(all_events)} ä¸ªäº‹ä»¶")
        for event_id, items in sorted(all_events.items(), key=lambda x: len(x[1]), reverse=True):
            logger.info(f"  - {event_id}: {len(items)} æ¡æ–°é—»")
            # æ˜¾ç¤ºè¯¥äº‹ä»¶ä¸‹çš„æ ‡é¢˜ç¤ºä¾‹
            for idx, item in enumerate(items[:3], 1):
                logger.info(f"    [{idx}] {item.title[:60]}...")
            if len(items) > 3:
                logger.info(f"    ... è¿˜æœ‰ {len(items) - 3} æ¡")
        logger.info("=" * 60)
        
        return news_items
    
    async def step3_deduplicate(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """
        ç¬¬ä¸‰æ­¥ï¼šå»é‡ (Deduplication)
        åœ¨æ¯ä¸ªäº‹ä»¶ä¸­ï¼Œä»…ä¿ç•™ä¸€æ¡æœ€æƒå¨ã€ä¿¡æ¯è´¨é‡æœ€é«˜çš„æ–°é—»
        
        Args:
            news_items: å½’ç±»åçš„æ–°é—»åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬ä¸‰æ­¥ã€‘å¼€å§‹å»é‡ (Deduplication)...")
        logger.info(f"æ“ä½œå‰ï¼šå…± {len(news_items)} æ¡æ–°é—»")
        
        # æŒ‰ event_id åˆ†ç»„
        events = defaultdict(list)
        for item in news_items:
            events[item.event_id].append(item)
        
        logger.info(f"å¾…å¤„ç†äº‹ä»¶æ•°: {len(events)}")
        
        deduplicated_items = []
        deleted_items = []
        
        for event_id, items in events.items():
            if len(items) == 1:
                # åªæœ‰ä¸€æ¡æ–°é—»ï¼Œç›´æ¥ä¿ç•™
                items[0].dedup_decision = "ä¿ç•™"
                items[0].dedup_reason = "å”¯ä¸€æ¥æº"
                deduplicated_items.append(items[0])
                continue
            
            logger.info(f"å¤„ç†äº‹ä»¶: {event_id} ({len(items)} æ¡æ–°é—»)")
            logger.info(f"  äº‹ä»¶å†…æ ‡é¢˜åˆ—è¡¨ï¼š")
            for idx, item in enumerate(items, 1):
                logger.info(f"    [{idx}] {item.title[:60]}... (æ¥æº: {item.source})")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in items:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source,
                    "url": item.url,
                    "publish_time": datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»å»é‡ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯æè¿°åŒä¸€äº‹ä»¶çš„å¤šæ¡æ–°é—»ï¼Œè¯·é€‰å‡ºæœ€æƒå¨ã€ä¿¡æ¯è´¨é‡æœ€é«˜çš„**æœ€å¤šä¸‰æ¡**ã€‚

**ä¿ç•™ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š**
1. å®˜æ–¹æ ¸å¿ƒä¿¡æº: å®˜ç½‘å‘å¸ƒã€å®˜æ–¹åšå®¢ã€arXiv è®ºæ–‡ã€GitHub Release
2. æ ¸å¿ƒäººå‘˜è§£è¯»: ä½œè€…ã€æ ¸å¿ƒå·¥ç¨‹å¸ˆæˆ–å®˜æ–¹ç ”ç©¶å‘˜çš„æ·±åº¦è§£è¯»
3. æƒå¨æŠ€æœ¯åª’ä½“: å¯¹ä¸Šè¿°ä¿¡æºçš„æ·±åº¦ã€å¿«é€Ÿè½¬è¿°æŠ¥é“
4. ç¤¾äº¤åª’ä½“/æ™®é€šè½¬è¿°: ä¼˜å…ˆçº§æœ€ä½

**äº‹ä»¶ID:** {event_id}

**æ–°é—»åˆ—è¡¨ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·é€‰æ‹©æœ€å¤šä¸‰æ¡æœ€æƒå¨çš„æ–°é—»ä¿ç•™ï¼ˆå¦‚æœåªæœ‰1-3æ¡ï¼Œå¯å…¨éƒ¨ä¿ç•™ï¼›å¦‚æœè¶…è¿‡3æ¡ï¼Œè¯·ç­›é€‰å‡ºæœ€å¥½çš„3æ¡ï¼‰ï¼Œå…¶ä½™æ ‡è®°ä¸ºåˆ é™¤ã€‚ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼š
- article_id: æ–‡ç« ID
- dedup_decision: "ä¿ç•™" æˆ– "åˆ é™¤"
- dedup_reason: åˆ¤æ–­ç†ç”±ï¼ˆä¸€å¥è¯ï¼‰

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "dedup_decision": "ä¿ç•™", "dedup_reason": "å®˜æ–¹åšå®¢é¦–å‘"}},
  {{"article_id": "yyy", "dedup_decision": "åˆ é™¤", "dedup_reason": "äºŒæ¬¡è½¬è¿°"}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    result_map = {r["article_id"]: r for r in results}
                    event_kept = []
                    event_deleted = []
                    for item in items:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.dedup_decision = r.get("dedup_decision")
                            item.dedup_reason = r.get("dedup_reason")
                            
                            if item.dedup_decision == "ä¿ç•™":
                                deduplicated_items.append(item)
                                event_kept.append(item)
                            else:
                                deleted_items.append(item)
                                event_deleted.append(item)
                    
                    # è¾“å‡ºäº‹ä»¶å»é‡ç»“æœ
                    logger.info(f"  å»é‡ç»“æœï¼šä¿ç•™ {len(event_kept)} æ¡ï¼Œåˆ é™¤ {len(event_deleted)} æ¡")
                    if event_kept:
                        logger.info(f"  ä¿ç•™çš„æ ‡é¢˜ï¼š")
                        for item in event_kept:
                            logger.info(f"    âœ“ {item.title[:60]}... (ç†ç”±: {item.dedup_reason})")
                    if event_deleted:
                        logger.info(f"  åˆ é™¤çš„æ ‡é¢˜ï¼š")
                        for item in event_deleted:
                            logger.info(f"    âœ— {item.title[:60]}... (ç†ç”±: {item.dedup_reason})")
                    break
                else:
                    logger.warning(f"äº‹ä»¶ {event_id} å»é‡å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        # å¤±è´¥æ—¶ä¿ç•™ç¬¬ä¸€æ¡
                        logger.error(f"äº‹ä»¶ {event_id} å»é‡å¤±è´¥ï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€æ¡")
                        items[0].dedup_decision = "ä¿ç•™"
                        items[0].dedup_reason = "å»é‡å¤±è´¥ï¼Œé»˜è®¤ä¿ç•™"
                        deduplicated_items.append(items[0])
                        # å…¶ä½™çš„æš‚æ—¶ä¸åŠ¨ï¼Œé¿å…è¯¯åˆ 
            
            await asyncio.sleep(0.5)
        
        # åˆ é™¤è¢«å»é‡çš„æ•°æ®
        if deleted_items:
            logger.info(f"æ­£åœ¨åˆ é™¤ {len(deleted_items)} æ¡é‡å¤æ–°é—»...")
            await self._delete_articles_from_db(deleted_items)
        
        logger.info("=" * 60)
        logger.info(f"å»é‡å®Œæˆï¼šä¿ç•™ {len(deduplicated_items)}/{len(news_items)} æ¡æ–°é—»")
        logger.info(f"æ“ä½œåï¼šå…± {len(deduplicated_items)} æ¡æ–°é—»")
        if deduplicated_items:
            logger.info("æ“ä½œåä¿ç•™çš„æ ‡é¢˜åˆ—è¡¨ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for idx, item in enumerate(deduplicated_items[:10], 1):
                logger.info(f"  [{idx}] {item.title[:60]}... (äº‹ä»¶: {item.event_id}, æ¥æº: {item.source})")
        logger.info("=" * 60)
        
        return deduplicated_items
    
    async def step4_rank(self, news_items: List[NewsItem], batch_size: int = 20) -> List[NewsItem]:
        """
        ç¬¬å››æ­¥ï¼šæ’åº (Ranking)
        å¯¹æœ€ç»ˆä¿ç•™çš„æ–°é—»æ¡ç›®è¿›è¡Œä»·å€¼åˆ¤æ–­
        
        Args:
            news_items: å»é‡åçš„æ–°é—»åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ’åºåçš„æ–°é—»åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬å››æ­¥ã€‘å¼€å§‹æ’åº (Ranking)...")
        logger.info(f"æ“ä½œå‰ï¼šå…± {len(news_items)} æ¡æ–°é—»")
        if news_items:
            logger.info("æ“ä½œå‰æ ‡é¢˜åˆ—è¡¨ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for idx, item in enumerate(news_items[:10], 1):
                logger.info(f"  [{idx}] {item.title[:60]}... (äº‹ä»¶: {item.event_id})")
        logger.info(f"å¾…è¯„åˆ†æ–°é—»æ•°: {len(news_items)}")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(news_items), batch_size):
            batch = news_items[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(news_items) - 1) // batch_size + 1}")
            
            # æ„å»ºæç¤ºè¯
            batch_data = []
            for item in batch:
                batch_data.append({
                    "article_id": item.article_id,
                    "title": item.title,
                    "description": item.description,
                    "source": item.source,
                    "event_count": item.event_count
                })
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯å½±å“åŠ›è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»è¿›è¡Œä»·å€¼è¯„åˆ†ã€‚

**è¯„åˆ†ç»´åº¦ï¼š**

1. **æŠ€æœ¯å½±å“åŠ› (tech_impact)** [1-5åˆ†]:
   - 5åˆ† (èŒƒå¼è½¬æ¢): æå‡ºå…¨æ–°æ¶æ„æˆ–ç†è®ºï¼Œå¯èƒ½æ”¹å˜ä¸€ä¸ªé¢†åŸŸçš„èµ°å‘ (å¦‚ Transformer)
   - 4åˆ† (é‡å¤§çªç ´): åœ¨å…³é”®èƒ½åŠ›ä¸Šæœ‰å·¨å¤§æå‡æˆ–å¼€æºäº†å¼ºå¤§çš„åŸºç¡€æ¨¡å‹
   - 3åˆ† (æ˜¾è‘—æ”¹è¿›): ç°æœ‰æ–¹æ³•ä¸Šçš„é‡è¦æ”¹è¿›ï¼Œæˆ–å‘å¸ƒäº†éå¸¸æœ‰ç”¨çš„å·¥å…·/æ¡†æ¶
   - 2åˆ† (å¸¸è§„ä¼˜åŒ–): æ€§èƒ½çš„å°å¹…æå‡æˆ–å¸¸è§„ç‰ˆæœ¬è¿­ä»£
   - 1åˆ† (å¾®å°æ”¹è¿›): å¢é‡å¼æ›´æ–°

2. **è¡Œä¸šå½±å“èŒƒå›´ (industry_scope)** [1-5åˆ†]:
   - 5åˆ† (å…¨è¡Œä¸š): å¯¹å‡ ä¹æ‰€æœ‰ AI åº”ç”¨å¼€å‘è€…å’Œå…¬å¸éƒ½äº§ç”Ÿå½±å“
   - 4åˆ† (å¤šé¢†åŸŸ): å½±å“å¤šä¸ªä¸»è¦ AI åº”ç”¨é¢†åŸŸ (å¦‚ NLP, CV)
   - 3åˆ† (ç‰¹å®šé¢†åŸŸ): æ·±åº¦å½±å“ä¸€ä¸ªå‚ç›´é¢†åŸŸ (å¦‚ AI for Science)
   - 2åˆ† (ç‰¹å®šä»»åŠ¡): ä¸»è¦å½±å“ä¸€ä¸ªæˆ–å°‘æ•°å‡ ä¸ªå…·ä½“ä»»åŠ¡
   - 1åˆ† (å°ä¼—åœºæ™¯): å½±å“èŒƒå›´éå¸¸æœ‰é™

3. **çƒ­åº¦ (hype_score)** [1-5åˆ†]:
   - æ ¹æ® event_count æ˜ å°„ï¼š
     * 1-2ç¯‡ â†’ 1åˆ†
     * 3-5ç¯‡ â†’ 2åˆ†
     * 6-10ç¯‡ â†’ 3åˆ†
     * 11-20ç¯‡ â†’ 4åˆ†
     * >20ç¯‡ â†’ 5åˆ†

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯æ¡æ–°é—»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- article_id: æ–‡ç« ID
- tech_impact: æŠ€æœ¯å½±å“åŠ›è¯„åˆ† (1-5)
- industry_scope: è¡Œä¸šå½±å“èŒƒå›´è¯„åˆ† (1-5)
- hype_score: çƒ­åº¦è¯„åˆ† (1-5ï¼Œæ ¹æ®event_countè®¡ç®—)

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```json
[
  {{"article_id": "xxx", "tech_impact": 5, "industry_scope": 5, "hype_score": 4}},
  {{"article_id": "yyy", "tech_impact": 3, "industry_scope": 3, "hype_score": 2}}
]
```

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚
"""
            
            # è°ƒç”¨ LLM
            for retry in range(self.max_retries):
                response = self._call_llm(prompt)
                results = self._parse_json_response(response)
                
                if results:
                    result_map = {r["article_id"]: r for r in results}
                    for item in batch:
                        if item.article_id in result_map:
                            r = result_map[item.article_id]
                            item.tech_impact = r.get("tech_impact", 2)
                            item.industry_scope = r.get("industry_scope", 2)
                            item.hype_score = r.get("hype_score", 1)
                            
                            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
                            # è°ƒæ•´æƒé‡ï¼šæé«˜æŠ€æœ¯å½±å“åŠ›å’Œè¡Œä¸šå½±å“åŠ›çš„æƒé‡ï¼Œé™ä½çƒ­åº¦æƒé‡ï¼ˆå› ä¸ºæ–°å‘å¸ƒçš„æ–°é—»çƒ­åº¦é€šå¸¸è¾ƒä½ï¼‰
                            item.final_score = (
                                item.tech_impact * 0.6 +      # åŸ 0.5
                                item.industry_scope * 0.3 +   # åŸ 0.3
                                item.hype_score * 0.1         # åŸ 0.2
                            )
                            
                            # è¯„çº§æ˜ å°„ - è°ƒæ•´é˜ˆå€¼
                            if item.final_score >= 4.0:       # åŸ 4.2
                                item.ranking_level = "S"
                            elif item.final_score >= 3.2:     # åŸ 3.5
                                item.ranking_level = "A"
                            elif item.final_score >= 2.4:     # åŸ 2.8
                                item.ranking_level = "B"
                            else:
                                item.ranking_level = "C"
                    
                    # è¾“å‡ºæ‰¹æ¬¡è¯„åˆ†ç»“æœ
                    logger.info(f"  æ‰¹æ¬¡è¯„åˆ†ç»“æœï¼š")
                    for item in sorted(batch, key=lambda x: x.final_score, reverse=True):
                        logger.info(f"    [{item.ranking_level}] {item.title[:50]}... "
                                  f"(è¯„åˆ†: {item.final_score:.2f}, "
                                  f"æŠ€æœ¯:{item.tech_impact}, è¡Œä¸š:{item.industry_scope}, çƒ­åº¦:{item.hype_score})")
                    break
                else:
                    logger.warning(f"æ‰¹æ¬¡ {i // batch_size + 1} è¯„åˆ†å¤±è´¥ï¼Œé‡è¯• {retry + 1}/{self.max_retries}")
                    if retry == self.max_retries - 1:
                        logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} è¯„åˆ†å¤±è´¥")
            
            await asyncio.sleep(1)
        
        # æŒ‰è¯„åˆ†æ’åº
        news_items.sort(key=lambda x: x.final_score, reverse=True)
        
        logger.info("=" * 60)
        logger.info(f"æ’åºå®Œæˆï¼š")
        logger.info(f"æ“ä½œåï¼šå…± {len(news_items)} æ¡æ–°é—»")
        logger.info(f"  Sçº§: {sum(1 for x in news_items if x.ranking_level == 'S')} æ¡")
        logger.info(f"  Açº§: {sum(1 for x in news_items if x.ranking_level == 'A')} æ¡")
        logger.info(f"  Bçº§: {sum(1 for x in news_items if x.ranking_level == 'B')} æ¡")
        logger.info(f"  Cçº§: {sum(1 for x in news_items if x.ranking_level == 'C')} æ¡")
        logger.info("æ“ä½œåæ’åºç»“æœï¼ˆå‰10æ¡ï¼‰ï¼š")
        for idx, item in enumerate(news_items[:10], 1):
            logger.info(f"  [{idx}] [{item.ranking_level}] {item.title[:60]}... "
                      f"(è¯„åˆ†: {item.final_score:.2f}, äº‹ä»¶: {item.event_id})")
        logger.info("=" * 60)
        
        return news_items

    def search_arxiv(self, query: str, max_results: int = 5) -> List[Dict[str, str]]:
        """
        æœç´¢ arXiv è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸² (ä¾‹å¦‚ "all:LLM")
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨ [{'title': ..., 'url': ..., 'summary': ...}]
        """
        base_url = "http://export.arxiv.org/api/query"
        
        # ç®€å•æ¸…ç† query
        query = query.replace('"', '%22')
        
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = {
            "search_query": query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "submittedDate",
            "sortOrder": "descending"
        }
        
        # æ‰‹åŠ¨æ‹¼æ¥ URL ä»¥ç¡®ä¿ encoded æ­£ç¡®ï¼Œæˆ–è€…ä½¿ç”¨ urllib.parse.quote ä½†ä¿ç•™ API ç‰¹æ®Šå­—ç¬¦
        # è¿™é‡Œä½¿ç”¨ urllib.parse.urlencode åº”è¯¥è¶³å¤Ÿå®‰å…¨
        try:
            query_string = urllib.parse.urlencode(params, safe=':')
            url = f"{base_url}?{query_string}"
            
            logger.info(f"æ­£åœ¨è°ƒç”¨ arXiv API: {url}")
            
            # éµå®ˆ API è§„åˆ™ï¼Œå¢åŠ å»¶æ—¶
            time.sleep(3)
            
            # è®¾ç½® User-Agent
            req = urllib.request.Request(
                url, 
                headers={'User-Agent': 'AIReportAgent/1.0 (mailto:your_email@example.com)'}
            )
            
            with urllib.request.urlopen(req, timeout=30) as response:
                xml_data = response.read()
                
            root = ET.fromstring(xml_data)
            # å¤„ç† namespace
            # Atom feed é€šå¸¸æœ‰é»˜è®¤ namespaceï¼ŒElementTree è§£ææ—¶éœ€è¦åœ¨ tag å‰åŠ  {uri}
            # è·å– root çš„ namespace
            ns_match = root.tag.split('}')[0] + '}' if '}' in root.tag else ''
            
            papers = []
            entries = root.findall(f'{ns_match}entry')
            
            for entry in entries:
                try:
                    title_elem = entry.find(f'{ns_match}title')
                    id_elem = entry.find(f'{ns_match}id')
                    summary_elem = entry.find(f'{ns_match}summary')
                    published_elem = entry.find(f'{ns_match}published')
                    
                    title = title_elem.text.strip().replace('\n', ' ') if title_elem is not None else "No Title"
                    link = id_elem.text.strip() if id_elem is not None else ""
                    summary = summary_elem.text.strip().replace('\n', ' ') if summary_elem is not None else ""
                    published = published_elem.text.strip() if published_elem is not None else ""
                    
                    # è½¬æ¢ article ID link åˆ° abstract page link
                    # arXiv API id é€šå¸¸æ˜¯ http://arxiv.org/abs/xxxx.xxxx
                    # æœ‰æ—¶æ˜¯ http://arxiv.org/api/xxxx
                    if link:
                        link = link.replace("/api/", "/abs/")
                    
                    papers.append({
                        "title": title,
                        "url": link,
                        "summary": summary[:200] + "...",
                        "published": published,
                        "source": "arXiv",
                        "type": "Paper"
                    })
                except Exception as e:
                    logger.warning(f"è§£æ arXiv entry å¤±è´¥: {e}")
                    continue
                
            return papers
            
        except Exception as e:
            logger.error(f"arXiv æœç´¢å¤±è´¥: {e}")
            return []

    async def step5_fetch_arxiv_papers(self, news_items: List[NewsItem]) -> List[Dict[str, str]]:
        """
        ç¬¬äº”æ­¥ï¼šè·å–ç›¸å…³ arXiv è®ºæ–‡
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            
        Returns:
            ç›¸å…³è®ºæ–‡åˆ—è¡¨
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äº”æ­¥ã€‘è·å–ç›¸å…³ arXiv è®ºæ–‡...")
        
        # é€‰å–æ‰€æœ‰ S/A çº§æ–°é—»ï¼Œç¡®ä¿è¦†ç›–é¢
        target_items = [item for item in news_items if item.ranking_level in ["S", "A"]]
        
        # å¦‚æœ S/A çº§å¤ªå°‘ï¼Œè¡¥å…… B çº§å‰å‡ å
        if len(target_items) < 5:
            b_items = [item for item in news_items if item.ranking_level == "B"]
            target_items.extend(b_items[:5 - len(target_items)])
            
        if not target_items:
            target_items = news_items[:5]
            
        if not target_items:
            return []
            
        # 1. æå–æœç´¢å…³é”®è¯
        titles = "\n".join([f"- {item.title}" for item in target_items])
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ AI é¢†åŸŸçš„çƒ­ç‚¹æ–°é—»æ ‡é¢˜ï¼Œæ„å»ºç”¨äº arXiv æœç´¢çš„æŸ¥è¯¢å…³é”®è¯åˆ—è¡¨ã€‚

æ–°é—»æ ‡é¢˜ï¼š
{titles}

è¦æ±‚ï¼š
1. åˆ†ææ¯æ¡æ–°é—»çš„æ ¸å¿ƒæŠ€æœ¯å®ä½“ï¼ˆå¦‚æ¨¡å‹åç§° "Gemini 3", "Claude Sonnet" æˆ–æŠ€æœ¯æœ¯è¯­ "World Model", "Scaling Law"ï¼‰ã€‚
2. æ„å»º 5-8 ä¸ªç‹¬ç«‹çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œæ—¨åœ¨å°½å¯èƒ½è¦†ç›–è¿™äº›æ–°é—»å¯¹åº”çš„ä¸»é¢˜ã€‚
3. æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œä½¿ç”¨ `all:` æˆ– `ti:` å‰ç¼€ã€‚
4. ç¤ºä¾‹ï¼š
all:"Gemini 3"
ti:"World Model" AND all:Genie
all:"Large Language Model" AND all:Reasoning

è¯·ç›´æ¥è¿”å›æŸ¥è¯¢å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªã€‚
"""
        response = self._call_llm(prompt)
        if not response:
            queries = ["all:Artificial Intelligence"]
        else:
            queries = [line.strip().strip('`').strip('"') for line in response.split('\n') if line.strip() and not line.strip().startswith('```')]
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
        queries = queries[:8]
        logger.info(f"ç”Ÿæˆçš„ arXiv æŸ¥è¯¢: {queries}")
        
        all_papers = []
        seen_ids = set()
        
        # 2. æ‰§è¡Œæœç´¢ (ä¸²è¡Œæ‰§è¡Œä»¥éµå®ˆé™æµ)
        for query in queries:
            if not query:
                continue
            # æ¸…ç† query
            if "search_query=" in query:
                query = query.replace("search_query=", "")
                
            # æ¯ä¸ª query å– 5 æ¡ï¼Œæ€»å…±å¯èƒ½è·å– 25-40 æ¡
            papers = self.search_arxiv(query, max_results=5) 
            
            for paper in papers:
                # ä½¿ç”¨ URL ä½œä¸ºå»é‡é”®
                if paper['url'] not in seen_ids:
                    all_papers.append(paper)
                    seen_ids.add(paper['url'])
            
        logger.info(f"å…±è·å–åˆ° {len(all_papers)} ç¯‡ä¸é‡å¤çš„ arXiv è®ºæ–‡")
        return all_papers
    
    def _validate_news_item_format(self, content: str) -> Tuple[bool, str]:
        """éªŒè¯æ–°é—»æ¡ç›®çš„ Markdown æ ¼å¼"""
        required_patterns = [
            (r"### \*\*.*?\*\*", "æ ‡é¢˜æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º ### **æ ‡é¢˜**"),
            # (r"\[é˜…è¯»åŸæ–‡\]\(.*?\)", "ç¼ºå°‘é˜…è¯»åŸæ–‡é“¾æ¥æˆ–æ ¼å¼é”™è¯¯"), # é˜…è¯»åŸæ–‡ç°åœ¨æ˜¯å¯é€‰çš„ï¼ˆé‡å­ä½é“¾æ¥ä¸æ˜¾ç¤ºï¼‰
            (r"> \*\*æ¦‚è¦\*\*:.*", "ç¼ºå°‘æ¦‚è¦æˆ–æ ¼å¼é”™è¯¯"),
            (r"\*\*ğŸ’¡å†…å®¹è¯¦è§£\*\*", "ç¼ºå°‘'ğŸ’¡å†…å®¹è¯¦è§£'åˆ†èŠ‚"),
            (r"- \*\*.*?\*\*", "ç¼ºå°‘è¦ç‚¹æ ‡é¢˜æˆ–æ ¼å¼é”™è¯¯")
        ]
        
        import re
        for pattern, error_msg in required_patterns:
            if not re.search(pattern, content, re.MULTILINE):
                return False, error_msg
        return True, ""

    def _get_primary_source_url(self, item: NewsItem) -> str:
        """
        è·å–ä¼˜å…ˆçº§æœ€é«˜çš„ä¿¡æº URL
        
        ä¼˜å…ˆçº§é¡ºåºï¼š
        1. å®˜æ–¹æ ¸å¿ƒä¿¡æº (official, blog, github ç­‰)
        2. arXiv è®ºæ–‡
        3. æƒå¨æŠ€æœ¯åª’ä½“
        4. åŸå§‹æ–°é—» URL (å…œåº•)
        
        Args:
            item: æ–°é—»æ¡ç›®
            
        Returns:
            ä¼˜å…ˆçº§æœ€é«˜çš„ URL
        """
        # å¦‚æœæ–°é—»æœ¬èº«æ¥è‡ªå®˜æ–¹æºï¼ˆéé‡å­ä½ç­‰äºŒæ‰‹åª’ä½“ï¼‰ï¼Œç›´æ¥è¿”å›
        non_official_sources = ["é‡å­ä½", "qbitai", "36kr", "æ–°æ™ºå…ƒ", "aibase"]
        if item.source and not any(s.lower() in item.source.lower() for s in non_official_sources):
            return item.url
        
        # å°è¯•ä» reference_links ä¸­é€‰æ‹©æœ€ä½³é“¾æ¥
        if not item.reference_links:
            return item.url
            
        try:
            refs = json.loads(item.reference_links)
            if not refs:
                return item.url
            
            # å®šä¹‰ä¿¡æºä¼˜å…ˆçº§
            priority_order = [
                ("official", 100),      # å®˜æ–¹å‘å¸ƒ
                ("blog", 90),           # å®˜æ–¹åšå®¢
                ("github", 85),         # GitHub Release
                ("arxiv", 80),          # arXiv è®ºæ–‡
                ("paper", 75),          # è®ºæ–‡
                ("announcement", 70),   # å…¬å‘Š
                ("external", 50),       # å¤–éƒ¨é“¾æ¥
                ("social", 20),         # ç¤¾äº¤åª’ä½“
            ]
            
            # æ ¹æ® URL ç‰¹å¾å’Œ type å­—æ®µåˆ¤æ–­ä¼˜å…ˆçº§
            def get_priority(ref: dict) -> int:
                url = ref.get("url", "").lower()
                ref_type = ref.get("type", "").lower()
                
                # æ ¹æ® URL åŸŸååˆ¤æ–­
                if any(domain in url for domain in ["openai.com", "blog.google", "ai.meta.com", "anthropic.com", "deepmind.google"]):
                    return 100  # å®˜æ–¹æ ¸å¿ƒåŸŸåæœ€é«˜ä¼˜å…ˆçº§
                if "arxiv.org" in url:
                    return 80
                if "github.com" in url:
                    return 85
                    
                # æ ¹æ® type å­—æ®µåˆ¤æ–­
                for ptype, score in priority_order:
                    if ptype in ref_type:
                        return score
                        
                return 30  # é»˜è®¤ä½ä¼˜å…ˆçº§
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶è¿”å›æœ€é«˜çš„
            sorted_refs = sorted(refs, key=get_priority, reverse=True)
            best_ref = sorted_refs[0]
            
            # åªæœ‰å½“æœ€ä½³é“¾æ¥ä¼˜å…ˆçº§é«˜äºé»˜è®¤æ—¶æ‰ä½¿ç”¨
            if get_priority(best_ref) >= 50:
                return best_ref.get("url", item.url)
                
        except Exception as e:
            logger.warning(f"è§£æ reference_links å¤±è´¥: {e}")
        
        return item.url

    def _strict_format_check_agent(self, content: str) -> Tuple[bool, str]:
        """
        ä½¿ç”¨ LLM è¿›è¡Œä¸¥æ ¼çš„æ ¼å¼æ£€æŸ¥
        """
        check_prompt = f"""è¯·ä¸¥æ ¼æ£€æŸ¥ä»¥ä¸‹ Markdown å†…å®¹æ˜¯å¦å®Œå…¨ç¬¦åˆæŒ‡å®šçš„æ ¼å¼æ¨¡æ¿ã€‚

**å¾…æ£€æŸ¥å†…å®¹ï¼š**
```markdown
{content}
```

**æ ‡å‡†æ¨¡æ¿æ ¼å¼ï¼š**
```markdown
### **[æ ‡é¢˜]**

[é˜…è¯»åŸæ–‡]([URL])  `[YYYY-MM-DD]`

> **æ¦‚è¦**: [å†…å®¹]

**ğŸ’¡å†…å®¹è¯¦è§£**

- **[å…³é”®ç‚¹å¤§æ ‡é¢˜]**
    - **[å…³é”®ç‚¹è§£é‡Š]**
    ...

[ç›¸å…³è®ºæ–‡]([URL])
```

**æ£€æŸ¥è§„åˆ™ï¼š**
1. **æ ‡é¢˜**ï¼šå¿…é¡»ä»¥ `### **` å¼€å¤´ï¼Œ`**` ç»“å°¾ã€‚
2. **é˜…è¯»åŸæ–‡**ï¼š
   - æ­¤è¡Œæ˜¯**å¯é€‰çš„**ã€‚
   - å¦‚æœå­˜åœ¨ï¼Œå¿…é¡»æ˜¯ `[é˜…è¯»åŸæ–‡](URL)  `[YYYY-MM-DD]` ` æ ¼å¼ã€‚
   - å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ç›´æ¥å¼€å§‹æ¦‚è¦ã€‚
   - **æ³¨æ„**ï¼šä¸èƒ½åªå‡ºç°æ—¥æœŸè€Œæ²¡æœ‰ `[é˜…è¯»åŸæ–‡](URL)`ï¼ŒURLä¸è¦å‡ºç°é‡å­ä½ã€36krã€qq.com ç­‰éå®˜æ–¹æºã€‚
3. **æ¦‚è¦**ï¼šå¿…é¡»ä»¥ `> **æ¦‚è¦**:` å¼€å¤´ã€‚
4. **å†…å®¹è¯¦è§£**ï¼šå¿…é¡»åŒ…å« `**ğŸ’¡å†…å®¹è¯¦è§£**` æ ‡é¢˜ã€‚
5. **å…³é”®ç‚¹**ï¼šå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå…³é”®ç‚¹å¤§æ ‡é¢˜ï¼ˆ`- **...**`ï¼‰å’Œè§£é‡Šã€‚
6. **ç›¸å…³è®ºæ–‡**ï¼š
   - å¦‚æœæœ‰è®ºæ–‡é“¾æ¥ï¼Œå¿…é¡»æ˜¯ `[ç›¸å…³è®ºæ–‡](URL)` æ ¼å¼ã€‚
   - å¦‚æœæ²¡æœ‰è®ºæ–‡é“¾æ¥ï¼Œ**ç»å¯¹ä¸èƒ½**å‡ºç° `[ç›¸å…³è®ºæ–‡]` å­—æ ·æˆ–ç©ºè¡Œã€‚
7. **çº¯å‡€åº¦**ï¼šä¸åº”åŒ…å« "Here is the report" æˆ–å…¶ä»–èŠå¤©å†…å®¹ã€‚

**è¾“å‡ºè¦æ±‚ï¼š**
- å¦‚æœæ ¼å¼å®Œå…¨æ­£ç¡®ï¼Œè¯·åªè¾“å‡º "PASS"ã€‚
- å¦‚æœæœ‰é”™è¯¯ï¼Œè¯·è¾“å‡º "FAIL: [å…·ä½“é”™è¯¯åŸå› ]"ã€‚
"""
        try:
            response = self._call_llm(check_prompt, temperature=0.0)
            if "PASS" in response:
                return True, ""
            else:
                return False, response.replace("FAIL:", "").strip()
        except Exception as e:
            logger.error(f"æ ¼å¼æ£€æŸ¥ Agent è°ƒç”¨å¤±è´¥: {e}")
            return True, ""

    async def _generate_event_entries_batch(self, batch_events: List[Dict], candidate_papers: List[Dict] = None, custom_instructions: str = "") -> List[Dict[str, str]]:
        """
        æŒ‰äº‹ä»¶ç”ŸæˆæŠ¥å‘Šæ¡ç›®ï¼ˆæ¯ä¸ªäº‹ä»¶ç»¼åˆå…¶ä¸‹æ‰€æœ‰æ–°é—»ï¼‰
        
        Args:
            batch_events: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å« {"event_id", "best_item", "all_items", "event_score"}
            candidate_papers: å€™é€‰ arXiv è®ºæ–‡åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ¡ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {"event_id", "category", "markdown_content"}
        """
        batch_data = []
        for event in batch_events:
            event_id = event["event_id"]
            all_items = event["all_items"]  # è¯¥äº‹ä»¶ä¸‹çš„æ‰€æœ‰æ–°é—»ï¼ˆå»é‡åä¿ç•™çš„ï¼Œæœ€å¤š3æ¡ï¼‰
            best_item = event["best_item"]
            
            # è·å–ä¼˜å…ˆçº§æœ€é«˜çš„å®˜æ–¹ä¿¡æº URLï¼ˆä»æœ€ä½³æ–°é—»ä¸­è·å–ï¼‰
            primary_url = self._get_primary_source_url(best_item)
            
            # ç»¼åˆæ‰€æœ‰æ–°é—»çš„å†…å®¹
            combined_content = ""
            sources_info = []
            combined_refs = []  # æ”¶é›†æ‰€æœ‰æ–°é—»çš„å‚è€ƒé“¾æ¥
            
            for idx, item in enumerate(all_items, 1):
                item_url = self._get_primary_source_url(item)
                sources_info.append({
                    "source": item.source,
                    "url": item_url,
                    "title": item.title
                })
                combined_content += f"\n--- æ¥æº {idx}: {item.source} ---\n"
                combined_content += f"æ ‡é¢˜: {item.title}\n"
                combined_content += f"å†…å®¹: {item.content}\n"
                
                # æ”¶é›†å‚è€ƒé“¾æ¥
                if item.reference_links:
                    try:
                        refs = json.loads(item.reference_links)
                        if refs:
                            combined_refs.extend(refs)
                    except:
                        pass
            
            pub_date = datetime.fromtimestamp(best_item.publish_time).strftime('%Y-%m-%d')
            
            batch_data.append({
                "event_id": event_id,
                "primary_title": best_item.title,  # ä½¿ç”¨æœ€é«˜åˆ†æ–°é—»çš„æ ‡é¢˜ä½œä¸ºä¸»æ ‡é¢˜
                "primary_url": primary_url,  # ä½¿ç”¨ä¼˜å…ˆçº§æœ€é«˜çš„å®˜æ–¹ä¿¡æº
                "primary_source": best_item.source,
                "publish_time": pub_date,
                "news_count": len(all_items),
                "combined_content": combined_content,  # ç»¼åˆæ‰€æœ‰æ–°é—»çš„å†…å®¹
                "all_sources": sources_info,  # æ‰€æœ‰æ¥æºä¿¡æ¯
                "reference_links": combined_refs  # ä¼ é€’å‚è€ƒé“¾æ¥ç»™ LLM
            })

        # æ„å»ºå€™é€‰è®ºæ–‡ä¸Šä¸‹æ–‡
        papers_context = ""
        if candidate_papers:
            papers_list = []
            for p in candidate_papers:
                papers_list.append(f"- Title: {p.get('title')}\n  URL: {p.get('url')}")
            papers_context = "\n".join(papers_list)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯åˆ†æå¸ˆã€‚è¯·ä¸ºä»¥ä¸‹**äº‹ä»¶**ç”Ÿæˆç¬¦åˆæŠ¥å‘Šæ ¼å¼çš„Markdownå†…å®¹å—ã€‚

**é‡è¦è¯´æ˜ï¼š**
{custom_instructions if custom_instructions else ""}
- æ¯ä¸ªäº‹ä»¶å¯èƒ½åŒ…å«å¤šæ¡æ¥è‡ªä¸åŒæ¥æºçš„æ–°é—»æŠ¥é“
- è¯·ç»¼åˆæ‰€æœ‰æ¥æºçš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€ä¸é‡å¤çš„äº‹ä»¶æŠ¥å‘Š
- ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æ¥æºçš„ä¿¡æ¯ï¼Œè¾…ä»¥å…¶ä»–æ¥æºçš„è¡¥å……ç»†èŠ‚

**å€™é€‰ arXiv è®ºæ–‡åº“ï¼š**
{papers_context if papers_context else "(æ— å€™é€‰è®ºæ–‡)"}

**è¾“å‡ºè¦æ±‚ï¼š**
å¯¹äºæ¯ä¸€ä¸ªäº‹ä»¶ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. **åˆ†ç±»**ï¼šå°†å…¶å½’å…¥ä»¥ä¸‹ä¸‰ç±»ä¹‹ä¸€ï¼š
   - "Infrastructure" (AI Infra):
     - ç®—åŠ›åŸºç¡€è®¾æ–½ï¼šGPUç›¸å…³ (Nvidia, Moore Threads, Kunlunxin, Pingtouge, Ascend, Hygon)
     - æ•°æ®ä¸AIä¸­å°å±‚ï¼šäº‘å‚å•†äº§å“ (AWS, GCP, Aliyun, Volcano, Tencent, Huawei)
     - ç»Ÿä¸€ç®—åŠ›ç®¡ç†ä¸è°ƒåº¦
   - "Model" (AI Model Progress):
     - å…¨çƒåŸºç¡€å¤§æ¨¡å‹å›¾è°± (GPT, Gemini, Grok, Claude, DS, Qwen, Kimi, GLM, Wenxin, Longcat, Kelingç­‰)
     - å¤§æ¨¡å‹æœ€æ–°å‘å¸ƒã€è®­ç»ƒæŠ€æœ¯ (Pretrain, Post-pretrain, SFT, RLHF)ã€æ¨ç†æŠ€æœ¯
     - æ•°æ®æ„å»ºæŠ€æœ¯ (æ¸…æ´—, å¢å¼º, åˆæˆ)
     - æ™ºèƒ½ä½“æ„å»ºæŠ€æœ¯ (æ¡†æ¶: LangChain, CrewAI, AutoGen...; äº§å“: Bedrock Agent, Dify, Coze...)
   - "Application" (AI Agent & Application):
     - å¤§æ¨¡å‹æ³›åº”ç”¨ (B/Cç«¯æ™ºèƒ½ä½“, å·¥å…·ç±»äº§å“)
     - æ¸¸æˆè¡Œä¸šåº”ç”¨

2. **ç”ŸæˆMarkdownå†…å®¹**ï¼šä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownæ ¼å¼æ¨¡æ¿ç”Ÿæˆå†…å®¹ã€‚
   
   **æ¨¡æ¿æ ¼å¼ï¼š**
   ```markdown
   ### **[äº‹ä»¶æ ‡é¢˜ - åŸºäºä¸»æ ‡é¢˜ä¼˜åŒ–ï¼ˆå°½é‡ä¸è¶…è¿‡8ä¸ªå­—ï¼‰]**
   
   [é˜…è¯»åŸæ–‡]([primary_url])  `[Publish_Time]`
   
   > **æ¦‚è¦**: [ç»¼åˆå¤šä¸ªæ¥æºï¼Œç”¨3-4å¥è¯ç®€ç»ƒæ¦‚æ‹¬æ ¸å¿ƒäº‹ä»¶]
   
   **ğŸ’¡å†…å®¹è¯¦è§£**
   (ç»¼åˆæ‰€æœ‰æ¥æºçš„ä¿¡æ¯ï¼Œæç‚¼å…³é”®æŠ€æœ¯ç‚¹ï¼Œå…³é”®ç‚¹æ•°é‡è‡³å°‘å¤§äº3ç‚¹)

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 1**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 2**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 3**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰   
        â€¦â€¦
    â€¦â€¦

    [ç›¸å…³è®ºæ–‡]([URL])
   ```

   **å…³äº [é˜…è¯»åŸæ–‡] çš„ç‰¹åˆ«è¯´æ˜ï¼š**
   - å¿…é¡»ä½¿ç”¨æä¾›çš„ primary_urlï¼Œè¿™æ˜¯ä¼˜å…ˆçº§æœ€é«˜çš„å®˜æ–¹æ ¸å¿ƒä¿¡æº
   - **é‡è¦ï¼š** å¦‚æœ primary_url åŒ…å« "qbitai.com"ã€"qq.com"ã€"é‡å­ä½" æˆ– "36kr"ï¼Œè¯·**ä¸è¦ç”Ÿæˆ** [é˜…è¯»åŸæ–‡] è¿™ä¸€è¡Œï¼ˆåŒ…æ‹¬æ—¥æœŸï¼‰ï¼Œç›´æ¥å¼€å§‹å¼•ç”¨å— (> **æ¦‚è¦**...)
   - **é‡è¦ï¼š** å¦‚æœ primary_url æ˜¯è®ºæ–‡é“¾æ¥ï¼ˆå¦‚åŒ…å« "arxiv.org", "openreview.net", "huggingface.co/papers"ï¼‰ï¼Œè¯·**ä¸è¦ç”Ÿæˆ** [é˜…è¯»åŸæ–‡] è¿™ä¸€è¡Œï¼ˆåŒ…æ‹¬æ—¥æœŸï¼‰ï¼Œç¡®ä¿è¯¥é“¾æ¥å‡ºç°åœ¨ [ç›¸å…³è®ºæ–‡] ä¸­ã€‚
   - ç¦æ­¢ä½¿ç”¨é‡å­ä½ã€36krç­‰äºŒæ‰‹åª’ä½“é“¾æ¥

   **å…³äº [ç›¸å…³è®ºæ–‡] çš„ç‰¹åˆ«è¯´æ˜ï¼š**
   - è¯·åœ¨"å€™é€‰ arXiv è®ºæ–‡åº“"ä¸­æŸ¥æ‰¾ä¸å½“å‰äº‹ä»¶**é«˜åº¦ç›¸å…³**çš„è®ºæ–‡
   - **æˆ–è€…**ï¼Œå¦‚æœæä¾›çš„äº‹ä»¶å†…å®¹ï¼ˆæ–°é—»åŸæ–‡ï¼‰ä¸­æ˜ç¡®åŒ…å«äº†ç›¸å…³è®ºæ–‡çš„é“¾æ¥ï¼ˆå¦‚ arXiv é“¾æ¥ï¼‰ï¼Œè¯·ç›´æ¥ä½¿ç”¨è¯¥é“¾æ¥
   - **æˆ–è€…**ï¼Œè¯·æ£€æŸ¥æä¾›çš„ `reference_links` å­—æ®µï¼Œå¦‚æœå…¶ä¸­åŒ…å«è®ºæ–‡ç±»å‹çš„é“¾æ¥ï¼ˆå¦‚ type="paper" æˆ– "arxiv"ï¼‰ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨
   - å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡ï¼Œè¯·å°† `[ç›¸å…³è®ºæ–‡]([URL])` æ›¿æ¢ä¸ºå®é™…çš„è®ºæ–‡é“¾æ¥
   - **å¦‚æœæ²¡æœ‰æ‰¾åˆ°é«˜åº¦ç›¸å…³çš„è®ºæ–‡ï¼Œè¯·åŠ¡å¿…åˆ é™¤è¿™ä¸€è¡Œ**

**äº‹ä»¶æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¿”å›æ ¼å¼ï¼š**
è¯·è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«æ¯ä¸ªäº‹ä»¶çš„ç”Ÿæˆç»“æœï¼š
```json
[
  {{
    "event_id": "xxx",
    "category": "Infrastructure", 
    "markdown_content": "### **æ ‡é¢˜**..."
  }},
  ...
]
```
è¯·åªè¿”å› JSONã€‚
"""
        
        for retry in range(self.max_retries):
            response = self._call_llm(prompt, temperature=0.3)
            results = self._parse_json_response(response)
            if results:
                # éªŒè¯æ ¼å¼
                valid_results = []
                errors = []
                for item in results:
                    # 1. åŸºç¡€æ­£åˆ™æ£€æŸ¥
                    is_valid, error = self._validate_news_item_format(item.get("markdown_content", ""))
                    if not is_valid:
                        errors.append(f"äº‹ä»¶ '{item.get('event_id', 'Unknown')}' åŸºç¡€æ ¼å¼é”™è¯¯: {error}")
                        continue
                    
                    # 2. LLM ä¸¥æ ¼æ£€æŸ¥
                    is_strict_valid, strict_error = self._strict_format_check_agent(item.get("markdown_content", ""))
                    if is_strict_valid:
                        valid_results.append(item)
                    else:
                        errors.append(f"äº‹ä»¶ '{item.get('event_id', 'Unknown')}' ä¸¥æ ¼æ ¼å¼é”™è¯¯: {strict_error}")
                
                if not errors:
                    return valid_results
                
                # å¦‚æœæœ‰é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œå°†é”™è¯¯åŠ å…¥ prompt é‡è¯•
                logger.warning(f"æ‰¹æ¬¡ç”Ÿæˆå­˜åœ¨æ ¼å¼é”™è¯¯: {'; '.join(errors)}")
                if retry < self.max_retries - 1:
                    prompt += f"\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆå­˜åœ¨ä»¥ä¸‹æ ¼å¼é”™è¯¯ï¼Œè¯·ä¸¥æ ¼ä¿®æ­£ï¼š\n" + "\n".join(errors)
                    continue
                else:
                    return valid_results
            
            await asyncio.sleep(1)
            
        return []

    async def _generate_news_entries_batch(self, batch_items: List[NewsItem], candidate_papers: List[Dict] = None) -> List[Dict[str, str]]:
        """
        åˆ†æ‰¹ç”Ÿæˆæ–°é—»æ¡ç›®å†…å®¹ (å¹¶å‘å¤„ç†)
        
        Args:
            batch_items: è¿™ä¸€æ‰¹çš„æ–°é—»åˆ—è¡¨
            candidate_papers: å€™é€‰ arXiv è®ºæ–‡åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ¡ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {"article_id", "category", "markdown_content"}
        """
        batch_data = []
        for item in batch_items:
            pub_date = datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d')
            
            # è·å–ä¼˜å…ˆçº§æœ€é«˜çš„å®˜æ–¹ä¿¡æº URL
            primary_url = self._get_primary_source_url(item)
            
            # è§£æå‚è€ƒé“¾æ¥
            refs = []
            if item.reference_links:
                try:
                    refs = json.loads(item.reference_links)
                except:
                    pass

            batch_data.append({
                "article_id": item.article_id,
                "title": item.title,
                "source": item.source,
                "url": primary_url,  # ä½¿ç”¨ä¼˜å…ˆçº§æœ€é«˜çš„å®˜æ–¹ä¿¡æº
                "publish_time": pub_date,
                "content": item.content,  # ä½¿ç”¨å®Œæ•´å†…å®¹è¿›è¡Œæ·±åº¦é˜…è¯»
                "reference_links": refs  # ä¼ é€’å‚è€ƒé“¾æ¥ç»™ LLM
            })

        # æ„å»ºå€™é€‰è®ºæ–‡ä¸Šä¸‹æ–‡
        papers_context = ""
        if candidate_papers:
            papers_list = []
            for p in candidate_papers:
                papers_list.append(f"- Title: {p.get('title')}\n  URL: {p.get('url')}")
            papers_context = "\n".join(papers_list)

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæŠ€æœ¯åˆ†æå¸ˆã€‚è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆç¬¦åˆæŠ¥å‘Šæ ¼å¼çš„Markdownå†…å®¹å—ã€‚

**å€™é€‰ arXiv è®ºæ–‡åº“ï¼š**
{papers_context if papers_context else "(æ— å€™é€‰è®ºæ–‡)"}

**è¾“å‡ºè¦æ±‚ï¼š**
å¯¹äºæ¯ä¸€æ¡æ–°é—»ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. **åˆ†ç±»**ï¼šå°†å…¶å½’å…¥ä»¥ä¸‹ä¸‰ç±»ä¹‹ä¸€ï¼š
   - "Infrastructure" (AI Infra):
     - ç®—åŠ›åŸºç¡€è®¾æ–½ï¼šGPUç›¸å…³ (Nvidia, Moore Threads, Kunlunxin, Pingtouge, Ascend, Hygon)
     - æ•°æ®ä¸AIä¸­å°å±‚ï¼šäº‘å‚å•†äº§å“ (AWS, GCP, Aliyun, Volcano, Tencent, Huawei)
     - ç»Ÿä¸€ç®—åŠ›ç®¡ç†ä¸è°ƒåº¦
   - "Model" (AI Model Progress):
     - å…¨çƒåŸºç¡€å¤§æ¨¡å‹å›¾è°± (GPT, Gemini, Grok, Claude, DS, Qwen, Kimi, GLM, Wenxin, Longcat, Kelingç­‰)
     - å¤§æ¨¡å‹æœ€æ–°å‘å¸ƒã€è®­ç»ƒæŠ€æœ¯ (Pretrain, Post-pretrain, SFT, RLHF)ã€æ¨ç†æŠ€æœ¯
     - æ•°æ®æ„å»ºæŠ€æœ¯ (æ¸…æ´—, å¢å¼º, åˆæˆ)
     - æ™ºèƒ½ä½“æ„å»ºæŠ€æœ¯ (æ¡†æ¶: LangChain, CrewAI, AutoGen...; äº§å“: Bedrock Agent, Dify, Coze...)
   - "Application" (AI Agent & Application):
     - å¤§æ¨¡å‹æ³›åº”ç”¨ (B/Cç«¯æ™ºèƒ½ä½“, å·¥å…·ç±»äº§å“)
     - æ¸¸æˆè¡Œä¸šåº”ç”¨

2. **ç”ŸæˆMarkdownå†…å®¹**ï¼šä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownæ ¼å¼æ¨¡æ¿ç”Ÿæˆå†…å®¹ã€‚
   
   **æ¨¡æ¿æ ¼å¼ï¼š**
   ```markdown
   ### **[æ–°é—»æ ‡é¢˜]**
   
   [é˜…è¯»åŸæ–‡]([URL])  `[Publish_Time]`
   
   > **æ¦‚è¦**: [3-4å¥è¯ç®€ç»ƒæ¦‚æ‹¬æ ¸å¿ƒäº‹ä»¶]
   
   **ğŸ’¡å†…å®¹è¯¦è§£**
   (å†…å®¹è¯¦è§£æ˜¯å¯¹å…³é”®æŠ€æœ¯çš„ç½—åˆ—ï¼Œå…³é”®ç‚¹æ•°é‡è‡³å°‘å¤§äº3ç‚¹ï¼Œè¯·å¯¹å…³é”®æŠ€æœ¯è¿›è¡Œè¯¦ç»†è§£è¯»ï¼Œæ­¤å¤„ä¸ç”¨æ·»åŠ æ¦‚è¿°)

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 1**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 2**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        â€¦â€¦

    - **å…³é”®ç‚¹å¤§æ ‡é¢˜ 3**
    ï¼ˆéœ€è¦è¯¦ç»†å¯¹å…³é”®ç‚¹è¿›è¡Œè§£é‡Šï¼Œå…³é”®ç‚¹è§£é‡Šçš„æ•°é‡æ ¹æ®è¦ç‚¹åŠ¨æ€è°ƒæ•´ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š1**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰
        - **å…³é”®ç‚¹è§£é‡Š2**
        ï¼ˆå¦èµ·ä¸€æ®µè¯¦ç»†è§£é‡Šè¯¥æŠ€æœ¯ï¼Œè¦æœ‰å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¶…è¿‡200å­—ï¼‰   
        â€¦â€¦
    â€¦â€¦

    [ç›¸å…³è®ºæ–‡]([URL])
   ```

   **å…³äº [é˜…è¯»åŸæ–‡] çš„ç‰¹åˆ«è¯´æ˜ï¼š**
   - å¿…é¡»ä½¿ç”¨æä¾›çš„ url
   - **é‡è¦ï¼š** å¦‚æœ url åŒ…å« "qbitai.com"ã€"é‡å­ä½" æˆ– "36kr"ï¼Œè¯·**ä¸è¦ç”Ÿæˆ** [é˜…è¯»åŸæ–‡] è¿™ä¸€è¡Œï¼ˆåŒ…æ‹¬æ—¥æœŸï¼‰ï¼Œç›´æ¥å¼€å§‹å¼•ç”¨å— (> **æ¦‚è¦**...)
   - **é‡è¦ï¼š** å¦‚æœ url æ˜¯è®ºæ–‡é“¾æ¥ï¼ˆå¦‚åŒ…å« "arxiv.org", "openreview.net", "huggingface.co/papers"ï¼‰ï¼Œè¯·**ä¸è¦ç”Ÿæˆ** [é˜…è¯»åŸæ–‡] è¿™ä¸€è¡Œï¼ˆåŒ…æ‹¬æ—¥æœŸï¼‰ï¼Œç¡®ä¿è¯¥é“¾æ¥å‡ºç°åœ¨ [ç›¸å…³è®ºæ–‡] ä¸­ã€‚

   **å…³äº [ç›¸å…³è®ºæ–‡] çš„ç‰¹åˆ«è¯´æ˜ï¼š**
   - è¯·åœ¨â€œå€™é€‰ arXiv è®ºæ–‡åº“â€ä¸­æŸ¥æ‰¾ä¸å½“å‰æ–°é—»**é«˜åº¦ç›¸å…³**çš„è®ºæ–‡ï¼ˆæ ‡é¢˜æˆ–å†…å®¹åŒ¹é…ï¼‰ã€‚
   - **æˆ–è€…**ï¼Œå¦‚æœæä¾›çš„äº‹ä»¶å†…å®¹ï¼ˆæ–°é—»åŸæ–‡ï¼‰ä¸­æ˜ç¡®åŒ…å«äº†ç›¸å…³è®ºæ–‡çš„é“¾æ¥ï¼ˆå¦‚ arXiv é“¾æ¥ï¼‰ï¼Œè¯·ç›´æ¥ä½¿ç”¨è¯¥é“¾æ¥ã€‚
   - **æˆ–è€…**ï¼Œè¯·æ£€æŸ¥æä¾›çš„ `reference_links` å­—æ®µï¼Œå¦‚æœå…¶ä¸­åŒ…å«è®ºæ–‡ç±»å‹çš„é“¾æ¥ï¼ˆå¦‚ type="paper" æˆ– "arxiv"ï¼‰ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ã€‚
   - å¦‚æœæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡ï¼Œè¯·å°† `[ç›¸å…³è®ºæ–‡]([URL])` æ›¿æ¢ä¸ºå®é™…çš„è®ºæ–‡é“¾æ¥ï¼Œä¾‹å¦‚ `[ç›¸å…³è®ºæ–‡](https://arxiv.org/abs/2412.xxxxx)`ã€‚
   - å¦‚æœæœ‰å¤šç¯‡ç›¸å…³ï¼Œå¯ä»¥åˆ—å‡ºå¤šè¡Œï¼Œæ ¼å¼å‡ä¸º `[ç›¸å…³è®ºæ–‡](URL)` æˆ– `[ç›¸å…³è®ºæ–‡: Title](URL)`ã€‚
   - **å¦‚æœæ²¡æœ‰æ‰¾åˆ°é«˜åº¦ç›¸å…³çš„è®ºæ–‡ï¼Œè¯·åŠ¡å¿…åˆ é™¤ `[ç›¸å…³è®ºæ–‡]([URL])` è¿™ä¸€è¡Œï¼Œä¸è¦ä¿ç•™ç©ºè¡Œæˆ–å ä½ç¬¦ã€‚**

**æ–°é—»æ•°æ®ï¼š**
```json
{json.dumps(batch_data, ensure_ascii=False, indent=2)}
```

**è¿”å›æ ¼å¼ï¼š**
è¯·è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼ŒåŒ…å«æ¯æ¡æ–°é—»çš„ç”Ÿæˆç»“æœï¼š
```json
[
  {{
    "article_id": "xxx",
    "category": "Infrastructure", 
    "markdown_content": "### **æ ‡é¢˜**..."
  }},
  ...
]
```
è¯·åªè¿”å› JSONã€‚
"""
        
        for retry in range(self.max_retries):
            response = self._call_llm(prompt, temperature=0.3)
            results = self._parse_json_response(response)
            if results:
                # éªŒè¯æ ¼å¼
                valid_results = []
                errors = []
                for item in results:
                    # 1. åŸºç¡€æ­£åˆ™æ£€æŸ¥
                    is_valid, error = self._validate_news_item_format(item.get("markdown_content", ""))
                    if not is_valid:
                        errors.append(f"æ–‡ç«  '{item.get('title', 'Unknown')}' åŸºç¡€æ ¼å¼é”™è¯¯: {error}")
                        continue
                    
                    # 2. LLM ä¸¥æ ¼æ£€æŸ¥
                    is_strict_valid, strict_error = self._strict_format_check_agent(item.get("markdown_content", ""))
                    if is_strict_valid:
                        valid_results.append(item)
                    else:
                        errors.append(f"æ–‡ç«  '{item.get('title', 'Unknown')}' ä¸¥æ ¼æ ¼å¼é”™è¯¯: {strict_error}")
                
                if not errors:
                    return valid_results
                
                # å¦‚æœæœ‰é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œå°†é”™è¯¯åŠ å…¥ prompt é‡è¯•
                logger.warning(f"æ‰¹æ¬¡ç”Ÿæˆå­˜åœ¨æ ¼å¼é”™è¯¯: {'; '.join(errors)}")
                if retry < self.max_retries - 1:
                    prompt += f"\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆå­˜åœ¨ä»¥ä¸‹æ ¼å¼é”™è¯¯ï¼Œè¯·ä¸¥æ ¼ä¿®æ­£ï¼Œç¡®ä¿Markdownæ ¼å¼å®Œå…¨ç¬¦åˆæ¨¡æ¿ï¼š\n" + "\n".join(errors)
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•ï¼Œä»…è¿”å›æœ‰æ•ˆçš„
                    return valid_results
            
            await asyncio.sleep(1)
            
        return []

    async def generate_final_report(self, news_items: List[NewsItem], arxiv_papers: List[Dict] = None, quality_check: bool = True, days: int = 7, target_count: int = 10, custom_instructions: str = "") -> Optional[str]:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (å¤šè½®ç”Ÿæˆæ¨¡å¼)
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            arxiv_papers: ç›¸å…³ arXiv è®ºæ–‡åˆ—è¡¨
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            days: æŠ¥å‘Šè¦†ç›–çš„å¤©æ•°èŒƒå›´
            target_count: æŠ¥å‘Šæ¡ç›®æ•°é‡ï¼ˆé€Ÿè§ˆæ¡ç›®å’ŒæŠ¥å‘Šå†…æ ‡é¢˜æ•°é‡ï¼‰
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬å…­æ­¥ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š (å¤šè½®ç”Ÿæˆæ¨¡å¼)...")
        
        if not news_items:
            logger.warning("æ²¡æœ‰æ–°é—»å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return None

        # 1. å‡†å¤‡æ•°æ®ï¼šæŒ‰äº‹ä»¶æ•´ä½“æ’åºï¼Œæ¯ä¸ªäº‹ä»¶ä½¿ç”¨æœ€é«˜åˆ†æ–°é—»ä½œä¸ºæ’åºä¾æ®
        # é¦–å…ˆæŒ‰ event_id åˆ†ç»„
        event_groups = defaultdict(list)
        for item in news_items:
            event_groups[item.event_id].append(item)
        
        # æ¯ä¸ªäº‹ä»¶é€‰æ‹©æœ€é«˜åˆ†çš„æ–°é—»ä½œä¸ºæ’åºä»£è¡¨ï¼Œä½†ä¿ç•™è¯¥äº‹ä»¶çš„å…¨éƒ¨æ–°é—»ç”¨äºç”Ÿæˆ
        event_representatives = []
        for event_id, items in event_groups.items():
            # æŒ‰ final_score æ’åº
            sorted_event_items = sorted(items, key=lambda x: x.final_score, reverse=True)
            best_item = sorted_event_items[0]
            event_representatives.append({
                "event_id": event_id,
                "best_item": best_item,
                "event_score": best_item.final_score,  # äº‹ä»¶åˆ†æ•° = ä»£è¡¨æ–°é—»çš„åˆ†æ•°
                "all_items": sorted_event_items  # ä¿ç•™è¯¥äº‹ä»¶çš„å…¨éƒ¨æ–°é—»ï¼ˆå»é‡åæœ€å¤š3æ¡ï¼‰
            })
        
        # æŒ‰äº‹ä»¶åˆ†æ•°æ’åº
        event_representatives.sort(key=lambda x: x["event_score"], reverse=True)
        
        # å–å‰ target_count ä¸ªäº‹ä»¶
        top_events = event_representatives[:target_count]
        
        logger.info(f"å…±è¯†åˆ« {len(event_groups)} ä¸ªç‹¬ç«‹äº‹ä»¶ï¼Œå°†ä¸ºå‰ {len(top_events)} ä¸ªäº‹ä»¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
        for i, e in enumerate(top_events, 1):
            logger.info(f"  [{i}] {e['event_id']}: åˆ†æ•°={e['event_score']:.2f}, åŒ…å« {len(e['all_items'])} æ¡æ–°é—», ä»£è¡¨: {e['best_item'].title[:40]}...")

        # 2. åˆ†æ‰¹ç”Ÿæˆå†…å®¹ (æŒ‰äº‹ä»¶ç”Ÿæˆï¼Œæ¯ä¸ªäº‹ä»¶ç»¼åˆå…¶ä¸‹æ‰€æœ‰æ–°é—»)
        batch_size = 3  # æ¯æ‰¹å¤„ç†3ä¸ªäº‹ä»¶
        generated_entries = []
        
        for i in range(0, len(top_events), batch_size):
            batch_events = top_events[i:i + batch_size]
            logger.info(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Šè¯¦æƒ…ï¼šæ‰¹æ¬¡ {i // batch_size + 1} (å…± {len(batch_events)} ä¸ªäº‹ä»¶)")
            
            # æŒ‰äº‹ä»¶ç”Ÿæˆå†…å®¹ï¼Œæ¯ä¸ªäº‹ä»¶ä½¿ç”¨å…¶ä¸‹æ‰€æœ‰æ–°é—»
            entries = await self._generate_event_entries_batch(batch_events, candidate_papers=arxiv_papers, custom_instructions=custom_instructions)
            if entries:
                generated_entries.extend(entries)
            else:
                logger.error(f"æ‰¹æ¬¡ {i // batch_size + 1} ç”Ÿæˆå¤±è´¥")
        
        # ç”¨äºåç»­é€Ÿè§ˆç”Ÿæˆçš„ä»£è¡¨æ–°é—»åˆ—è¡¨
        valid_items = [e["best_item"] for e in top_events]

        # 3. ç»„ç»‡å†…å®¹ å¹¶ æå–å·²ä½¿ç”¨çš„é“¾æ¥
        # å»ºç«‹ article_id åˆ° news_item çš„æ˜ å°„ï¼Œæ–¹ä¾¿è·å–é¢å¤–ä¿¡æ¯
        item_map = {item.article_id: item for item in valid_items}
        
        category_map = {
            "Infrastructure": [],
            "Model": [],
            "Application": []
        }
        
        # ç”¨äºè®°å½•åœ¨æ­£æ–‡ä¸­å·²ç»å‡ºç°è¿‡çš„é“¾æ¥ï¼Œé¿å…åœ¨æ‹“å±•é˜…è¯»ä¸­é‡å¤
        used_urls = set()
        link_pattern = re.compile(r'\[.*?\]\((https?://.*?)\)')
        
        for entry in generated_entries:
            cat = entry.get("category", "Model")
            if cat not in category_map:
                cat = "Model"  # Fallback
            
            content = entry.get("markdown_content", "")
            category_map[cat].append(content)
            
            # æå–æ­£æ–‡ä¸­çš„æ‰€æœ‰é“¾æ¥
            found_links = link_pattern.findall(content)
            for link in found_links:
                # ç®€å•æ ‡å‡†åŒ–
                clean_link = link.strip().rstrip('/')
                used_urls.add(clean_link)
                # é’ˆå¯¹ arXivï¼ŒåŒæ—¶è®°å½• abs å’Œ pdf ç‰ˆæœ¬ä»¥é˜²ä¸‡ä¸€
                if "arxiv.org/abs/" in clean_link:
                    used_urls.add(clean_link.replace("/abs/", "/pdf/"))
                elif "arxiv.org/pdf/" in clean_link:
                    used_urls.add(clean_link.replace("/pdf/", "/abs/"))

        # 4. ç”Ÿæˆ"æœ¬æœŸé€Ÿè§ˆ" (ä½¿ç”¨ä¸è¯¦ç»†æŠ¥å‘Šç›¸åŒçš„ valid_items)
        top_items = valid_items  # é€Ÿè§ˆå’Œè¯¦ç»†æŠ¥å‘Šä½¿ç”¨ç›¸åŒçš„æ–°é—»åˆ—è¡¨
        overview_prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–°é—»ç”Ÿæˆ"æœ¬æœŸé€Ÿè§ˆ"åˆ—è¡¨ã€‚
è¦æ±‚ï¼š
- æ¯æ¡æ–°é—»ç”¨ä¸€è¡Œ Markdown åˆ—è¡¨é¡¹è¡¨ç¤ºã€‚
- æ ¼å¼ï¼š* **[æ ‡ç­¾]** **æ–°é—»æ ‡é¢˜**: [1-2å¥è¯æ ¸å¿ƒçœ‹ç‚¹]
- æ ‡ç­¾ç¤ºä¾‹ï¼š[å¤§æ¨¡å‹], [èŠ¯ç‰‡], [åº”ç”¨]ç­‰
- å¿…é¡»ä¸¥æ ¼éµå®ˆä¸Šè¿°æ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ã€‚

æ–°é—»æ•°æ®ï¼š
{json.dumps([{"title": item.title, "description": item.description} for item in top_items], ensure_ascii=False, indent=2)}

è¯·ç›´æ¥è¿”å› Markdown åˆ—è¡¨ã€‚
"""
        overview_content = self._call_llm(overview_prompt) or "ç”Ÿæˆå¤±è´¥"
        
        # ç®€å•éªŒè¯æ¦‚è§ˆæ ¼å¼
        if "**[[" not in overview_content:
             logger.warning("æ¦‚è§ˆæ ¼å¼å¯èƒ½ä¸ç¬¦åˆè¦æ±‚ï¼Œå°è¯•ä¿®å¤...")
             # ç®€å•çš„é‡è¯•é€»è¾‘
             overview_prompt += "\n\n**ä¿®æ­£è¦æ±‚**: ä¸Šæ¬¡ç”Ÿæˆæ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ç¡®ä¿æ¯è¡Œä»¥ '* **[æ ‡ç­¾]**' å¼€å¤´ã€‚"
             retry_content = self._call_llm(overview_prompt)
             if retry_content and "**[[" in retry_content:
                 overview_content = retry_content

        # 5. è§£æâ€œæœ¬æœŸé€Ÿè§ˆâ€æ ‡ç­¾ï¼Œæ„å»º æ ‡é¢˜->æ ‡ç­¾ æ˜ å°„ï¼Œä¾¿äºæ‹“å±•é˜…è¯»åˆ†ç»„
        title_tag_map = {}
        tag_line_pattern = re.compile(r"\*\s+\*\*\[\[(?P<tag>.+?)\]\]\*\*\s+\[\*\*(?P<title>.+?)\*\*\]")
        for line in overview_content.splitlines():
            m = tag_line_pattern.search(line)
            if m:
                title_tag_map[m.group("title").strip()] = m.group("tag").strip()

        # å»ºç«‹ event_id -> category çš„æ˜ å°„ (ç”¨äºé Top 10 æ–°é—»çš„æ ‡ç­¾å›é€€)
        # åŒæ—¶å»ºç«‹ article_id -> category çš„æ˜ å°„
        id_category_map = {}
        event_category_map = {}
        for entry in generated_entries:
            cat = entry.get("category", "Model")
            # æ˜ å°„è‹±æ–‡åˆ†ç±»åˆ°ä¸­æ–‡æ ‡ç­¾
            cn_cat = {
                "Infrastructure": "[åŸºç¡€è®¾æ–½]",
                "Model": "[æ¨¡å‹ä¸æŠ€æœ¯]",
                "Application": "[åº”ç”¨ä¸æ™ºèƒ½ä½“]"
            }.get(cat, "[å…¶ä»–]")
            event_id = entry.get("event_id")
            event_category_map[event_id] = cn_cat
            # åŒæ—¶ä¸ºè¯¥äº‹ä»¶ä¸‹çš„æ‰€æœ‰æ–‡ç« å»ºç«‹æ˜ å°„
            for e in top_events:
                if e["event_id"] == event_id:
                    for item in e["all_items"]:
                        id_category_map[item.article_id] = cn_cat
                    break

        # 6. ç”Ÿæˆâ€œæ‹“å±•é˜…è¯»â€ (Reference Links)
        # è¿™é‡Œæ”¶é›†æ‰€æœ‰æ–°é—»ï¼ˆåŒ…æ‹¬ C çº§ï¼‰çš„å‚è€ƒé“¾æ¥ï¼Œä»¥åŠ arXiv è®ºæ–‡
        reference_section = ""
        # å€™é€‰é“¾æ¥åˆ—è¡¨ï¼Œç»“æ„: {'markdown': str, 'type': 'arxiv'|'other', 'tag': str}
        candidates = []
        
        seen_urls = set(used_urls)
        used_arxiv_urls = {u for u in used_urls if "arxiv.org" in u}
        
        def is_valid_ref_link(url: str, title: str) -> bool:
            if not url or not title:
                return False
            # è¿‡æ»¤ç¤¾äº¤åˆ†äº«é“¾æ¥
            if any(x in url for x in ["facebook.com/sharer", "twitter.com/intent", "linkedin.com/share", "reddit.com/submit", "weibo.com", "service.weibo.com"]):
                return False
            # è¿‡æ»¤é€šç”¨ä¸»é¡µ (ä¾‹å¦‚ https://blog.google/ )
            # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœ URL å¾ˆçŸ­æˆ–è€…æ˜¯æ ¹åŸŸåï¼Œå¯èƒ½ä¸æ˜¯å…·ä½“çš„æ–‡ç« 
            if url.count('/') < 3: 
                 return False
            return True
        
        # 6.1 æ”¶é›†ç‹¬ç«‹ arXiv è®ºæ–‡
        if arxiv_papers:
            for paper in arxiv_papers:
                url = paper['url']
                # å¦‚æœè¯¥è®ºæ–‡å·²åœ¨æ­£æ–‡ä¸­å¼•ç”¨ï¼Œåˆ™ä¸æ”¾å…¥æ‹“å±•é˜…è¯»
                if url in used_arxiv_urls:
                    continue
                    
                if url not in seen_urls:
                    candidates.append({
                        "markdown": f"* [{paper['title']}]({url}) - arXiv",
                        "type": "arxiv",
                        "tag": "[å‰æ²¿ç ”ç©¶]"
                    })
                    seen_urls.add(url)
        
        # 6.2 æ”¶é›†æ–°é—»å‚è€ƒé“¾æ¥
        for item in news_items:
            if not item.reference_links:
                continue
                
            # ç¡®å®šæ ‡ç­¾
            tag = title_tag_map.get(item.title)
            if not tag:
                # å›é€€åˆ°åˆ†ç±»
                tag = id_category_map.get(item.article_id, "[è¡Œä¸šåŠ¨æ€]")
            
            try:
                refs = json.loads(item.reference_links)
                for ref in refs:
                    url = ref.get('url', '')
                    title = ref.get('title', 'Ref')
                    
                    if url and url not in seen_urls and is_valid_ref_link(url, title):
                        is_arxiv = "arxiv.org" in url
                        source_tag = f" - {ref.get('type', 'Reference')}"
                        
                        candidates.append({
                            "markdown": f"* [{title}]({url}){source_tag}",
                            "type": "arxiv" if is_arxiv else "other",
                            "tag": tag
                        })
                        seen_urls.add(url)
            except:
                pass
        
        # 6.3 ç­›é€‰é€»è¾‘ (Total 25, Arxiv 15, Other 10)
        MAX_TOTAL = 25
        TARGET_ARXIV = 15 # 60%
        
        arxiv_candidates = [c for c in candidates if c['type'] == 'arxiv']
        other_candidates = [c for c in candidates if c['type'] == 'other']
        
        final_list = []
        
        # 1. ä¼˜å…ˆå– arXiv
        take_arxiv = min(len(arxiv_candidates), TARGET_ARXIV)
        final_list.extend(arxiv_candidates[:take_arxiv])
        
        # 2. å¡«è¡¥å…¶ä»–
        remaining_slots = MAX_TOTAL - len(final_list)
        take_other = min(len(other_candidates), remaining_slots)
        final_list.extend(other_candidates[:take_other])
        
        # 3. å¦‚æœè¿˜æœ‰ç©ºä½ä¸”æœ‰å‰©ä½™ arXivï¼Œç»§ç»­å¡«
        if len(final_list) < MAX_TOTAL and len(arxiv_candidates) > take_arxiv:
            rest_slots = MAX_TOTAL - len(final_list)
            final_list.extend(arxiv_candidates[take_arxiv : take_arxiv + rest_slots])
            
        # 6.4 æŒ‰æ ‡ç­¾åˆ†ç»„è¾“å‡º
        grouped_links = defaultdict(list)
        for item in final_list:
            grouped_links[item['tag']].append(item['markdown'])
            
        sections = []
        # è¾“å‡º Top æ ‡ç­¾ (æŒ‰é€Ÿè§ˆé¡ºåº)
        sorted_top_tags = []
        for t in title_tag_map.values():
            if t not in sorted_top_tags: sorted_top_tags.append(t)
            
        for tag in sorted_top_tags:
            if tag in grouped_links:
                sections.append(f"### {tag}\n" + chr(10).join(grouped_links[tag]))
                del grouped_links[tag]
        
        # è¾“å‡º [å‰æ²¿ç ”ç©¶] (arXiv)
        if "[å‰æ²¿ç ”ç©¶]" in grouped_links:
             sections.append(f"### [å‰æ²¿ç ”ç©¶]\n" + chr(10).join(grouped_links["[å‰æ²¿ç ”ç©¶]"]))
             del grouped_links["[å‰æ²¿ç ”ç©¶]"]
             
        # è¾“å‡ºå‰©ä½™
        for tag, links in grouped_links.items():
            sections.append(f"### {tag}\n" + chr(10).join(links))
            
        reference_section = "\n\n".join(sections)

        # 6. æœ€ç»ˆç»„è£…
        # ä½¿ç”¨ days å‚æ•°è®¡ç®—æ—¥æœŸèŒƒå›´ï¼Œè€Œä¸æ˜¯æ ¹æ®æ–°é—»å‘å¸ƒæ—¶é—´
        today = datetime.now()
        date_range_end = today.strftime('%Y-%m-%d')
        date_range_start = (today - timedelta(days=days - 1)).strftime('%Y-%m-%d')

        final_report = f"""# AI å‰æ²¿åŠ¨æ€é€ŸæŠ¥ ({date_range_start} è‡³ {date_range_end})

## âš¡ æœ¬æœŸé€Ÿè§ˆ

{overview_content}

---

## 1. AI åŸºç¡€è®¾æ–½

{chr(10).join(category_map["Infrastructure"]) if category_map["Infrastructure"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## 2. AI æ¨¡å‹ä¸æŠ€æœ¯

{chr(10).join(category_map["Model"]) if category_map["Model"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## 3. AI åº”ç”¨ä¸æ™ºèƒ½ä½“

{chr(10).join(category_map["Application"]) if category_map["Application"] else "*(æœ¬æœŸæ— ç›¸å…³å†…å®¹)*"}

---

## æ‹“å±•é˜…è¯»

{reference_section}
"""
        return final_report

    def generate_final_report_old(self, news_items: List[NewsItem], quality_check: bool = True) -> Optional[str]:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        
        Args:
            news_items: æ’åºåçš„æ–°é—»åˆ—è¡¨
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("ã€ç¬¬äº”æ­¥ã€‘ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        
        if not news_items:
            logger.warning("æ²¡æœ‰æ–°é—»å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return None
        
        # è¯»å–æ¨¡æ¿
        try:
            with open(self.template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        except Exception as e:
            logger.error(f"è¯»å–æ¨¡æ¿å¤±è´¥: {e}")
            return None
        
        # æŒ‰è¯„çº§åˆ†ç»„
        news_by_level = defaultdict(list)
        for item in news_items:
            news_by_level[item.ranking_level].append(item)
        
        # è®¡ç®—è¦†ç›–æ—¥æœŸèŒƒå›´ï¼ˆç”¨äºæç¤ºæ¨¡å‹æ­£ç¡®å¡«å†™å¤´éƒ¨åŒºé—´ï¼‰
        publish_times = [item.publish_time for item in news_items if item.publish_time]
        if publish_times:
            date_range_start = datetime.fromtimestamp(min(publish_times)).strftime('%Y-%m-%d')
            date_range_end = datetime.fromtimestamp(max(publish_times)).strftime('%Y-%m-%d')
        else:
            today_str = datetime.now().strftime('%Y-%m-%d')
            date_range_start = today_str
            date_range_end = today_str

        # æ ¼å¼åŒ–æ–°é—»æ•°æ®
        formatted_news = ""
        for level in ["S", "A", "B", "C"]:
            items = news_by_level[level]
            if items:
                formatted_news += f"\n## {level}çº§æ–°é—» ({len(items)}æ¡)\n\n"
                for i, item in enumerate(items, 1):
                    pub_date = datetime.fromtimestamp(item.publish_time).strftime('%Y-%m-%d %H:%M')
                    formatted_news += f"### [{i}] {item.title}\n"
                    formatted_news += f"- **æ¥æº**: {item.source}\n"
                    formatted_news += f"- **é“¾æ¥**: {item.url}\n"
                    formatted_news += f"- **å‘å¸ƒæ—¶é—´**: {pub_date}\n"
                    formatted_news += f"- **è¯„åˆ†**: {item.final_score:.2f} (æŠ€æœ¯å½±å“:{item.tech_impact}, è¡Œä¸šèŒƒå›´:{item.industry_scope}, çƒ­åº¦:{item.hype_score})\n"
                    formatted_news += f"- **äº‹ä»¶çƒ­åº¦**: {item.event_count} æ¡ç›¸å…³æŠ¥é“\n"
                    formatted_news += f"- **æ‘˜è¦**: {item.description}\n"
                    
                    # æ·»åŠ å‚è€ƒé“¾æ¥
                    if item.reference_links:
                        try:
                            ref_links = json.loads(item.reference_links)
                            if ref_links:
                                formatted_news += f"- **åŸå§‹æ¥æº**:\n"
                                for ref in ref_links:
                                    formatted_news += f"  - [{ref['title']}]({ref['url']}) (ç±»å‹: {ref['type']})\n"
                        except:
                            pass
                    
                    formatted_news += "\n"
        
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå‰æ²¿ç§‘æŠ€åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ç»è¿‡ç­›é€‰ã€å½’ç±»ã€å»é‡å’Œæ’åºçš„æ–°é—»æ•°æ®ï¼Œç¼–å†™ä¸€ä»½é«˜è´¨é‡çš„AIå‰æ²¿åŠ¨æ€é€ŸæŠ¥ã€‚

**å½“å‰æ—¥æœŸ**: {today_str}
**æŠ¥å‘Šè¦†ç›–æ—¥æœŸèŒƒå›´ï¼ˆåŠ¡å¿…ç”¨äºæ–‡é¦–æ‹¬å·åŒºé—´ï¼‰**: {date_range_start} è‡³ {date_range_end}

**æŠ¥å‘Šæ¨¡æ¿å’Œè¦æ±‚:**
{template_content}

**ç»è¿‡å¤„ç†çš„æ–°é—»æ•°æ®:**
{formatted_news}

**ç‰¹åˆ«æŒ‡ä»¤:**
1. ä¸¥æ ¼éµå¾ªæ¨¡æ¿æ ¼å¼ï¼Œä¸€çº§æ ‡é¢˜å’ŒäºŒçº§æ ‡é¢˜å¿…é¡»ä¸æ¨¡æ¿ä¸€è‡´
2. æ–°é—»å·²æŒ‰ S/A/B/C çº§åˆ«æ’åºï¼Œä¼˜å…ˆå…³æ³¨ S çº§å’Œ A çº§æ–°é—»
3. æ·±åº¦è§£è¯»éƒ¨åˆ†è¦æœ‰å®è´¨å†…å®¹ï¼Œç»“åˆæŠ€æœ¯èƒŒæ™¯å’Œè¡Œä¸šå½±å“è¿›è¡Œåˆ†æ
4. å¦‚æœæ–°é—»æ•°æ®ä¸­æä¾›äº†"åŸå§‹æ¥æº"ï¼Œé˜…è¯»åŸæ–‡çš„é“¾æ¥å¿…é¡»ä½¿ç”¨å®˜æ–¹æ ¸å¿ƒä¿¡æºæ¥æºURLï¼Œç¦æ­¢ä½¿ç”¨é‡å­ä½è‡ªèº«é“¾æ¥
5. Source å­—æ®µä¼˜å…ˆå¡«å†™åŸå§‹æ¥æºåç§°ï¼ˆå¦‚ OpenAI, arXiv ç­‰ï¼‰
6. è¯­è¨€é£æ ¼è¦ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ´å¯ŸåŠ›
7. è¾“å‡ºå¿…é¡»æ˜¯ Markdown æ ¼å¼
8. ä¸‰çº§æ ‡é¢˜åŸºäºå†…å®¹åˆ†æç”Ÿæˆï¼Œè¦æœ‰ä¸ªæ€§åŒ–å’Œæ´å¯ŸåŠ›

è¯·ç”Ÿæˆå®Œæ•´çš„æŠ¥å‘Šå†…å®¹ã€‚
"""
        
        # è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
        max_attempts = 3 if quality_check else 1
        
        for attempt in range(max_attempts):
            logger.info(f"æ­£åœ¨ç”ŸæˆæŠ¥å‘Š (å°è¯• {attempt + 1}/{max_attempts})...")
            
            report_content = self._call_llm(prompt, temperature=0.3)
            
            if not report_content:
                logger.error("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                continue
            
            # è´¨é‡æ£€æŸ¥
            if quality_check and attempt < max_attempts - 1:
                if self._check_report_quality(report_content, template_content):
                    logger.info("æŠ¥å‘Šè´¨é‡æ£€æŸ¥é€šè¿‡")
                    return report_content
                else:
                    logger.warning(f"æŠ¥å‘Šè´¨é‡æ£€æŸ¥æœªé€šè¿‡ï¼Œé‡æ–°ç”Ÿæˆ (å°è¯• {attempt + 2}/{max_attempts})")
                    # æ·»åŠ è´¨é‡åé¦ˆåˆ°æç¤ºè¯
                    prompt += "\n\n**è´¨é‡é—®é¢˜**: ä¸Šä¸€æ¬¡ç”Ÿæˆçš„æŠ¥å‘Šæ ¼å¼æˆ–å†…å®¹ä¸ç¬¦åˆè¦æ±‚ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿ç”Ÿæˆã€‚"
            else:
                return report_content
        
        return report_content
    
    def _check_report_quality(self, report: str, template: str) -> bool:
        """
        æ£€æŸ¥æŠ¥å‘Šè´¨é‡
        
        Args:
            report: ç”Ÿæˆçš„æŠ¥å‘Š
            template: æ¨¡æ¿å†…å®¹
            
        Returns:
            æ˜¯å¦é€šè¿‡è´¨é‡æ£€æŸ¥
        """
        # ç®€å•æ£€æŸ¥ï¼šæ˜¯å¦åŒ…å«å…³é”®ç« èŠ‚
        required_sections = ["AIå‰æ²¿åŠ¨æ€é€ŸæŠ¥", "æœ¬å‘¨ç„¦ç‚¹", "æ·±åº¦è§£è¯»"]
        
        for section in required_sections:
            if section not in report:
                logger.warning(f"æŠ¥å‘Šç¼ºå°‘å¿…éœ€ç« èŠ‚: {section}")
                return False
        
        # æ£€æŸ¥æŠ¥å‘Šé•¿åº¦
        if len(report) < 500:
            logger.warning("æŠ¥å‘Šå†…å®¹è¿‡çŸ­")
            return False
        
        return True
    
    async def run(self, days: int = 3, save_intermediate: bool = True, report_count: int = 10) -> Optional[str]:
        """
        è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹
        
        Args:
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            report_count: æŠ¥å‘Šæ¡ç›®æ•°é‡ï¼ˆé€Ÿè§ˆæ¡ç›®å’ŒæŠ¥å‘Šå†…æ ‡é¢˜æ•°é‡ï¼‰
            
        Returns:
            æœ€ç»ˆæŠ¥å‘Šå†…å®¹
        """
        logger.info("=" * 60)
        logger.info("GeminiAIReportAgent å¼€å§‹è¿è¡Œ")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        # 1. è·å–æ•°æ®
        news_items = await self.fetch_articles_from_db(days=days)
        if not news_items:
            logger.error("æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return None
        
        # 2. è¿‡æ»¤
        news_items = await self.step1_filter(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "01_filtered")
        
        # 3. å½’ç±»
        news_items = await self.step2_cluster(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "02_clustered")
        
        # 4. å»é‡
        news_items = await self.step3_deduplicate(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "03_deduplicated")
        
        # 5. æ’åº
        news_items = await self.step4_rank(news_items)
        if save_intermediate:
            self._save_intermediate_results(news_items, "04_ranked")
            
        # 6. è·å– arXiv è®ºæ–‡
        arxiv_papers = await self.step5_fetch_arxiv_papers(news_items)
        if save_intermediate:
            # ä¿å­˜ arXiv ç»“æœï¼ˆç®€å•åŒ…è£…ä¸€ä¸‹ä»¥ä¾¿å¤ç”¨ä¿å­˜é€»è¾‘ï¼Œæˆ–è€…ç›´æ¥å­˜jsonï¼‰
            arxiv_output_dir = Path("final_reports") / "intermediate"
            arxiv_output_dir.mkdir(exist_ok=True, parents=True)
            arxiv_output_file = arxiv_output_dir / f"05_arxiv_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.json"
            with open(arxiv_output_file, 'w', encoding='utf-8') as f:
                json.dump(arxiv_papers, f, ensure_ascii=False, indent=2)
            logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {arxiv_output_file}")
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        report_content = await self.generate_final_report(news_items, arxiv_papers=arxiv_papers, quality_check=True, days=days, target_count=report_count)
        
        if report_content:
            # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
            output_dir = Path("final_reports")
            output_dir.mkdir(exist_ok=True)
            output_file = output_dir / f"AI_Report_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.md"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info("=" * 60)
            logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼è€—æ—¶: {elapsed:.2f}ç§’")
            logger.info(f"æŠ¥å‘Šä¿å­˜è‡³: {output_file}")
            logger.info("=" * 60)
            
            return report_content
        else:
            logger.error("æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            return None
    
    def _save_intermediate_results(self, news_items: List[NewsItem], stage: str):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        output_dir = Path("final_reports") / "intermediate"
        output_dir.mkdir(exist_ok=True, parents=True)
        
        output_file = output_dir / f"{stage}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.json"
        
        data = [item.to_dict() for item in news_items]
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    async def test_agent():
        agent = GeminiAIReportAgent(max_retries=2)
        await agent.run(days=3, save_intermediate=True)
    
    asyncio.run(test_agent())

